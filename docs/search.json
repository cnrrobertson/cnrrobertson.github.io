[
  {
    "objectID": "other/fun/fun.html",
    "href": "other/fun/fun.html",
    "title": "Just for Fun",
    "section": "",
    "text": "This is a collection of code doodles (almost literally since most produce nice plots or videos)."
  },
  {
    "objectID": "other/fun/fun.html#doodles",
    "href": "other/fun/fun.html#doodles",
    "title": "Just for Fun",
    "section": "Doodles",
    "text": "Doodles\n\nExternal angles of a polygon sum to \\(2\\pi\\)\nVisually comparing the complexity of the heliocentric and geocentric models of the solar system"
  },
  {
    "objectID": "other/fun/heliocentric_geocentric.html",
    "href": "other/fun/heliocentric_geocentric.html",
    "title": "Heliocentrism vs Geocentrism",
    "section": "",
    "text": "Another fun little animation that I saw recently was the difference in the dynamics of the solar system when approaching the equations from a heliocentric (sun-centered) or geocentric (Earth-centered) perspective. Although it’s clear now that we orbit the sun and a sun-centered model of the solar system is more correct, it wasn’t always obvious. This concept extends to systems that we have not yet nailed down completely. This simple animation demonstrates that identifying the correct perspective when modeling a system can have a huge pay off in the simplicity of your result.\nI thought I’d try to recreate this little animation with Makie.jl as I did previously with the polygon GIF."
  },
  {
    "objectID": "other/fun/heliocentric_geocentric.html#setting-up",
    "href": "other/fun/heliocentric_geocentric.html#setting-up",
    "title": "Heliocentrism vs Geocentrism",
    "section": "Setting up",
    "text": "Setting up\nIn this animation, we’ll be rotating some circular shapes around the point representing either the sun or the Earth and tracing their paths as they progress. First, let’s plot the initial frame of the animation using a Figure with 2 axes:\n\nusing Pkg;\nPkg.activate(\".\");\nusing CairoMakie;\n\n\nf = Figure(resolution=(800,400));\naxes = [Axis(f[1,1]);Axis(f[1,2])]\nfor ax in axes ax.limits=(-22,22,-22,22) end\nfunction remove_axis_decor!(ax)\n  ax.topspinevisible = false; ax.bottomspinevisible = false\n  ax.leftspinevisible = false; ax.rightspinevisible = false\n  ax.xgridvisible = false; ax.ygridvisible = false\n  ax.xticksvisible = false; ax.yticksvisible = false\n  ax.xticklabelsvisible = false; ax.yticklabelsvisible = false\nend\nremove_axis_decor!.(axes)\n\nWe can now layout the different planets via a simple scatter plot in each axis. Of course, we cannot use the correct proportions or distances or the plot would be hard to understand. Instead, I’ll settle for simple size differences between the planets and the sun and a somewhat uniform distance between each.\n\nnum_bodies = 9\nbody_locs1 = [(float(i),0.0) for i in 0:2:2(num_bodies-1)]\nbody_locs2 = [(float(i),0.0) for i in -6:2:2(num_bodies-1)-6]\nbody_sizes = 3 .* [9,3,3,4,2,5,6,4,4]\nbody_colors = [:yellow,:red,:red,:blue,:red,:red,:red,:red,:red]\ns1 = scatter!(axes[1], body_locs1, markersize=body_sizes, color=body_colors)\ns2 = scatter!(axes[2], body_locs2, markersize=body_sizes, color=body_colors)\ndisplay(f)"
  },
  {
    "objectID": "other/fun/heliocentric_geocentric.html#animation",
    "href": "other/fun/heliocentric_geocentric.html#animation",
    "title": "Heliocentrism vs Geocentrism",
    "section": "Animation",
    "text": "Animation\nOkay! Easy as that. Now, we can move on to animating the rotation of the bodies. Each planet will rotate at a different speed and will go until again lining up as they started.\n\nbody_speeds = [0.0,47.87,35.02,29.78,24.077,13.07,9.69,6.81,5.43] ./ 200\nsun_speed2 = body_speeds[4]\norbit_radii1 = [bl[1] for bl in body_locs1]\norbit_radii2 = [bl[1] for bl in body_locs2]\n\n# Use Observable to add time dependence to planet locations\ntime_i = Observable(0.0)\nbody_xs1 = @lift(orbit_radii1 .* cos.(-1 .* body_speeds .* $time_i))\nbody_ys1 = @lift(orbit_radii1 .* sin.(-1 .* body_speeds .* $time_i))\nbody_xs2 = @lift(vcat(\n    orbit_radii2[1]*cos(-sun_speed2*$time_i),\n    orbit_radii2[1]*cos(-sun_speed2*$time_i) + orbit_radii1[2]*cos(-body_speeds[2]*$time_i),\n    orbit_radii2[1]*cos(-sun_speed2*$time_i) + orbit_radii1[3]*cos(-body_speeds[3]*$time_i),\n    0.0,\n    orbit_radii2[1]*cos(-sun_speed2*$time_i) .+ orbit_radii1[5:end] .* cos.(-1 .* body_speeds[5:end] .* $time_i)\n))\nbody_ys2 = @lift(vcat(\n    orbit_radii2[1]*sin(-sun_speed2*$time_i),\n    orbit_radii2[1]*sin(-sun_speed2*$time_i) + orbit_radii1[2]*sin(-body_speeds[2]*$time_i),\n    orbit_radii2[1]*sin(-sun_speed2*$time_i) + orbit_radii1[3]*sin(-body_speeds[3]*$time_i),\n    0.0,\n    orbit_radii2[1]*sin(-sun_speed2*$time_i) .+ orbit_radii1[5:end] .* sin.(-1 .* body_speeds[5:end] .* $time_i)\n))\n\nempty!(axes[1].scene.plots)\nempty!(axes[2].scene.plots)\ns1 = scatter!(axes[1], body_xs1, body_ys1, markersize=body_sizes, color=body_colors)\ns2 = scatter!(axes[2], body_xs2, body_ys2, markersize=body_sizes, color=body_colors)\n\n# Create GIF by iterating time\nsteps = 300\nrecord(f, \"gifs/heliocentric_geocentric1.gif\", 1:steps) do t\n    time_i[] = t\nend\n\n\nNice! We’ve got the two animations moving well. Note that since the animation was fairly straightforward and only required updating the scatter plot locations, we were able to use an Observable for time in Makie. This object allows us to create the initial scatter plots where the scatter locations are wrapped with the @lift macro with the interpolating $time_i. Now, when our Observable, time_i is updated, the scatter points and subsequently the scatter plots are updated. Using this nifty tool, our recording loop is very straightforward. However, using the @lift macro is not particularly intuitive and it took some trial and error to get the definition of the scatter points correctly wrapped in an Observable. Hence, the definitions of body_xs2 and body_ys2 are so messy..\nOur next step is to add the path tracing of the planets to each plot. Again, this is a fairly simple procedure that could be completed with an Observable.\n\n# Line observables\nline_locs1 = [Observable([body_locs1[i]]) for i in 1:num_bodies]\nline_locs2 = [Observable([body_locs2[i]]) for i in 1:num_bodies]\n\nempty!(axes[1].scene.plots)\nempty!(axes[2].scene.plots)\nfor i in 1:num_bodies\n    lines!(axes[1], line_locs1[i], color=body_colors[i])\n    lines!(axes[2], line_locs2[i], color=body_colors[i])\nend\ns1 = scatter!(axes[1], body_xs1, body_ys1, markersize=body_sizes, color=body_colors)\ns2 = scatter!(axes[2], body_xs2, body_ys2, markersize=body_sizes, color=body_colors)\n\n# Create GIF by iterating time\nsteps = 300\nrecord(f, \"gifs/heliocentric_geocentric.gif\", 1:steps) do t\n    time_i[] = t\n    for i in 1:num_bodies\n        line_locs1[i][] = push!(line_locs1[i][], (body_xs1[][i], body_ys1[][i]))\n        line_locs2[i][] = push!(line_locs2[i][], (body_xs2[][i], body_ys2[][i]))\n    end\nend\n\n\nAlright! Our animation is now complete."
  },
  {
    "objectID": "other/fun/polygon_angles.html",
    "href": "other/fun/polygon_angles.html",
    "title": "The exterior angles of a polygon make a circle",
    "section": "",
    "text": "I recently saw a fun little GIF from a weekly news email I get called the New Paper. It shows a simple plot of the exterior angles of a few polygons. As the polygons shrink, the exterior angles combine to eventually make a circle, which shows a simple graphical example of how the exterior angles of any polygon add to \\(2\\pi\\). \n\nI thought I’d try to recreate this little GIF with my favorite plotting library Makie.jl."
  },
  {
    "objectID": "other/fun/polygon_angles.html#setting-up",
    "href": "other/fun/polygon_angles.html#setting-up",
    "title": "The exterior angles of a polygon make a circle",
    "section": "Setting up",
    "text": "Setting up\nBasically, we can start by getting the plots of each polygon set. We can then animate the sides of the polygons shrinking.\nTo start we are going to need a Figure with 4 axes:\n\nusing Pkg;\nPkg.activate(\".\");\nusing CairoMakie;\n\n\nf = Figure(resolution=(800,800));\naxes = [\n  Axis(f[1,1]) Axis(f[1,2]);\n  Axis(f[2,1]) Axis(f[2,2])\n]\nfor ax in axes ax.limits=(-6,6,-6,6) end\n\nWe can now list the vertices for each polygon:\n\npoly11 = [(-2.0,3.0),(3.0,-3.0),(-4.0,-2.0),(-2.0,3.0)];\npoly12 = [(-3.0,2.0),(1.0,1.0),(3.0,-2.0),(-4.0,-1.0),(-3.0,2.0)];\npoly21 = [(-1.0,3.0),(1.0,3.0),(3.0,-1.0),(1.0,-3.0),(-2.0,-2.0),(-3.0,1.0),(-1.0,3.0)];\npoly22 = [(-1.0,2.0),(1.0,2.0),(4.0,-1.0),(2.0,-3.0),(-4.0,-1.0),(-1.0,2.0)];\n\nwhere poly11 is the polygon in the 1st row and 1st column. Plotting these lines on each respective axis, we get:\n\nlines!(axes[1,1],poly11,color=:black);\nlines!(axes[1,2],poly12,color=:black);\nlines!(axes[2,1],poly21,color=:black);\nlines!(axes[2,2],poly22,color=:black);\npoly!(axes[1,1],poly11,transparency=true,color=RGBAf(1.0,0.6,0.0,0.2));\npoly!(axes[1,2],poly12,transparency=true,color=RGBAf(0.0,0.0,1.0,0.2));\npoly!(axes[2,1],poly21,transparency=true,color=RGBAf(0.5,0.0,0.5,0.2));\npoly!(axes[2,2],poly22,transparency=true,color=RGBAf(0.0,0.5,0.0,0.2));\ndisplay(f)\n\n\n\n\nThese are obviously not exactly the polygons in the GIF, but they are generally similar and use nice easy vertex coordinates. Now, in order to accentuate the exterior angles, the GIF uses lines which extend beyond the vertices. To achieve this, we can consider each line segment and shift the first vertex some distance in the opposite direction of the second vertex. To do so, we should shift adjust our polygon representation to separate each line segment:\n\nlpoly11 = [[poly11[i],poly11[i+1]] for i in 1:length(poly11)-1];\nlpoly12 = [[poly12[i],poly12[i+1]] for i in 1:length(poly12)-1];\nlpoly21 = [[poly21[i],poly21[i+1]] for i in 1:length(poly21)-1];\nlpoly22 = [[poly22[i],poly22[i+1]] for i in 1:length(poly22)-1];\ndisplay(lpoly11)\n\n3-element Vector{Vector{Tuple{Float64, Float64}}}:\n [(-2.0, 3.0), (3.0, -3.0)]\n [(3.0, -3.0), (-4.0, -2.0)]\n [(-4.0, -2.0), (-2.0, 3.0)]\n\n\nWe can now the vector between the first and second indices of each line segment and shift our first vertex by the negative of that vector. That is a mouthful but more easily written mathematically. If we consider a single line segment with vertices \\(v_1\\) and \\(v_2\\), we can calculate the distance between them \\(d = v_2 - v_1\\) such that \\(v_1 + d = v_2\\) and then redefine our first vertex in the opposite direction as \\(v_1^* = v_1 - l\\frac{d}{\\|d\\|}\\) where \\(l\\) is the length of the external line. This boils down to the following:\n\nfunction shift_first_vertices!(lpoly, l=2)\n   for line in lpoly\n     v1 = collect(line[1]); v2 = collect(line[2])\n     d = v2 - v1\n     v1star = v1 - l*d/sqrt(d[1]^2+d[2]^2)\n     line[1] = tuple(v1star...)\n   end\nend\nshift_first_vertices!(lpoly11)\nshift_first_vertices!(lpoly12)\nshift_first_vertices!(lpoly21)\nshift_first_vertices!(lpoly22)\nfunction plot_line_segments!(ax,lpoly)\n  lines = []\n  for line in lpoly push!(lines,lines!(ax,line,color=:black)) end\n  return lines\nend\nplot_line_segments!(axes[1,1],lpoly11)\nplot_line_segments!(axes[1,2],lpoly12)\nplot_line_segments!(axes[2,1],lpoly21)\nplot_line_segments!(axes[2,2],lpoly22)\ndisplay(f)\n\n\n\n\nOnce we have these lines in place, we can add the external angles. Ironically, the best tool in Makie for these angle drawings is the poly! function which plots a filled polygon from some given vertices. Thus, we need to compute the vertices of the arc for each angle of each polygon.\nThis computation can be done by taking two connected line segments \\(d_1\\) and \\(d_2\\), identifying the angle between them using the law of cosines \\(\\arccos(d_1 \\cdot d_2)\\), and sampling points along the arc of given radius \\(l\\). Sampling the arc requires a little change of coordinates to center the points around the vertex connecting the two line segments and to rotate the standard \\(x\\) and \\(y\\) coordinates to align \\(x\\) with \\(d_1\\) and \\(y\\) with \\(d_1^\\perp\\). This is, in my opinion, the most challenging part of the plot.\n\nfunction angle_vertices(line1,line2,l=1)\n  v1 = collect(line1[1])\n  v2 = collect(line1[2]) # Shared vertex\n  v3 = collect(line2[2])\n  d1 = v2-v1\n  d2 = v3-v2\n  # Line segment directions (normalized\n  d1 ./= sqrt(d1[1]^2+d1[2]^2)\n  d2 ./= sqrt(d2[1]^2+d2[2]^2)\n  d1perp = [d1[2],-d1[1]]\n  vertex = tuple(v2...)\n  # Computing angle between lines, then sampling arc points\n  angle = acos(d1'*d2)\n  angle = isnan(angle) ? 0.0 : angle\n  angles = range(0, angle, length=10)\n  # arc has radius l, origin at v2, \"x\"-direction is d1, \"y\"-direction is d1perp\n  arc_points = [tuple(@.( v2 - l*(d1*cos(a) + d1perp*sin(a)))...) for a in angles]\n  vertices = vcat(vertex,arc_points,vertex)\n  return vertices\nend\nfunction plot_arcs!(ax,lpoly)\n  arcs = []\n  colors = to_colormap(:seaborn_colorblind)\n  for i in 1:length(lpoly)\n    if i+1 > length(lpoly) # The angle between the last line segment and first\n      color = colors[i+1]\n      arc_vertices = angle_vertices(lpoly[i],lpoly[1])\n    else\n      color = colors[i]\n      arc_vertices = angle_vertices(lpoly[i],lpoly[i+1])\n    end\n    push!(arcs,poly!(ax,arc_vertices,color=color))\n  end\n  return arcs\nend\nplot_arcs!(axes[1,1],lpoly11)\nplot_arcs!(axes[1,2],lpoly12)\nplot_arcs!(axes[2,1],lpoly21)\nplot_arcs!(axes[2,2],lpoly22)\ndisplay(f)\n\n\n\n\nNow, removing the axes decorations, we have a clean plot of (almost) the first frame of the GIF:\n\nfunction remove_axis_decor!(ax)\n  ax.topspinevisible = false; ax.bottomspinevisible = false\n  ax.leftspinevisible = false; ax.rightspinevisible = false\n  ax.xgridvisible = false; ax.ygridvisible = false\n  ax.xticksvisible = false; ax.yticksvisible = false\n  ax.xticklabelsvisible = false; ax.yticklabelsvisible = false\nend\nremove_axis_decor!.(axes)\ndisplay(f)"
  },
  {
    "objectID": "other/fun/polygon_angles.html#animating",
    "href": "other/fun/polygon_angles.html#animating",
    "title": "The exterior angles of a polygon make a circle",
    "section": "Animating",
    "text": "Animating\nWith the initial plot now done, to complete the animation, it remains to shrink each polygon until the angles come together to form a circle. This can be simply done (with slight error) by computing the center of each polygon via averaging, centering the vertices around that center, then shrinking the vertices proportional to the number of steps in the animation. Putting everything together:\n\n# Initialize\nf = Figure(resolution=(800,800));\naxes = [\n  Axis(f[1,1]) Axis(f[1,2]);\n  Axis(f[2,1]) Axis(f[2,2])\n]\nfor ax in axes ax.limits = (-6,6,-6,6) end\nremove_axis_decor!.(axes)\npoly11 = [(-2.0,3.0),(3.0,-3.0),(-4.0,-2.0),(-2.0,3.0)];\npoly12 = [(-3.0,2.0),(1.0,1.0),(3.0,-2.0),(-4.0,-1.0),(-3.0,2.0)];\npoly21 = [(-1.0,3.0),(1.0,3.0),(3.0,-1.0),(1.0,-3.0),(-2.0,-2.0),(-3.0,1.0),(-1.0,3.0)];\npoly22 = [(-1.0,2.0),(1.0,2.0),(4.0,-1.0),(2.0,-3.0),(-4.0,-1.0),(-1.0,2.0)];\n# Polygon average centers\nfunction compute_center(poly)\n  vec(sum(hcat(collect.(poly)...),dims=2)./length(poly))\nend\nc11 = compute_center(poly11)\nc12 = compute_center(poly12)\nc21 = compute_center(poly21)\nc22 = compute_center(poly22)\nfunction shrink_polygon(poly,c,step,steps)\n  new_vertices = similar(poly)\n  for i in eachindex(poly)\n    vertex = collect(poly[i]) - c\n    new_vertex = @. vertex*((steps-step)/(steps))\n    new_vertices[i] = tuple((new_vertex + c)...)\n  end\n  return new_vertices\nend\n\n# Animation (somewhat inefficient since it doesn't use Observables)\nsteps = 120\nrecord(f, \"gifs/angle_gif.gif\", vcat(1:(steps-1),fill(steps-1,steps÷4),(steps-1):-1:1)) do t\n  empty!(axes[1,1].scene.plots)\n  empty!(axes[1,2].scene.plots)\n  empty!(axes[2,1].scene.plots)\n  empty!(axes[2,2].scene.plots)\n  npoly11 = shrink_polygon(poly11,c11,t,steps)\n  npoly12 = shrink_polygon(poly12,c12,t,steps)\n  npoly21 = shrink_polygon(poly21,c21,t,steps)\n  npoly22 = shrink_polygon(poly22,c22,t,steps)\n  lpoly11 = [[npoly11[i],npoly11[i+1]] for i in 1:length(npoly11)-1];\n  lpoly12 = [[npoly12[i],npoly12[i+1]] for i in 1:length(npoly12)-1];\n  lpoly21 = [[npoly21[i],npoly21[i+1]] for i in 1:length(npoly21)-1];\n  lpoly22 = [[npoly22[i],npoly22[i+1]] for i in 1:length(npoly22)-1];\n  shift_first_vertices!(lpoly11)\n  shift_first_vertices!(lpoly12)\n  shift_first_vertices!(lpoly21)\n  shift_first_vertices!(lpoly22)\n  poly!(axes[1,1],npoly11,transparency=true,color=RGBAf(1.0,0.6,0.0,0.2));\n  poly!(axes[1,2],npoly12,transparency=true,color=RGBAf(0.0,0.0,1.0,0.2));\n  poly!(axes[2,1],npoly21,transparency=true,color=RGBAf(0.5,0.0,0.5,0.2));\n  poly!(axes[2,2],npoly22,transparency=true,color=RGBAf(0.0,0.5,0.0,0.2));\n  plot_arcs!(axes[1,1],lpoly11)\n  plot_arcs!(axes[1,2],lpoly12)\n  plot_arcs!(axes[2,1],lpoly21)\n  plot_arcs!(axes[2,2],lpoly22)\n  plot_line_segments!(axes[1,1],lpoly11)\n  plot_line_segments!(axes[1,2],lpoly12)\n  plot_line_segments!(axes[2,1],lpoly21)\n  plot_line_segments!(axes[2,2],lpoly22)\nend"
  },
  {
    "objectID": "other/other.html",
    "href": "other/other.html",
    "title": "Other",
    "section": "",
    "text": "This section is made up of various other resources related to my interests or activities. Some are for specific events like the Machine Learning and Optimization seminar at NJIT, some might be useful for other people, and some are just for fun."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "",
    "text": "Other formats\n\n\n\n\n\nDownload the workshop as a Jupyter notebook here.\nAfter downloading the workshop Jupyter notebook, you can upload it to Google Colab to get a quick start, but you will not be able to see the animations."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#overview",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#overview",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "Overview",
    "text": "Overview\n\nMachine Learning and Optimization Seminar\nThe goal of the Machine Learning and Optimization seminar this year is to expose the participants to topics in machine learning and optimization by:\n\nFacilitating hands-on workshops and group discussions to explore and gain experience\nInviting speakers to introduce machine learning and optimization concepts\n\nWe are excited to get started on this but recognize that since neither machine learning nor optimization are standardized in the department, participants will have a varied level of exposure to different topics. Our hope is that we can use this disparity of experience to increase collaboration during the workshops in a way that can’t be achieved during the talks. All are encouraged to share their knowledge and experience with one another openly during the workshops and to give feedback to the organizers after.\nAll workshop material will be available here for later reference.\nThis first workshop is focused on tooling and the basic concepts of machine learning and optimization with the goal that everyone can be on the same footing for later workshops."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#your-python-environment",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#your-python-environment",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "Your Python environment",
    "text": "Your Python environment\nIt is safe to say that Python is the language of choice for machine learning. This interpreted language has a very clear and high-level syntax, is extremely convenient for interactive programming and debugging, and has an enormous user base of enterprises, researchers, and hobbyists who have built an almost infinite collection of open-source packages for every topic. For these three reasons the workshops for this seminar will use Python.\nTo get started, we will give the basics of the Python programming language. Just kidding! That would take too long. We will instead guide you on how to install Python most conveniently, teach you how to get started learning about Python, and then point you to a curated library of much more high-quality instruction for using Python. A list of some such references as well as documentation for setup and important Python packages can be found in the Appendix here.\n\nSetting up\n\n\n\n\n\n\nTip\n\n\n\nIf you are looking for the easiest and most immediate way to get going with a Python Jupyter Notebook, check out the Google Colab section.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis section will require use of a terminal emulator for installing and running Python. If you are not familiar with the terminal, check out this quick tutorial to get started.\nIf you are using a computer with Windows, the terminal instructions may not apply.\n\n\nPython is installed by default on MacOS and most Linux distributions. However, it can be challenging to navigate between the versions and packages that your operating system uses and those needed for other projects. Thus, there are a variety of version, package, and environment management tools:\n\nVersion management: Which version of Python are you using? Can you change versions to run a specific Python program if it requires?\n\npyenv\nconda/mamba\n\nPackage management: How can you install the many amazing Python packages people have created?\n\npip\nconda/mamba\n\nEnvironment management: If you have two projects that require different packages (or different versions of the same package), can you switch which packages are available depending on which project you are working on?\n\nvenv\nvirtualenv\npoetry\nconda/mamba\nmany more\n\n\nThe conda package manager is the only one that fills all three roles. It is formally a part of the Anaconda Python distribution which is a favorite in the fields of data science and machine learning. mamba is a newer and faster rewrite used in exactly the same way and which is highly recommended.\nThe best way to get started with mamba is to install mambaforge. You can find installer downloads for Windows, MacOS, or Linux here.\nFor Windows, run the .exe file once it is downloaded.\nFor MacOS and Linux, open a terminal and navigate to the download location:\ncd ~/Downloads\nThen run the installer as follows:\n./Mambaforge-Linux-x86_64.sh\nThe installer will walk you through a few steps and end by asking if you’d like to “initialize Mambaforge by running conda init?” Answer yes and restart your terminal. This final command will have added conda and mamba to your system $PATH variable, which means it is available to your terminal. Once restarted, run mamba -V to print the version and to verify that the installation worked.\n\n\nEnvironments\nThe idea of a conda/mamba environment is that once an environment is created and activated, all new packages installed will be added to that environment and will be accessible to any Python program run while the environment is active. As an example, let’s create an environment called workshop with a specific version of Python installed. The following will create the environment and install a specific version of python:\nmamba create -n workshop python=3.9\nOnce created, we can list our environments via the command\nmamba env list\n# conda environments:\n#\nbase                     /home/user/mambaforge\nworkshop                 /home/user/mambaforge/envs/workshop\nNote that there is a “base” environment which is where conda and mamba themselves are installed as well as their dependencies. The best practice is to create an environment for each of your projects to minimize dependency issues (when packages require separate versions of the same package).\nTo activate our new environment:\nmamba activate workshop\nRunning mamba env list will now show our active environment via an asterisk:\nbase                     /home/user/mambaforge\nworkshop              *  /home/user/mambaforge/envs/workshop\n\n\nInstalling packages\nNow that we have activated the workshop conda environment, let’s install some common machine learning packages in Python. It is as easy as writing:\nmamba install numpy matplotlib pandas jupyter scipy scikit-learn scikit-image\nThis command will search the conda-forge repository of packages and install the most up-to-date versions (the forge in mambaforge).\n\n\n\n\n\n\nTip\n\n\n\nEither conda or mamba could be used for all the commands discussed in this section. However, mamba is significantly faster when installing packages.\n\n\nNow that these packages have been installed, we can easily use them in an interactive ipython prompt (installed with the jupyter package):\nipython\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import numpy as np\n\nIn [2]: import matplotlib.pyplot as plt\n\nIn [3]: x = np.linspace(0,10,100)\n\nIn [4]: y = np.sin(x)\n\nIn [5]: plt.plot(x,y); plt.show()\nThis should plot a simple \\(\\sin\\) curve.\n\n\nCleaning up\nAfter we are done using the environment that has our desired version of Python and the needed packages, we can go back to our regular terminal by deactivating the environment:\nmamba deactivate workshop\nIf we have somehow broken our environment and need to remove it:\nmamba env remove -n workshop\nThere are many more commands and functionalities that conda and mamba provide that can be found in the python resources and packages section of the Appendix.\n\n\nGoogle Colab\nAs an alternative to the entire procedure above, you can use an online Jupyter Notebook service hosted by Google called Colab. This service will get you up and running immediately but cannot save your environment between notebooks and has limited functionality to run scripts, save data, view animations, change package versions, etc. Thus, if your notebook requires a package that is not installed by default, you will need to add the installation command in one of the first notebook cells. For example, to install the reservoirpy package, we would write in a notebook cell:\n!pip install reservoirpy\nIn a notebook, the ! denotes a terminal command. The package will now be ready for import and use within the current notebook session:\nfrom reservoirpy.nodes import Input,Reservoir,Ridge"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#basic-concepts-of-machine-learning",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#basic-concepts-of-machine-learning",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "Basic concepts of machine learning",
    "text": "Basic concepts of machine learning\nMachine learning is, at its most basic, automated data analysis, usually with the goal of finding patterns or making predictions. The “machines” in this analysis are equations or algorithms and the “learning” is usually some form of parameter selection and/or fitting. Due to the uncertain nature of most data, the majority of these models are probabilistic in nature. In fact, it can be hard to distinguish the methodological lines between what is termed “machine learning” and the field of statistics. However, there are a few important distinctions between the tools, goals, and terminology of the two areas. Today, machine learning has emerged as a broad description of almost any data-driven computing which may or may not include classical descriptive and inferential statistics [1].\nAt first glance, machine learning can be separated into three main classes:\n\nSupervised learning: Given dependent and independent variable data, train a model which effectively maps the independent variable data to produce the dependent variable data.\n\nGeneralized linear models (linear, logistic, etc. regression)\nNaive Bayes\nNeural networks (most)\nSupport vector machine (SVM)\nRandom forests\netc.\n\nUnsupervised learning: Given data, find patterns (no specified output, though there is still a measure of success)\n\nClustering\nMixture models\nDimensionality reduction\nAssociation rules\netc.\n\nReinforcement learning: Given input data and desired outcomes, simulate and use the results to update a model to improve the simulation’s ability to achieve those outcomes\n\nQ-learning\nSARSA\netc.\n\n\nThere is an enormous amount of current interest in machine learning methods and there is a corresponding amount of high-quality material discussing it. We will end the introduction here and direct you to established textbooks [1–3], NJIT classes (Math 478, Math 678, Math 680, CS 675, CS 677), and online resources (too many to even start listing).\n\nGeneral procedure\nIn practice, machine learning algorithms often boil down to an optimization problem. To characterize this in a few steps, consider a problem with data \\(x\\):\n\nSelect a model representation \\(f\\) with parameters \\(p\\) for the problem: \\[\ny = f(x;p)\n\\]\nDetermine an appropriate objective function \\(\\mathcal{L}\\): \\[\n\\mathcal{L}(f(x;p),x)\n\\]\nUse an optimization method \\(\\mathcal{O}\\) with parameters \\(d\\) to find parameters \\(p\\) that minimize or maximize the objective for the model: \\[\np^* = \\mathcal{O}(\\mathcal{L},f,x;d)\n\\]\n\nIn some sense, this is the same procedure used for inverse problems in traditional applied mathematics but with a broader set of models \\(f\\) that may or may not be based on first-principles understanding of the problem.\n\n\nIncorporating data\nDepending on the class of problem considered (supervised, unsupervised, or reinforcement), there are a variety of choices for models \\(f\\) and objectives \\(\\mathcal{L}\\). For supervised learning (the most common), the objective is often to predict or generate the output or dependent variable data of some process. For this, data can be separated into three sets:\n\nTraining data (\\(x\\)): used to tune the parameters \\(p\\)\nValidation data (\\(x^v\\)): used to evaluate the generalization of the model \\(f\\) to data not in the training set during training\nTesting data (\\(x^t\\)): used to benchmark the predictive or generative ability of the model after training is completed"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#a-first-machine-learning-problem",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#a-first-machine-learning-problem",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "A first machine learning problem",
    "text": "A first machine learning problem\n\n\n\n\n\n\nNote\n\n\n\nThe code for this problem will require the following packages: numpy, matplotlib, autograd\n\n\nThese workshops are about learning by doing, so let’s build understanding by fitting a simple “machine” to some data as a supervised problem. Consider some data \\((x,y)\\):    \n\n\nData generation\nimport numpy as np\nimport matplotlib.pyplot as plt\n# To see animations in a Jupyter notebook, uncomment the following line:\n# %matplotlib notebook\ndef f_known(x):\n    part1 = np.exp(-x)*(np.sin((x)**3) + np.sin((x)**2) - x)\n    part2 = 1/(1 + np.exp(-1*(x-1)))\n    return part1 + part2\nxsamples = np.random.uniform(-1/2,5,100)\nysamples = f_known(xsamples)\nplt.scatter(xsamples,ysamples)\nplt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Data\")\nplt.show()\n\n\n\n\n\nWe would like to fit a model of the following form to this data: \\[\nf(x;p_0,p_1) = e^{-p_0x}(\\sin((p_0x)^3) + \\sin((p_0x)^2) - p_0x) + \\frac{1}{1 + e^{-p_1(x-1)}}\n\\] \nTo formulate this as a machine learning/optimization problem, we can consider an objective/loss to minimize the \\(L^2\\) norm distance between the model output \\(f(x)\\) and the true data \\(y\\): \\[\n\\mathcal{L(f(x;\\vec{p}),y)} = ||f(x) - y||_2^2\n\\] The problem can then be written as the unconstrained optimization problem: \\[\np^* = \\underset{\\vec{p}}{\\text{minimize }} \\mathcal{L}(f(x;\\vec{p}),y)\n\\] We then expect our model \\(f(x;p^*)\\) to represent a “machine” that has accurately “learned” the relationship between \\(x\\) and \\(y\\).\nThere are several ways to approach this problem, but a simple and popular approach for a continuous and unconstrained problem is to use an iterative gradient method.\n\nGradient descent\nGradient descent is a straightforward method taught early in an undergraduate numerical methods class. Its simplicity and relatively low computational cost has made it popular for machine learning methods (which can contain enough parameters that second-order methods, like Newton’s method, are infeasibly expensive because of the Hessian computation). Beginning with an initial parameter guess \\(\\vec{p}_0\\), its update procedure can be written as: \\[\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\nv^i &= -\\alpha \\nabla_p \\mathcal{L}\n\\end{align*}\n\\] where \\(\\alpha\\) controls the step size in the direction of the gradient (usually called a “learning rate” in machine learning). This method will follow the gradient of the objective/loss \\(\\mathcal{L}\\) until the objective is sufficiently small, or until it reaches a steady state.\nSimply implemented in Python, this method can be written as:\n\ndef gradient_descent(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    for s in range(steps):\n        v_i = -alpha*f_p(x)\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\nHowever, this method contains a troublesome parameter \\(\\alpha\\) which, if chosen too large, could prevent convergence of the solution or, if chosen too small, could require an unreasonable number of steps to converge. The method itself is also prone to terminate in local minima rather than in a global minimum unless the correct initial guess and learning rate are chosen. For this reason, “vanilla” (or normal) gradient descent is almost always replaced with a modified method in learning problems [4,5].\nThe following demonstrates an animation of the above gradient descent method applied to our data with two different learning rates, one successful, one not. It uses the following animation code and the autograd automatic differentiation library that will be further discussed later:\n\n\nAnimation code\nfrom matplotlib import animation as anim\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import Axes3D\n# To display the animations in a jupyer notebook uncomment the following line:\n# %matplotlib notebook\n\ndef animate_steps_2d(xs,loss,xmin=-.1,xmax=2.5,ymin=-1,ymax=3,interval=50):\n    fig = plt.figure(figsize=(10,6),constrained_layout=True)\n    gs = gridspec.GridSpec(ncols=6,nrows=2,figure=fig)\n    ax = fig.add_subplot(gs[:,0:4],projection=\"3d\")\n    ax1 = fig.add_subplot(gs[0,4:])\n    ax2 = fig.add_subplot(gs[1,4:])\n    ax.view_init(47,47)\n    ax.set_xlabel(\"$p_0$\"); ax.set_ylabel(\"$p_2$\"); ax.set_zlabel(\"loss\",rotation=90)\n    ax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"loss\")\n    ax2.set_xlabel(\"$p_1$\"); ax2.set_ylabel(\"loss\")\n\n    xs_arr = np.array(xs)\n    fxs = np.linspace(xmin,xmax,100)\n    fys = np.linspace(ymin,ymax,100)\n    loss_fx = [loss([fxs[j],xs[0][1]]) for j in range(len(fxs))]\n    loss_fy = [loss([xs[0][0],fys[j]]) for j in range(len(fys))]\n    X,Y = np.meshgrid(fxs,fys)\n    Z = np.zeros_like(X)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Z[i,j] = loss([X[i,j],Y[i,j]])\n    # Add surface plot\n    surf = ax.plot_surface(X,Y,Z,cmap=\"gist_earth\")\n    ax1.set_xlim(np.min(X),np.max(X)); ax1.set_ylim(np.min(Z),np.max(Z))\n    ax2.set_xlim(np.min(Y),np.max(Y)); ax2.set_ylim(np.min(Z),np.max(Z))\n    plot1 = ax.plot(xs[0][0],xs[0][1],loss(xs[0]),zorder=100,color=\"red\",linestyle=\"\",marker=\"o\")[0]\n    plot2 = ax.plot([],[],[],color=\"orange\")[0]\n    # Add flat plots for perspective\n    plot3 = ax1.plot(fxs,loss_fx)[0]\n    plot4 = ax1.scatter(xs[0][0],loss(xs[0]),color=\"red\",s=100,zorder=100)\n    plot5 = ax2.plot(fys,loss_fy)[0]\n    plot6 = ax2.scatter(xs[0][1],loss(xs[0]),color=\"red\",s=100,zorder=100)\n    def anim_func(i):\n        x_loss = loss(xs[i])\n        plot1.set_data_3d(xs[i][0],xs[i][1],x_loss)\n        temp_x1 = [xs[j][0] for j in range(i)]\n        temp_x2 = [xs[j][1] for j in range(i)]\n        temp_losses = [loss(xs[j]) for j in range(i)]\n        plot2.set_data_3d(temp_x1,temp_x2,temp_losses)\n        loss_fx = [loss([fxs[j],xs[i][1]]) for j in range(len(fxs))]\n        loss_fy = [loss([xs[i][0],fys[j]]) for j in range(len(fys))]\n        plot3.set_data(fxs,loss_fx)\n        plot4.set_offsets([xs[i][0],x_loss])\n        plot5.set_data(fys,loss_fy)\n        plot6.set_offsets([xs[i][1],x_loss])\n        plots = [plot1,plot2,plot3,plot4,plot5,plot6]\n        return plots\n\n    tanim = anim.FuncAnimation(fig,anim_func,interval=50,frames=len(xs),blit=True)\n    return tanim\n\n\n\nfrom autograd import grad\nimport autograd.numpy as anp\ndef f_model(p):\n    part1 = anp.exp(-p[0]*xsamples)*(anp.sin((p[0]*xsamples)**3) + anp.sin((p[0]*xsamples)**2) - p[0]*xsamples) \n    part2 = 1/(1 + anp.exp(-p[1]*(xsamples-1)))\n    return part1 + part2\nloss = lambda p: anp.sum((f_model(p) - ysamples)**2)\ngrad_loss = grad(loss) # automatically differentiated\n\n\np0 = np.array([2.1,.2])\nxs = gradient_descent(grad_loss,p0,.005,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n\nAlthough this behavior is somewhat typical of vanilla gradient descent, this model was pathologically chosen to be challenging. The curvature of the loss function for each parameter at the correct parameter values are as follows:\n\n\nPlotting parameter loss curves\nps = np.linspace(-.1,2.5,1000)\nfig,axs = plt.subplots(1,2,sharey=True,figsize=(9,4))\nax1,ax2 = axs\nax1.plot(ps,[loss(np.array([p_i,1])) for p_i in ps])\nax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"Loss\")\nax2.plot(ps,[loss(np.array([1,p_i])) for p_i in ps])\nax2.set_xlabel(\"$p_1$\")\n# plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Model\")\nplt.show()\n\n\n\n\n\nIn this loss, \\(p_0\\) demonstrates a plethora of local minima which could trap the descent algorithm while \\(p_1\\) has a small gradient which will slow down the convergence. The following methods are meant to simplify the choice of a learning rate while overcoming these specific convergence issues.\n\n\nAdaptive steps\nDue to the challenges of determining a good learning rate (especially in models with many parameters and large variance in loss gradients), many methods have been developed to automatically adjust the \\(\\alpha\\) parameter with each step and for each parameter. One of the most common adaptive algorithms is called adagrad (for adaptive gradient. very creative). Originally developed to provide parameter specific learning rates in sparse problems (in the case that some parameters are only occasionally important) it scales the learning rate by a squared sum of previous gradients:\nadagrad: \\[\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\nv^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\nG^i &= \\sum_{j=0}^i (\\nabla_p \\mathcal{L}(x;\\vec{p}^j))^2\n\\end{align*}\n\\]\n\n\nadagrad code\ndef adagrad(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_sq_grad = 0\n    for s in range(steps):\n        sum_sq_grad = f_p(x)**2 + sum_sq_grad\n        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\n\n\np0 = np.array([2.1,.2])\nxs = adagrad(grad_loss, p0,.2,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n However, depending on the problem, this scaled learning rate may slow down convergence considerably. As an adjustment to remove this monotone decreasing learning rate, RMSprop attempts to balance the current gradient with a dampened version of the sum of squares of previous gradients from adagrad:\nRMSprop: \\[\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\nv^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\nG^i &= \\gamma G^{i-1} + (1-\\gamma)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n\\end{align*}\n\\]\n\n\nRMSprop code\ndef rmsprop(f_p,x0,gamma=0.9,alpha=0.001,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_sq_grad = 0\n    for s in range(steps):\n        sum_sq_grad = (1-gamma)*(f_p(x)**2) + gamma*sum_sq_grad\n        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\n\n\np0 = np.array([2.1,.2])\nxs = rmsprop(grad_loss, p0,0.2,.05,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n This helps with the challenge of a monotone decreasing learning rate, but it introduces a dampening parameter \\(\\gamma\\) that must be chosen (recommended values are \\(\\alpha = 0.001\\) and \\(\\gamma = 0.9\\)).\n\n\nWith momentum\nThough the previous adaptive methods address the challenge of determining a learning rate, these methods are still likely to terminate in local minima. To address this issue, there are several methods which utilize a concept of “momentum” to propel iterations out of local minima with the hope of landing in the global minimum. This momentum is most simply added by incorporating previous gradients into the current update:\nGradient descent with momentum: \\[\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n    v^i &= -\\alpha G^i \\\\\n    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i) + \\gamma G^{i-1}\n\\end{align*}\n\\]\n\n\nGradient descent with momentum code\ndef gradient_descent_momentum(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    for s in range(steps):\n        sum_grad = f_p(x) + gamma*sum_grad\n        v_i = -alpha*sum_grad\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\n\n\np0 = np.array([2.1,.2])\nxs = gradient_descent_momentum(grad_loss, p0,.9,.005,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n where \\(\\gamma\\) is a momentum parameter that determines how much of previous updates are kept for the current step (\\(0 <= \\gamma <=1\\) where \\(\\gamma = 0\\) includes no momentum).\nHowever, iterations that include this momentum may jump right out of global minima and/or delay convergence. To incorporate a counterbalance to the momentum based on current success (to slow down in the right places), Nesterov acceleration adjusts the gradient according to an approximated step (to see how successful it may be in the future). By so doing, it can effectively reduce or increase the momentum according to the next future iteration:\nNesterov accelerated gradient descent: \\[\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n    v^i &= -\\alpha G^i \\\\\n    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i - \\gamma v^{i-1}) + \\gamma G^{i-1}\n\\end{align*}\n\\]\n\n\nGradient descent with Nesterov acceleration code\ndef gradient_descent_nesterov(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    for s in range(steps):\n        sum_grad = f_p(x-gamma*v_i) + gamma*sum_grad\n        v_i = -alpha*sum_grad\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\n\n\np0 = np.array([2.1,.2])\nxs = gradient_descent_nesterov(grad_loss, p0,.8,.002,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n where again \\(\\gamma\\) is a momentum parameter. Notice that the only adjustment compared to vanilla gradient descent with momentum is calculating the gradient at an approximation of the next parameter values rather than at the current parameters.\n\n\nCombining ideas\nOf course, the ideas of adaptive gradients and momentum both address important issues and are not mutually exclusive, so they can both be used simultaneously (with the downside of adding more parameters to tune). Note that the adaptive step size methods (adagrad,RMSprop) use a sum of squares of the previous gradient while the momentum methods (gradient descent with momentum or Nesterov acceleration) use a sum of the previous gradient. These can be seen as the second moment and first moment of the previous gradients respectively. The Adam (adaptive moment estimation) method uses both of these gradient moments to incorporate both momentum and adaptivity:\nadam: \\[\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n    v^i &= -\\frac{\\alpha}{\\sqrt{b^i}}m^i \\\\\n    m^i &= \\beta_1m^{i-1} + (1-\\beta_1)\\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\n    b^i &= \\beta_2b^{i-1} + (1-\\beta_2)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n\\end{align*}\n\\]\n\n\nAdam code\ndef adam(f_p,x0,beta1,beta2,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    sum_sq_grad = 0\n    for s in range(1,steps):\n        sum_grad = beta1*sum_grad + (1-beta1)*f_p(x)\n        sum_sq_grad = beta2*sum_sq_grad + (1-beta2)*(f_p(x)**2)\n        v_i = -alpha*sum_grad/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n\n\n\np0 = np.array([2.1,.2])\nxs = adam(grad_loss, p0,0.9,0.99,.05,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n\n Parameters \\(\\beta_1,\\beta_2\\) represent decay rates in incorporating previous moments into the update step.\nAlthough this list is not comprehensive, it demonstrates the reasoning behind the common solutions for the challenges of using gradient descent in learning methods. For a more comprehensive list of recently proposed methods, see [5].\n\n\nAutomatic differentiation\nGradient descent methods rely on computing the gradient of the loss \\(\\nabla_p \\mathcal{L}\\) for a given parameter set \\(\\vec{p}^i\\). For simple models, the gradients can be calculated by hand. However, for models with many nested functions and parameters (neural networks in particular) or methods whose form depends on hyperparameters, we will need an automated method. The “automatic differentiation” method is the solution to these challenges. Though it was developed for neural networks and mainly used there, its breadth of applications is only just becoming apparent. (Consider the Julia language which has worked hard to make automatic differentiation possible everywhere).\nThere are both “forward” and “backward” automatic differentiation methods, but we will consider only the forward which best demonstrates the “automatic” moniker. To do so, consider the first order expansion of two functions at a given point \\(a\\): \\[\n\\begin{align*}\nf(a + \\epsilon) &= f(a) + \\epsilon f'(a) \\\\\ng(a + \\epsilon) &= g(a) + \\epsilon g'(a)\n\\end{align*}\n\\] where \\(\\epsilon\\) is a small perturbation. Basic operations with these functions at this point can then be written as: \\[\n\\begin{align*}\nf + g &= [f(a) + g(a)] + \\epsilon[f'(a) + g'(a)] \\\\\nf - g &= [f(a) - g(a)] + \\epsilon[f'(a) - g'(a)] \\\\\nf \\cdot g &= [f(a) \\cdot g(a)] + \\epsilon[f(a)\\cdot g'(a) + g(a)\\cdot f'(a)] \\\\\nf \\div g &= [f(a) \\div g(a)] + \\epsilon\\left[\\frac{f'(a)\\cdot g(a) + f(a)\\cdot g'(a)}{g(a)^2}\\right]\n\\end{align*}\n\\] The derivatives are then represented by the \\(\\epsilon\\) terms in the above equalities.\nTo implement this on a computer, you can create a new “Dual” number that performs additions, subtractions, multiplications, and divisions with the above operating rules (and can be extended for other functions: \\(\\sin,\\cos,\\exp\\),etc).\n\n\n\n\n\n\nNote\n\n\n\nThis is cumbersome and slow to implement in Python which is why large and complicated libraries have been written in C (tensorflow,pytorch,jax, and others).\nIt can be cleanly and easily implemented in Julia. To see a demonstration of this, see [6].\n\n\nA quick demonstration of a simple implementation of this is as follows [7]. Define a dual number class:\n\nclass DualNumber:\n    def __init__(self, val, der):\n        self.val = val\n        self.der = der\n    def __add__(self, other):\n        if isinstance(other, DualNumber):\n            # DualNumber + DualNumber\n            return DualNumber(self.val + other.val, self.der + other.der)\n        else:\n            # DualNumber + a number\n            return DualNumber(self.val + other, self.der)\n    def __radd__(self, other):\n        # a number + DualNumber\n        return self.__add__(other)\n    def __mul__(self,other):\n        if isinstance(other, DualNumber):\n            # DualNumber * DualNumber\n            return DualNumber(self.val * other.val, self.val*other.der + self.der*other.val)\n        else:\n            # DualNumber * a number\n            return DualNumber(self.val * other, self.der * other)\n    def __rmul__(self,other):\n        # a number * DualNumber\n        return self.__mul__(other)\n    def __pow__(self,power):\n        # DualNumber ^ a number\n            return DualNumber(self.val ** power, self.der * power * (self.val ** (power - 1)))\n    def __repr__(self):\n        # Printing a DualNumber\n        return \"DualNumber({},{})\".format(self.val,self.der)\ndual1 = DualNumber(1,2); dual2 = DualNumber(3,4); other = 5\nprint(dual1,\"+\",dual2,\"=\",dual1+dual2)\nprint(dual1,\"+\",other,\"=\",dual1+other)\nprint(dual1,\"*\",dual2,\"=\",dual1*dual2)\nprint(dual1,\"*\",other,\"=\",dual1*other)\nprint(dual1,\"^\",2,\"=\",dual1**2)\n\nDualNumber(1,2) + DualNumber(3,4) = DualNumber(4,6)\nDualNumber(1,2) + 5 = DualNumber(6,2)\nDualNumber(1,2) * DualNumber(3,4) = DualNumber(3,10)\nDualNumber(1,2) * 5 = DualNumber(5,10)\nDualNumber(1,2) ^ 2 = DualNumber(1,4)\n\n\nThis seems simple, but by using a few rules like this, we can compute simple polynomial derivatives at a point automatically. For example, computing the derivative of \\(f(x) = 3x^3 + 2x + 4\\) at \\(x=2\\). We first initialize the function and the dual number \\((2,1)\\) (\\(1\\) because the derivative of \\(x\\) is \\(1\\)).\n\nf = lambda x: 3*x**3 + 2*x + 4\ndf = lambda x: 9*x**2 + 2\nx = 2; dualx = DualNumber(x,1)\n\nWe then pass the dual number through the polynomial function, unpack the results, and compare the result with the true derivative \\(f'(2)\\):\n\npoly_dual = f(dualx)\ndual_val = poly_dual.val; dual_der = poly_dual.der\ntrue_der = df(x)\nprint(\"True derivative at x=2: \", true_der)\nprint(\"Dual number derivative: \", dual_der)\n\nTrue derivative at x=2:  38\nDual number derivative:  38\n\n\nUsing the rules for dual numbers we provided above, the derivative “automatically” popped out of the polynomial evaluation.\nIn contrast with the forward method, the backward method keeps a log of the operations performed and then uses defined chain rules (similar to how we defined rules for the forward pass) to backtrack from the final output back to the start. This approach is more efficient when the dimension of the gradient is large and that of the output small. Forward evaluation is more efficient when the dimension of the gradient is small and that of the output large.\nAll in all, automatic differentiation is programming with hard-coded differential rules for primitive operations. It can also require keeping track of which operations happen and in what order. Implementing such a method relies heavily on computer science techniques since it boils down to parsing operation calls. This represents one of the clearest differences in the approaches of machine learning to those of statistics or optimization due to its computer language-heavy principles. Through this lens, machine learning could be viewed as “inferential statistics using new tools from computer science for non-traditional problems.”"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#further-exploration",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#further-exploration",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "Further Exploration",
    "text": "Further Exploration\n\nEach of the parameters for the gradient descent methods was chosen by hand for the above animations. Try adjusting the starting point and parameters for each to get a feel for how they behave.\nTry splitting the sample data into a training and a testing set. Use the training set and one of the gradient descent methods to fit some parameters, then see how well the model generalizes to the testing set with those parameters.\nTry implementing another gradient descent method from [4] using the given methods as a template.\nAdd the missing subtraction (__sub__, __rsub__) and division (__truediv__, __rtruediv__) methods in the DualNumber class and play around with automatically differentiation functions using those operators (you can use this as a reference). You can also make functions such as sin,cos, or exp that are meant to compute with DualNumbers."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#appendix",
    "href": "other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html#appendix",
    "title": "Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop2_nn/workshop2_nn.html",
    "href": "other/mlseminar/fall_2022/workshop2_nn/workshop2_nn.html",
    "title": "Workshop 2: Neural networks - structure, building, and trainig",
    "section": "",
    "text": "Learning a simple model\nAs a quick example, we’ll try to fit a wide, shallow neural network to \\(sin(x)\\) on \\([0, 4\\pi]\\). We pick a network with a single layer, using ReLU as the activation function. We create a set of sample data \\(\\overline{X},\\overline{Y}\\), then try to fit the network \\(N(x_i,\\beta) \\approx y_i\\) \\[\n\\beta^* = \\text{argmin} \\sum (N(x_i;\\beta)-y_i)^2\n\\] From the universal approximation theorem, we know that some network exists which can approximate this curve to any precision, however it’s unclear that the network we discover from our optimization problem will be that network.\n\n# Generate sin data\nxmin_s = 0\nxmax_s= 4*np.pi\nnx_s = 100\nx_s= np.linspace(xmin_s,xmax_s,nx_s)\ny_s= np.sin(x_s)\n\n# Build a model\nmodel_s = Sequential()\nmodel_s.add(Dense(700, input_shape=(1,), activation='relu'))\nmodel_s.add(Dense(1))\nmodel_s.compile(loss = 'mae',optimizer = 'adam')\nmodel_s.fit(x_s,y_s,epochs=10000,batch_size = 25, verbose = 0)\n\n# Make Prediction\ny_pred_s = model_s.predict(x_s) # Prediction with training data\nx_test_s = np.linspace(xmin_s,2*xmax_s,4*nx_s)\ny_test_s = model_s.predict(x_test_s)\n\n# Plot Results\nplt.figure(figsize = (12,4))\nplt.plot(x_test_s,y_test_s)\nplt.subplot(1,3,1)\nplt.plot(x_s,y_s)\nplt.subplot(1,3,2)\nplt.plot(x_s,y_pred_s)\nplt.subplot(1,3,3)\nplt.plot(x_test_s,y_test_s)\n\n1/4 [======>.......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 2ms/step\n\n\n 1/13 [=>............................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/13 [==============================] - 0s 1ms/step\n\n\n/tmp/ipykernel_77088/2768104420.py:23: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,1)\n\n\n\n\n\n\n\nLearning a PDE\nWe consider a simple 1D transport equation with periodic boundary conditions\n\\[\\begin{cases}\nu_t = u_x, \\quad (x,t) \\in [0,L]\\times(0,\\infty)\n\\\\\nu(x,0) = u_0(x)\n\\\\\nu(x,t) = u(x+L,t)\n\\end{cases}\\]\nThe true solution to this PDE is given by \\(u(x,t) = u_0(\\text{mod}(x-t,1))\\), which can be found using the method of characteristics on the free domain and then truncating to a periodic domain. Though we have access to the true solution, we will generate our training data using a prepackaged numerical PDE solver (py-pde). We will use a Gaussian as our initial data \\(u_0(x) = e^{-100(x-.3)^2}\\) and solve on the domain \\([0,1]\\).\nNote that this isn’t an attempt to solve the PDE using NNs, rather we are just using the PDE data as a specific set of training data.\n\nimport pde\n\n# Domain\nxmin = 0.0\nxmax = 1.0\nnx = 101\ntmin = 0.0\ntmax = 1.0\ndt = 1e-6\nsave_dt = 0.01\ninit_cond = \"1*exp(-(1/.01)*(x-0.3)**2)\"\n\n# Initialize objects\ngrid = pde.CartesianGrid([(xmin,xmax)],nx,periodic=True)\nh = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\neq = pde.PDE({\"h\": \"-d_dx(h)\"})\nstorage = pde.MemoryStorage()\n\n# Run\nresult = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt))\n\n# Save data\ndata = np.array(storage.data)\nnp.save(\"simple_wave.npy\", data)\n\n\n\nData Processing\nArugably the most important part of training a machine learning algorithm is the data. Most prepackaged algorithms expect data to be formatted in a specific way, usually as an array where each column represents different features and each row represents the samples. As it stands, we have our target \\(h(x,t)\\) data represented as a matrix, and have our \\(x,t\\) each represented as single arrays. We need create an array of each pair of \\(x,t\\) data points, and map the \\(h(x,t)\\) to array of the corresponding values.\n\n# Create training data\nx = np.linspace(xmin,xmax,nx)\nnt = int((tmax-tmin)/save_dt+1)\nt = np.linspace(tmin,tmax,nt)\nT = np.repeat(t,nx)\nX = np.tile(x,int(nt))\nXT = np.transpose(np.array([X,T]))\ny = np.transpose(data.reshape(nt*nx))\n\n\n\nBuilding a Network\nWe’ll use the Keras package to build our Neural Networks. Keras is an API for building Neural Networks built on tensforslow. We can intitialize the network using the Sequential() class, then add layers .add() method for our model. We will use Dense layers, which means that each node takes inputs from all of the other nodes in the previous layer. We can specify the initial input shape in the first layer, and each size and activation function for each layer.\n\n# Build model\nmodel = Sequential()\nmodel.add(Dense(12, input_shape=(2,), activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n\n# Train model\nmodel.fit(XT,y,epochs=1000, batch_size=200, verbose = 0)\nyPred = np.array(model.predict(XT)).reshape(nt,nx)\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\np = plt.imshow(data,aspect = 'auto')\nplt.colorbar()\n\nplt.subplot(1,2,2)\np = plt.imshow(yPred,aspect = 'auto')\nplt.colorbar()\n\n  1/319 [..............................] - ETA: 20s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37/319 [==>...........................] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73/319 [=====>........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b114/319 [=========>....................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b154/319 [=============>................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b201/319 [=================>............] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b245/319 [======================>.......] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b295/319 [==========================>...] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b319/319 [==============================] - 0s 1ms/step\n\n\n<matplotlib.colorbar.Colorbar at 0x7fdddfa6c7c0>\n\n\n\n\n\n\n# Create subset of training data\nt_stop= .5/tmax\nt_split = nx*int(t_stop*nt)\nXT_Train = XT[0:t_split,:]\ny_Train = y[0:t_split]\n\n# Fit new model\nmodel.fit(XT_Train,y_Train,epochs=1000, batch_size=200, verbose = 0)\n\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\np = plt.imshow(y_Train.reshape(int(t_stop*nt),nx))\nplt.colorbar()\n\nplt.subplot(1,2,2)\nyPred = np.array(model.predict(XT)).reshape(nx,nt)\np = plt.imshow(yPred)\nplt.colorbar()\n\n  1/319 [..............................] - ETA: 9s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35/319 [==>...........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67/319 [=====>........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b106/319 [========>.....................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b149/319 [=============>................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b189/319 [================>.............] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b227/319 [====================>.........] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b265/319 [=======================>......] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b309/319 [============================>.] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b319/319 [==============================] - 0s 1ms/step\n\n\n<matplotlib.colorbar.Colorbar at 0x7fdddfe53160>\n\n\n\n\n\n\n\nHyper Parameters\nOne of the many challenges you face when working with Neural Networks is the wide range of hyper parameters you need to choose in order to build the network. Some of the more obvious ones are the number of layers, the depth of each layer, and the activation function you use. It’s clear that these can have dramatic effects on the resulting neural network, but even smaller changes can too. In this example, we double the number of epochs that the network is trained on. This tends to result in a lower quality prediction, likely from overfitting to specific training data. There are ways to better chose hyper parameters and mitigate things like over fitting, but that is beyond the scope of this workshop.\n\nmodel.fit(XT_Train,y_Train,epochs=2000, batch_size=200, verbose = 0)\nyPred = np.array(model.predict(XT)).reshape(nx,nt)\np = plt.imshow(yPred)\nplt.colorbar()\n\n  1/319 [..............................] - ETA: 6s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46/319 [===>..........................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89/319 [=======>......................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b136/319 [===========>..................] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b181/319 [================>.............] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b231/319 [====================>.........] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b280/319 [=========================>....] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b319/319 [==============================] - 0s 1ms/step\n\n\n<matplotlib.colorbar.Colorbar at 0x7fdddfe180d0>"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "",
    "text": "One common hallmark of popular machine learning methods is their “black-box” nature. Since many of these methods are meant solely for prediction, this has not been too much of an issue. After all, a black box method can be as complex as needed since it does not need to be analyzed after the fact. This mentality has given birth to increasingly complex but effective models (just take a look at the model that defeated the worlds best Go player).\nHowever, there has been some recent interest in models that can be understood and analyzed. This is particularly true in the scientific realm, where practicioners looking to use machine learning would like to get an idea of the mechanisms underlying their system of study. In order to do so, new tools have been created and old, interpretable tools, such as linear regression, have been adapted to meet this challenge.\nMany of these new, interpretable, models have been named “data-driven model discovery.” Their goals is to model collected data from a system with machine learning tools to determine a human-readable model."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sparse-identification-of-nonlinear-dynamics",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sparse-identification-of-nonlinear-dynamics",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "Sparse Identification of Nonlinear Dynamics",
    "text": "Sparse Identification of Nonlinear Dynamics\nOne method for model discovery as described above is called Sparse Identification of Nonlinear Dynamics (SINDy) [1]. The goal of this method is to extract the most probable differential equation directly from data of the important state variables of a continuum system.\n\nSetting up linear problem\nAs its name suggests, this method works discover models for linear or nonlinear systems. It is based on a simple idea that nonlinear differential equations can be expressed as a linear combination of nonlinear terms [2]. Assuming we are looking at the nonlinear time evolution of some quantity, this could then be written as the sum of \\(K\\) nonlinear terms: \\[\nu_t(x,t) = \\xi_1\\mathcal{N}_1(u,x,t) + \\ldots + \\xi_K\\mathcal{N}_K(u,x,t)\n\\] If we can then determine what nonlinear terms are possible \\(\\mathcal{N}_i(u,x,t)\\), we can sift through these terms to determine which best contribute to the time evolution of the system.\nUltimately, this boils down to a regression problem. Given some space and time samples of our state variable: \\(u(x_i,t_j)\\) for \\(i \\leq N\\) and \\(j \\leq M\\), we can consider the linear system: \\[\nu_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)\n\\] Expanded for all the data samples (flattened across space and time), this can be written as the system: \\[\n\\begin{bmatrix}\nu_t(x_1, t_1) \\\\\n\\vdots \\\\\nu_t(x_N, t_1) \\\\\n\\vdots \\\\\nu_t(x_N, t_M) \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathcal{N}_1(x_1, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n\\vdots &  & \\vdots \\\\\n\\mathcal{N}_1(x_N, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n\\vdots &  & \\vdots \\\\\n\\mathcal{N}_1(x_N, t_M) & \\ldots & \\mathcal{N}_K(x_1, t_1)\n\\end{bmatrix}\n\\vec{\\xi}\n\\tag{1}\\]\nSolving this system is then a straightforward linear regression.\n\n\nDetermining nonlinear “library” of terms\nDetermining what \\(\\mathcal{N}_i(u,x,t)\\) are reasonable for the system is somewhat of a traditional modeling problem. Are there any symmetries in the system that need to be satisfied? Is there periodic behavior that might warrant inclusion of trignometric terms? What order of polynomial interactions are possible for the system?\nThe most common library of terms for a 1D function is to put together polynomial interactions with spatial derivatives. Such a library up to 3rd order polynomials and derivatives could be written: \\[\n\\begin{align*}\n\\mathcal{N}_1(u,x,t) &= u\\\\\n\\mathcal{N}_2(u,x,t) &= u^2\\\\\n&\\vdots \\\\\n\\mathcal{N}_i(u,x,t) &= u_x\\\\\n\\mathcal{N}_{i+1}(u,x,t) &= u_x^2\\\\\n&\\vdots \\\\\n\\mathcal{N}_K(u,x,t) &= u^3u_{xxx}\\\\\n\\end{align*}\n\\]\n\n\nNumerical differentiation of the terms\nIn order to actually compute the values in the linear system written in Equation 1, we must compute numerical derivatives in both \\(t\\) and in \\(x\\). This isn’t an issue if we have smooth, reliable data and can be quickly computed with finite differences.\nHowever, the intent of this method is to use data samples \\(u(x_i,t_j)\\) that are collected from the real world, implying that they will each be polluted with some level of noise. There have been several classical methods presented for dealing with numerical differentiation of noisy data that could be used, but generally the methods revolve around an approximate fitting of a differentiable function basis to the data. Notable among these are:\n\nLocal polynomial regression (LOESS [3], Savitsky-Golay filter [4], etc.)\nRadial basis functions (Gaussian kernel)\nSmoothing splines\nLeast squares spectral analysis (LSSA)\n\nThese can be written along the lines of: \\[\n\\underset{\\vec{c}}{\\text{argmin}} \\; \\sum_{i,j}^{N,M}\\|u(x_i,t_j) - F(x_i,t_j,\\vec{c})\\|_2\n\\] where \\[\nF(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\phi_l(x_i,t_j)\n\\] and \\(\\phi\\) represents our chosen basis function. Once computed, we can easily approximate derivatives of \\(u\\) via: \\[\nu_x(x_i,t_j) \\approx F_x(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\frac{d}{dx}\\phi_l(x_i,t_j)\n\\]\nEach of these has the goal of smoothing the given data while simultaneously providing an exact derivative of the approximation. This is a similar idea as we have discussed with automatic differentiation of neural networks. In fact, you could consider fitting a neural network to be the same as fitting a randomly initialized nested basis of nonlinear functions (since they are dense according to the universal approximation theorem). We will explore this idea in the example problem in Section 3.\n\n\nSparse regression\nOnce the matrix in Equation 1 has been created using numerical differentiation, it remains to sift through the nonlinear terms to determine which, if any, contribute to the time evolution of our state variable of interest. It is usually reasonable to consider that not all the nonlinear terms should be included in the equation, so we would like to determine the most parsimonious (smallest) combination of them that will capture our desired qualitative and quantitative behavior in the system.\nThere are two main families of sparse regression methods:\nGreedy methods: Iterative add/remove terms that best match the time derivative in some metric (\\(R^2\\) coefficient of determination, Akaike Information Criteria (AIC), etc.).\n\nForward selection: Start with no terms, add one by one according to which maximizes \\(R^2\\) or AIC at each step\nBackward selection: Start with all terms, remove one by one according to which least reduces \\(R^2\\) or AIC\n(Orthogonal) Matching pursuit: Start with no terms, add one by one according to which maximizes correlation (orthogonalizing after each step)\n\nRegularization methods: Add a penalty to the regression for having too many terms or large coefficients \\(\\xi_i\\). These can be written roughly as: \\[\n\\underset{\\vec{\\xi}}{\\text{argmin}}\\; \\|u_t(x_i,t_j) - \\mathbf{\\mathcal{N}}(u_{ij},x_i,t_j) \\cdot \\vec{\\xi}\\|_2^2 + \\lambda \\|\\xi\\|_C\n\\]\n\nRidge regression: Let \\(C=2\\) forcing coefficients \\(\\vec{\\xi}\\) to be smaller. We hope that important coefficients will remain larger while unimportant ones shrink.\nLasso regression: Let \\(C=1\\) forcing coefficients \\(\\vec{\\xi}\\) to be smaller and various to be set to 0 (due to the geometry of the 1-norm).\n0-norm regression: Let \\(C=0\\) which is a measure that counts the number of nonzero coefficients in \\(\\vec{\\xi}\\). Computing this usually requires a combination of regularization and relaxation best captured by the SR3 method [5].\n\nCombinations of these two methods which iterative perform regularization methods removing terms with small coefficients according to a given threshold have also been proposed (Sequential Threshold Ridge Regression [6] or the original SINDy algorithm [1]).\n\n\nSummary of the method\nIn summary, the procedure to use SINDy is as follows:\n\nCollect sample points of a continuum state variable of interest \\(u(x_i,t_j)\\)\nForm a “library” of possible terms for the differential model of the system \\(\\mathcal{N}_k(u,x,t)\\)\nCompute the libary at sample points using noise robust numerical differentiation to compute both \\(u_t(x_i,t_j)\\) and \\(\\mathcal{N}_k(u_{ij},x_i,t_j)\\)\nUse sparse regression to determine a sparse vector \\(\\vec{\\xi}\\) which closely approximates \\(u_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)\\)\n\nTo really explore this method, we will walk through this process using simulated traveling wave data in Section 3 and using real extracted data in Section 5."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-simulated",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-simulated",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "Application to simulated wave data",
    "text": "Application to simulated wave data\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop you will need to install the following packages:\nmamba install numpy matplotlib py-pde sympy jax optax flax scikit-learn scikit-image av\n\n\nGiven some data generated via finite differences of the simple advection equation: \\[\nh_t(x,t) = h_x(x,t)\n\\] with periodic boundaries and a Gaussian initial condition, we have the following measurement of state variable \\(h\\) (height of the wave):\n\n\nGenerate simple wave data\nimport numpy as np\nimport pde\nimport matplotlib.pyplot as plt\n\n# Domain\nxmax = 1.0\nnx = 100\ndt = 1e-6\ntmax = 1.0-2*dt\nsave_dt = 0.01\ninit_cond = \".1*exp(-(1/.01)*(x-0.3)**2)\"\n\ngrid = pde.CartesianGrid([(0.0,xmax)],nx,periodic=True)\nh = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\neq = pde.PDE({\"h\": \"-d_dx(h)\"})\nstorage = pde.MemoryStorage()\n\nresult = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt),ret_info=False)\n\n# pde.plot_kymograph(storage)\nmovie = pde.visualization.movie(storage,\"simple_wave.gif\")\n\nh=np.array(storage.data)\nx=storage.grid.coordinate_arrays[0]\nt=np.array(storage.times)\nnp.savez(\"simple_wave.npz\",h=h,x=x,t=t)\nplt.close()\n\n\n\n\n\n\n\nGenerating nonlinear library\nGenerating a library can be most easily accomplished using the sympy symbolic math Python library. To be overly thorough, we will generate up to 4th order polynomial combinations of up to 4th order spatial derivatives.\nWe can first initialize our spatial and state variables:\n\nimport sympy as sp\n\nx_sym,t_sym = sp.symbols(\"x t\")\nh_sym = sp.Function(\"h\")\n\nGiven a specified order, we can now create symbolic derivative terms (constructed to be most legible):\n\n# Library parameters\nmax_poly_order = 4\nmax_diff_order = 4\n\ndiff_terms = [h_sym(x_sym,t_sym)]\ndiff_terms += [sp.Function(str(h_sym)+\"_\"+(i*str(x_sym)))(x_sym,t_sym) for i in range(1,max_diff_order+1)]\nprint(diff_terms)\n\n[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t)]\n\n\nNow, combining these into polynomials up to 4th order (again, this is overkill, but for a system you don’t fully understand, you may want to have a very complete library):\n\nfrom itertools import combinations_with_replacement\n\nterms = []\nfor po in range(max_poly_order+1):\n    if po == 0:\n        term = sp.core.numbers.One()\n    else:\n        combos = combinations_with_replacement(diff_terms,po)\n        for combo in combos:\n            term = 1\n            for combo_term in combo:\n                term *= combo_term\n            terms.append(term)\nprint(terms)\n\n[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t), h(x, t)**2, h(x, t)*h_x(x, t), h(x, t)*h_xx(x, t), h(x, t)*h_xxx(x, t), h(x, t)*h_xxxx(x, t), h_x(x, t)**2, h_x(x, t)*h_xx(x, t), h_x(x, t)*h_xxx(x, t), h_x(x, t)*h_xxxx(x, t), h_xx(x, t)**2, h_xx(x, t)*h_xxx(x, t), h_xx(x, t)*h_xxxx(x, t), h_xxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t), h_xxxx(x, t)**2, h(x, t)**3, h(x, t)**2*h_x(x, t), h(x, t)**2*h_xx(x, t), h(x, t)**2*h_xxx(x, t), h(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_xxx(x, t)**2, h(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xxxx(x, t)**2, h_x(x, t)**3, h_x(x, t)**2*h_xx(x, t), h_x(x, t)**2*h_xxx(x, t), h_x(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxxx(x, t)**2, h_xx(x, t)**3, h_xx(x, t)**2*h_xxx(x, t), h_xx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)*h_xxxx(x, t)**2, h_xxx(x, t)**3, h_xxx(x, t)**2*h_xxxx(x, t), h_xxx(x, t)*h_xxxx(x, t)**2, h_xxxx(x, t)**3, h(x, t)**4, h(x, t)**3*h_x(x, t), h(x, t)**3*h_xx(x, t), h(x, t)**3*h_xxx(x, t), h(x, t)**3*h_xxxx(x, t), h(x, t)**2*h_x(x, t)**2, h(x, t)**2*h_x(x, t)*h_xx(x, t), h(x, t)**2*h_x(x, t)*h_xxx(x, t), h(x, t)**2*h_x(x, t)*h_xxxx(x, t), h(x, t)**2*h_xx(x, t)**2, h(x, t)**2*h_xx(x, t)*h_xxx(x, t), h(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxx(x, t)**2, h(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxxx(x, t)**2, h(x, t)*h_x(x, t)**3, h(x, t)*h_x(x, t)**2*h_xx(x, t), h(x, t)*h_x(x, t)**2*h_xxx(x, t), h(x, t)*h_x(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t)**2, h(x, t)*h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xx(x, t)**3, h(x, t)*h_xx(x, t)**2*h_xxx(x, t), h(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxx(x, t)**3, h(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxxx(x, t)**3, h_x(x, t)**4, h_x(x, t)**3*h_xx(x, t), h_x(x, t)**3*h_xxx(x, t), h_x(x, t)**3*h_xxxx(x, t), h_x(x, t)**2*h_xx(x, t)**2, h_x(x, t)**2*h_xx(x, t)*h_xxx(x, t), h_x(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxx(x, t)**2, h_x(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxxx(x, t)**2, h_x(x, t)*h_xx(x, t)**3, h_x(x, t)*h_xx(x, t)**2*h_xxx(x, t), h_x(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxx(x, t)**3, h_x(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxxx(x, t)**3, h_xx(x, t)**4, h_xx(x, t)**3*h_xxx(x, t), h_xx(x, t)**3*h_xxxx(x, t), h_xx(x, t)**2*h_xxx(x, t)**2, h_xx(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)**2*h_xxxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)**3, h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_xx(x, t)*h_xxxx(x, t)**3, h_xxx(x, t)**4, h_xxx(x, t)**3*h_xxxx(x, t), h_xxx(x, t)**2*h_xxxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t)**3, h_xxxx(x, t)**4]\n\n\n\n\nApproximating data\nIn order to provide numerical derivatives of our data, we will use a neural network approximation.\n\n\n\n\n\n\nNote\n\n\n\nThis is far beyond what is necessary for this particular setting, but is a method that can generalize to data not on a uniform grid and in high dimension, which can be useful. The lack of requirement for a grid can also help with robustly fitting to noisy data by using a train-test methodology in Section 4 which classical basis functions do not handle well. Using neural networks in this way as a combination with SINDy is explored more in [7].\n\n\nTo begin, we will be using the Google developed flax neural network framework which is built on their jax automatic differentiation library and the optax optimization library. The reason for this will become clearer when we consider taking a fourth order derivative in \\(x\\) of the network, a task which many other popular frameworks (pytorch, keras, tensorflow, etc.) cannot do (at least not nearly as concisely). However, the jax library is state-of-the-art for automatic differentiation and is used heavily for differentiable programming and neural network research today (see Appendix for more information).\n\nCreating the neural network model\nFirst, we will create a simple dense neural network model using the \\(\\tanh\\) activation (to ensure a smooth approximation):\n\nimport flax.linen as nn\n\nclass MyNet(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(60)(x)\n        x = nn.tanh(x)\n        x = nn.Dense(12)(x)\n        x = nn.tanh(x)\n        x = nn.Dense(1)(x)\n        return x\n\nThis model will take an input of \\((x_i,t_j)\\) (a dimension 2 array), linearly map it to a dimension 60 space, apply a tanh activation, linearly map to a dimension 12 space, apply a tanh activation, then linearly map to a dimension 1 output (this particular width and depth was chosen arbitrarily).\nWe next initialize the parameters of the network (each of the linear transformation matrices) and print out the dimensions of the corresponding arrays:\n\nimport jax\njax.config.update(\"jax_platform_name\", \"cpu\")\n\n# Random generator seed\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel1 = MyNet()\nparams1 = model1.init(rng2,random_data)\nprint(jax.tree_util.tree_map(lambda x: x.shape, params1))\n\nFrozenDict({\n    params: {\n        Dense_0: {\n            bias: (60,),\n            kernel: (2, 60),\n        },\n        Dense_1: {\n            bias: (12,),\n            kernel: (60, 12),\n        },\n        Dense_2: {\n            bias: (1,),\n            kernel: (12, 1),\n        },\n    },\n})\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe confusing tree_util.tree_map command is a convenience function for mapping a function (in this case lambda x: x.shape) across a set of different objects. This is useful because these objects can be arrays, dictionaries, lists, classes (i.e. other neural networks), etc.\n\n\n\n\nLoading and processing data\nIn order to fit this model to the data, we must load the data into batches of \\((x_i,t_j,u(x_i,t_j))\\) points. Since our data is known to be quite smooth and we want to maximize the fit, we will use batches of size 10000:\n\nimport jax.numpy as jnp\n\ndef load_data(data_path,noise_scale=0,norm=True):\n    raw_data = np.load(data_path)\n    h = raw_data[\"h\"].astype(jnp.float32)\n    x = raw_data[\"x\"].astype(jnp.float32)\n    t = raw_data[\"t\"].astype(jnp.float32)\n\n    # Add noise if needed\n    h += noise_scale*jnp.std(h)*np.random.normal(size=h.shape)\n\n    # Mean center, std center data\n    if norm:\n        h = (h - jnp.mean(h)) / jnp.std(h)\n        x = (x - jnp.mean(x)) / jnp.std(x)\n        t = (t - jnp.mean(t)) / jnp.std(t)\n    return x,t,h\n\ndef batch_data(x,t,h,batch_size):\n    # Split data into batches\n    data = []\n    for i in range(0,len(x),batch_size):\n        temp_xt = jnp.vstack((x[i:i+batch_size], t[i:i+batch_size])).T\n        temp_h = h[i:i+batch_size].reshape((-1,1))\n        data.append((temp_xt,temp_h))\n    return data\n\nx,t,h = load_data(\"simple_wave.npz\")\nX,T = jnp.meshgrid(x,t)\ndata = batch_data(X.flatten(),T.flatten(),h.flatten(),10000)\n\nNote that the data needed to be centered and scaled to have a mean of \\(\\bar{h}=0\\) and standard deviation of \\(\\overline{(h - \\bar{h})}=1\\) in order to best use the \\(\\tanh\\) activation (which extends from -1 to 1).\n\n\nTraining the model\nWe will use the mean squared error fit of the data to our neural network output (just in time compiled with @jax.jit for maximum speed):\n\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model1.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\nWith this loss defined, we initialize an ADAM optimizer and optimizer state and wrap the loss function to return both the output and gradient:\n\nimport optax\n\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params1)\n\nWe can now train the model to take in \\((x_i,t_j)\\) and output \\(u(x_i,t_j)\\). Performing 1000 iterations over the data, we will print the mean squared error on the data as we proceed with the training:\n\nepochs = 1000\nall_xt = jnp.array([data[i][0] for i in range(len(data))])\nall_h = jnp.array([data[i][1] for i in range(len(data))])\nfor i in range(epochs):\n    xt_batch = data[i%len(data)][0]\n    h_batch = data[i%len(data)][1]\n    loss_val, grads = loss_grad_fn(params1, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params1 = optax.apply_updates(params1, updates)\n    if i % 100 == 0:\n        train_loss = mse(params1,all_xt,all_h)\n        print(\"Training loss step {}: {}\".format(i,train_loss))\n\nTraining loss step 0: 1.0898101329803467\n\n\nTraining loss step 100: 0.16434670984745026\n\n\nTraining loss step 200: 0.09155044704675674\n\n\nTraining loss step 300: 0.01834406703710556\n\n\nTraining loss step 400: 0.0012183982180431485\n\n\nTraining loss step 500: 0.00044023292139172554\n\n\nTraining loss step 600: 0.00026629428612068295\n\n\nTraining loss step 700: 0.00018376149819232523\n\n\nTraining loss step 800: 0.00013974880857858807\n\n\nTraining loss step 900: 0.00011441211245255545\n\n\nAs you can tell, this procedure is somewhat more manual than other libraries such as keras but keep you closer to the details, allowing for more flexibility in implementation.\n\n\nValidating fit\nThe fit to the model can be visualized as follows:\n\nimport matplotlib.animation as anim\n\nX,T = jnp.meshgrid(x,t)\nxt_points = jnp.vstack([X.flatten(),T.flatten()]).T\nhhat1 = model1.apply(params1,xt_points).reshape(X.shape)\ndiff = np.sqrt((h - hhat1)**2)\n\ndef animate_data(x,t,data_list,labels):\n    fig = plt.figure()\n    plt.xlabel(\"$x$\")\n    plots = []\n\n    for i in range(len(data_list)):\n        plot = plt.plot(x,data_list[i][0,:],label=labels[i])[0]\n        plots.append(plot)\n\n    def anim_func(j):\n        for i in range(len(plots)):\n            plots[i].set_ydata(data_list[i][j,:])\n        return plots\n\n    plt.legend()\n    approx_anim = anim.FuncAnimation(fig, anim_func, range(len(t)))\n    return approx_anim\n\nanimation1 = animate_data(x,t,[h,hhat1,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation1.save(\"clean_h_compare.gif\")\nplt.close()\n\n\n\n\n\nNumerically differentiating the neural network model\nThe original reason to fit this model to the data was to be able to construct each of the terms in our nonlinear libary for the system. In order to differentiate the model, we must wrap it in a function that takes our inputs and returns the output.\n\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model1.apply(params1, new_x)[0]\n\n# Take a derivative with respect to the first input (x) at point (x_i,t_j)\nx_i = 0.3; t_j = 0.3\njax.grad(model_for_diff,0)(x_i,t_j)\n\nDeviceArray(0.01142649, dtype=float32, weak_type=True)\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we were to differentiate the model directly, we would compute derivatives for all the parameters! This is the main challenge with using other neural network frameworks for this kind of function approximation.\n\n\nApplying this iteratively, we can construct derivatives \\(h_x(x,t), \\ldots, h_{xxxx}(x,t)\\) as is required by our library:\n\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))\n\nWe can then reconstruct our terms attaching them to their corresponding values on our \\((x,t)\\) grid:\n\ndef construct_terms(diff_term_values):\n    term_values = {}\n    term_shape = np.shape(diff_term_values[list(diff_term_values.keys())[0]])\n    for order in range(max_poly_order+1):\n        if order == 0:\n            term = sp.core.numbers.One()\n            term_values[term] = np.ones(term_shape)\n        else:\n            combos = combinations_with_replacement(diff_terms,order)\n            for combo in combos:\n                term = 1\n                temp_term_value = 1\n                for combo_term in combo:\n                    term *= combo_term\n                    temp_term_value *= diff_term_values[combo_term]\n                term_values[term] = temp_term_value\n    return term_values\nterm_values = construct_terms(diff_term_values)\n\nFinally, we compute the derivative of the network with respect to time:\n\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))\n\n\n\nSolving the sparse regression problem\nIn order to cleanly work with our term library, we will use a very popular Python data science package called pandas. Simply put, this library allows you to easily load, manipulate, and save tabular data. Here is our library as a pandas DataFrame:\n\nimport pandas as pd\n\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\nterm_matrix\n\n\n\n\n\n  \n    \n      \n      \n      1\n      h(x, t)\n      h_x(x, t)\n      h_xx(x, t)\n      h_xxx(x, t)\n      h_xxxx(x, t)\n      h(x, t)**2\n      h(x, t)*h_x(x, t)\n      h(x, t)*h_xx(x, t)\n      h(x, t)*h_xxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n    \n      x\n      t\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      -1.71\n      -1.71\n      1.0\n      -0.585482\n      0.032624\n      0.299084\n      3.230795\n      30.858774\n      0.342789\n      -0.019101\n      -0.175108\n      -1.891572\n      ...\n      85.180908\n      10.086037\n      96.336273\n      920.151062\n      8.788777e+03\n      108.952553\n      1040.654907\n      9.939764e+03\n      9.493916e+04\n      9.068066e+05\n    \n    \n      -1.68\n      -1.71\n      1.0\n      -0.584148\n      0.045154\n      0.431638\n      4.487691\n      42.300426\n      0.341229\n      -0.026377\n      -0.252140\n      -2.621474\n      ...\n      333.371216\n      39.011093\n      367.713806\n      3466.025391\n      3.267033e+04\n      405.594208\n      3823.081543\n      3.603590e+04\n      3.396700e+05\n      3.201688e+06\n    \n    \n      -1.65\n      -1.71\n      1.0\n      -0.582291\n      0.063117\n      0.615363\n      6.208451\n      57.811291\n      0.339062\n      -0.036753\n      -0.358320\n      -3.615123\n      ...\n      1265.576294\n      147.258820\n      1371.231323\n      12768.506836\n      1.188966e+05\n      1485.706909\n      13834.469727\n      1.288226e+05\n      1.199558e+06\n      1.116994e+07\n    \n    \n      -1.61\n      -1.71\n      1.0\n      -0.579688\n      0.088592\n      0.868977\n      8.553091\n      78.522736\n      0.336039\n      -0.051356\n      -0.503736\n      -4.958127\n      ...\n      4655.944824\n      543.723083\n      4991.718750\n      45827.109375\n      4.207215e+05\n      5351.707520\n      49132.027344\n      4.510628e+05\n      4.141040e+06\n      3.801734e+07\n    \n    \n      -1.58\n      -1.71\n      1.0\n      -0.576034\n      0.124417\n      1.217481\n      11.722495\n      105.593773\n      0.331815\n      -0.071668\n      -0.701310\n      -6.752550\n      ...\n      16527.279297\n      1961.203003\n      17666.103516\n      159132.562500\n      1.433433e+06\n      18883.400391\n      170097.718750\n      1.532204e+06\n      1.380178e+07\n      1.243235e+08\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1.58\n      1.71\n      1.0\n      -0.593596\n      -0.031916\n      0.124317\n      0.865784\n      5.410497\n      0.352357\n      0.018945\n      -0.073794\n      -0.513926\n      ...\n      0.452417\n      0.080679\n      0.504183\n      3.150766\n      1.968991e+01\n      0.561873\n      3.511281\n      2.194287e+01\n      1.371264e+02\n      8.569366e+02\n    \n    \n      1.61\n      1.71\n      1.0\n      -0.594622\n      -0.027050\n      0.157993\n      1.092908\n      7.853273\n      0.353575\n      0.016084\n      -0.093946\n      -0.649867\n      ...\n      1.539486\n      0.206247\n      1.482023\n      10.649325\n      7.652252e+01\n      1.426704\n      10.251823\n      7.366621e+01\n      5.293410e+02\n      3.803669e+03\n    \n    \n      1.65\n      1.71\n      1.0\n      -0.595456\n      -0.020860\n      0.201213\n      1.422060\n      11.352819\n      0.354567\n      0.012421\n      -0.119814\n      -0.846774\n      ...\n      5.218203\n      0.578643\n      4.619516\n      36.879261\n      2.944204e+02\n      4.089518\n      32.648094\n      2.606415e+02\n      2.080795e+03\n      1.661173e+04\n    \n    \n      1.68\n      1.71\n      1.0\n      -0.596046\n      -0.012949\n      0.258202\n      1.896693\n      16.339481\n      0.355271\n      0.007718\n      -0.153901\n      -1.130517\n      ...\n      17.799061\n      1.761778\n      15.177228\n      130.747604\n      1.126354e+03\n      12.941596\n      111.488258\n      9.604404e+02\n      8.273929e+03\n      7.127759e+04\n    \n    \n      1.71\n      1.71\n      1.0\n      -0.596326\n      -0.002742\n      0.335011\n      2.578223\n      23.413252\n      0.355605\n      0.001635\n      -0.199776\n      -1.537462\n      ...\n      61.523586\n      5.741437\n      52.138901\n      473.481598\n      4.299761e+03\n      44.185734\n      401.257629\n      3.643884e+03\n      3.309068e+04\n      3.005017e+05\n    \n  \n\n10000 rows × 126 columns\n\n\n\nWe then use another extremely popular machine learning Python package called scikit-learn to easily work with our regression models.\n\nOrdinary least squares\nFirst, let’s apply ordinary least squares to see if the solution is clear:\n\nimport sklearn.linear_model as lm\nimport sklearn.metrics as met\n\ndef compute_ols_results(A,b):\n    ols = lm.LinearRegression()\n    ols.fit(A, b)\n    Rsquare = met.r2_score(ols.predict(A), b)\n    print(\"R^2: {}\".format(Rsquare))\n    ols_results = pd.DataFrame(\n        data=[ols.coef_],\n        columns=term_matrix.columns,\n        index=[\"Coefficients\"]\n    )\n    return ols_results\ncompute_ols_results(term_matrix, h_t)\n\nR^2: 0.9982899874867719\n\n\n\n\n\n\n  \n    \n      \n      1\n      h(x, t)\n      h_x(x, t)\n      h_xx(x, t)\n      h_xxx(x, t)\n      h_xxxx(x, t)\n      h(x, t)**2\n      h(x, t)*h_x(x, t)\n      h(x, t)*h_xx(x, t)\n      h(x, t)*h_xxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.003291\n      -0.001123\n      0.010196\n      -0.010514\n      0.017121\n      0.000644\n      0.000319\n      0.00216\n      -0.002166\n      -0.003493\n      ...\n      -2.442078e-10\n      1.701862e-09\n      7.807828e-10\n      -3.123881e-12\n      4.619499e-13\n      8.305367e-11\n      4.131124e-12\n      1.370824e-12\n      2.732623e-15\n      1.113909e-15\n    \n  \n\n1 rows × 126 columns\n\n\n\nAlthough the \\(R^2\\) value implies that we have successful explained the variance in \\(h_t\\) by linearly combining our term library, it is unclear which of all the terms most contributes to the time evolution from their coefficients.\n\n\nLasso\nNow, let’s add some regularization to try to remove some terms with the Lasso regression:\n\ndef compute_lasso_results(A,b,lamb):\n    lasso = lm.Lasso(lamb)\n    lasso.fit(A,b)\n    lasso_results = pd.DataFrame(\n        data=[lasso.coef_[lasso.coef_ != 0]],\n        columns=term_matrix.columns[lasso.coef_ != 0],\n        index=[\"Coefficients\"]\n    )\n    return lasso_results\ncompute_lasso_results(term_matrix,h_t,30)\n\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+03, tolerance: 1.101e+01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)\n      h_xx(x, t)*h_xxxx(x, t)\n      h_xxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)*h_x(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxx(x, t)**2\n      h(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.000019\n      0.000042\n      0.000006\n      -0.000016\n      0.000005\n      -9.063679e-08\n      0.000033\n      -0.000011\n      -0.000004\n      -0.000003\n      ...\n      2.762206e-11\n      7.854020e-10\n      -2.607649e-10\n      4.844760e-12\n      -2.047027e-13\n      -2.796414e-11\n      7.585174e-12\n      -3.492139e-13\n      1.942518e-14\n      -4.624874e-16\n    \n  \n\n1 rows × 86 columns\n\n\n\nNow this at least removed some of the terms, but it also removed the term we know is correct! It’s somewhat hard to interpret exactly what this means. A convenient analysis using the Lasso method is to perform a “lasso path” in which we steadily decrease the regularization \\(\\lambda\\) to add more and more terms and pay attention to the order with which they are added:\n\ndef compute_lasso_path_results(A,b):\n    lambs, coef_path, _ = lm.lasso_path(A, b, alphas=[1000,200,100,10,2])\n    for i in range(coef_path.shape[1]):\n        print(\"lambda = {}\".format(lambs[i]))\n        temp_results = pd.DataFrame(\n            data=[coef_path[:,i][coef_path[:,i] != 0]],\n            columns=term_matrix.columns[coef_path[:,i] != 0],\n            index=[\"Coefficients\"]\n        )\n        display(temp_results)\ncompute_lasso_path_results(term_matrix,h_t)\n\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6479.129684776085, tolerance: 11.011602711575614\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4239.993919608305, tolerance: 11.011602711575614\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3002.895267125694, tolerance: 11.011602711575614\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1179.9755031335178, tolerance: 11.011602711575614\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 654.8430901348651, tolerance: 11.011602711575614\n  model = cd_fast.enet_coordinate_descent_gram(\n\n\nlambda = 1000\n\n\n\n\n\n\n  \n    \n      \n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxxx(x, t)**2\n      h_x(x, t)**2*h_xxxx(x, t)\n      h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)\n      h_x(x, t)*h_xxx(x, t)**2\n      h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      h_x(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)**2*h_xxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      0.000004\n      -1.358433e-07\n      -0.000003\n      1.734871e-07\n      -0.000024\n      -0.000018\n      0.000018\n      1.798392e-07\n      2.194954e-08\n      0.00002\n      ...\n      1.756186e-12\n      5.409151e-09\n      -4.538996e-10\n      -1.582507e-11\n      -5.175566e-13\n      -1.599983e-10\n      1.446234e-11\n      -1.393064e-12\n      5.006079e-15\n      -1.081721e-15\n    \n  \n\n1 rows × 65 columns\n\n\n\nlambda = 200\n\n\n\n\n\n\n  \n    \n      \n      h_xx(x, t)*h_xxxx(x, t)\n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)*h_xxx(x, t)**2\n      h(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxxx(x, t)**2\n      h_x(x, t)**2*h_xxxx(x, t)\n      h_x(x, t)*h_xx(x, t)**2\n      h_x(x, t)*h_xx(x, t)*h_xxx(x, t)\n      h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.000006\n      0.000006\n      -7.521125e-08\n      -0.00002\n      -0.000005\n      1.823139e-07\n      -0.000026\n      0.000536\n      0.000015\n      -0.000011\n      ...\n      -2.414092e-11\n      5.320438e-09\n      -4.616132e-10\n      -1.611746e-11\n      -6.934042e-13\n      -1.620875e-10\n      1.653041e-11\n      -8.339666e-13\n      -2.720528e-15\n      -1.134805e-15\n    \n  \n\n1 rows × 73 columns\n\n\n\nlambda = 100\n\n\n\n\n\n\n  \n    \n      \n      h_xxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)*h_xxx(x, t)**2\n      h(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxxx(x, t)**2\n      h_x(x, t)**2*h_xxxx(x, t)\n      h_x(x, t)*h_xx(x, t)**2\n      h_x(x, t)*h_xx(x, t)*h_xxx(x, t)\n      h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.000002\n      0.000008\n      -8.488352e-08\n      -0.000036\n      -0.000005\n      2.101201e-07\n      -0.000021\n      0.000994\n      0.000005\n      -0.000006\n      ...\n      -4.268550e-11\n      5.269479e-09\n      -4.910509e-10\n      -1.314512e-11\n      -7.742979e-13\n      -1.331645e-10\n      2.042814e-11\n      -5.987451e-13\n      2.408353e-15\n      -1.135248e-15\n    \n  \n\n1 rows × 74 columns\n\n\n\nlambda = 10\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)*h_xxxx(x, t)\n      h_xx(x, t)*h_xxxx(x, t)\n      h_xxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)*h_xx(x, t)*h_xxx(x, t)\n      h(x, t)*h_xx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxx(x, t)**2\n      h(x, t)*h_xxx(x, t)*h_xxxx(x, t)\n      h(x, t)*h_xxxx(x, t)**2\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.000078\n      0.000022\n      -0.00003\n      0.000009\n      -3.621404e-08\n      -0.000142\n      0.000003\n      -0.000044\n      -0.000006\n      2.333753e-07\n      ...\n      -5.158142e-11\n      5.223207e-09\n      -5.293262e-10\n      -6.898387e-12\n      -7.832643e-13\n      -9.675523e-11\n      2.194763e-11\n      -3.257953e-13\n      9.419456e-15\n      -1.060554e-15\n    \n  \n\n1 rows × 87 columns\n\n\n\nlambda = 2\n\n\n\n\n\n\n  \n    \n      \n      h(x, t)*h_xxxx(x, t)\n      h_x(x, t)*h_xxx(x, t)\n      h_x(x, t)*h_xxxx(x, t)\n      h_xx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)\n      h_xx(x, t)*h_xxxx(x, t)\n      h_xxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)\n      h_xxxx(x, t)**2\n      h(x, t)**2*h_xxxx(x, t)\n      ...\n      h_xx(x, t)**2*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxx(x, t)**3\n      h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)\n      h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2\n      h_xx(x, t)*h_xxxx(x, t)**3\n      h_xxx(x, t)**4\n      h_xxx(x, t)**3*h_xxxx(x, t)\n      h_xxx(x, t)**2*h_xxxx(x, t)**2\n      h_xxx(x, t)*h_xxxx(x, t)**3\n      h_xxxx(x, t)**4\n    \n  \n  \n    \n      Coefficients\n      -0.000016\n      0.000541\n      -0.00018\n      -0.000066\n      0.000133\n      0.000032\n      -0.000033\n      0.000008\n      -1.995555e-08\n      -0.000084\n      ...\n      -6.687737e-11\n      4.931543e-09\n      -4.735362e-10\n      -7.393856e-12\n      -6.988533e-13\n      -7.077761e-11\n      1.972168e-11\n      -3.251701e-14\n      7.291442e-15\n      -8.182989e-16\n    \n  \n\n1 rows × 97 columns\n\n\n\nAgain, although this gives us a sense of sparsity, it also doesn’t seem to capture the solution well.\n\n\nGreedy forward selection\nLet’s instead try a greedy method for our system that will inform which terms should be included. To do so, we will use a generic scikit-learn interface called SequentialFeatureSelector as well as the \\(R^2\\) coefficient of determination r2_score to select terms one by one that best “explain the variance” in the time evolution \\(h_t(x,t)\\). As the terms are selected, we will compute the coefficients of the small libraries via ordinary least squares:\n\nimport sklearn.feature_selection as fs\n\ndef forward_r2_select(A,b,num_terms=4):\n    for i in range(1,num_terms+1):\n        sfs = fs.SequentialFeatureSelector(\n            lm.LinearRegression(),\n            n_features_to_select=i,\n            scoring=met.make_scorer(met.r2_score)\n        )\n        new_A = sfs.fit_transform(A,b)\n        new_ols = sfs.estimator\n        new_ols.fit(new_A,b)\n        Rsquare = met.r2_score(new_ols.predict(new_A),b)\n        feat_names = sfs.get_feature_names_out(A.columns)\n        print(\"R^2: {}\".format(Rsquare))\n        temp_results = pd.DataFrame(\n            data=[new_ols.coef_],\n            columns=feat_names,\n            index=[\"Coefficients\"]\n        )\n        display(temp_results)\n\nforward_r2_select(term_matrix, h_t)\n\nR^2: 0.9999009926990204\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      0.9948\n    \n  \n\n\n\n\nR^2: 0.9999137791391229\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      1.000245\n      -0.004588\n    \n  \n\n\n\n\nR^2: 0.9999166090137175\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n      h_x(x, t)**2*h_xxx(x, t)\n    \n  \n  \n    \n      Coefficients\n      1.000994\n      -0.003494\n      0.000001\n    \n  \n\n\n\n\nR^2: 0.9999179600640669\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n      h_x(x, t)**2*h_xxx(x, t)\n      h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)\n    \n  \n  \n    \n      Coefficients\n      1.001286\n      -0.003129\n      0.000001\n      1.281138e-08\n    \n  \n\n\n\n\nThis seems to easily pick up that the only term needed to completely resolve the time evolution is \\(h_x(x,t)\\)!"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-noisy",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-noisy",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "Application to noisy simulated wave data",
    "text": "Application to noisy simulated wave data\nIn a real system, we could not expect to immediately have data as smooth as that we used in Section 3. However, the procedure is unchanged. The only challenge will be fitting the neural network to our data. Let’s add some noise to the data:\n\nx,t,noisy_h = load_data(\"simple_wave.npz\",.2)\nanimation2 = animate_data(x,t,[noisy_h], [\"h noisy\"])\nanimation2.save(\"noisy_h.gif\")\nplt.close()\n\n\nGiven our data is now noisy, we may want to implement a train-validation-test method for fitting. Simply put, this means that we will hold out a portion of our data from the training procedure. Part of this held-back data (validation set) will be used to validate that our model can generalize to other points during training. The other part of the held-back data (test set) will be used as a final check on how well the model extrapolates out of the training data.\n\nimport sklearn.model_selection as ms\n\nX,T = jnp.meshgrid(x,t)\nxt_noisy = np.vstack((X.flatten(),T.flatten())).T\nh_noisy = noisy_h.flatten()\nxt_train, xt_test, h_train, h_test = ms.train_test_split(xt_noisy,h_noisy,test_size=.1,train_size=.9)\nxt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n\ntrain_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\nvalid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\ntest_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)\n\nNow, we apply our previous model construction and training:\n\n# Initialize model\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel2 = MyNet()\nparams2 = model2.init(rng2,random_data)\n\n# Loss function\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model2.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\n# Optimizer\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params2)\n\n# Training (adjusted to use our validation data\nepochs = 1200\nfor i in range(epochs):\n    xt_batch = train_data[i%len(train_data)][0]\n    h_batch = train_data[i%len(train_data)][1]\n    loss_val, grads = loss_grad_fn(params2, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params2 = optax.apply_updates(params2, updates)\n    if i % 100 == 0:\n        train_loss = mse(params2,xt_train,h_train)\n        valid_loss = mse(params2,xt_valid,h_valid)\n        print(\"Step {}\".format(i))\n        print(\"Training loss: {}\".format(train_loss))\n        print(\"Validation loss: {}\".format(valid_loss))\n        print()\ntest_loss = mse(params2,xt_test,h_test)\nprint(\"Test loss after training: {}\".format(test_loss))\n\nhhat2 = model2.apply(params2,xt_points).reshape(X.shape)\ndiff = np.sqrt((noisy_h - hhat2)**2)\ndiff2 = np.sqrt((hhat1 - hhat2)**2)\nanimation3 = animate_data(x,t,[noisy_h,hhat2,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation3.save(\"noisy_h_compare.gif\")\nplt.close()\nanimation3 = animate_data(x,t,[hhat1,hhat2,diff2],[\"$\\hat{h}$ clean\",\"$\\hat{h}$ noisy\",\"$L^2$ error\"])\nanimation3.save(\"noisy_hhat_compare.gif\")\nplt.close()\n\nStep 0\nTraining loss: 1.144004464149475\nValidation loss: 1.01979660987854\n\n\n\nStep 100\nTraining loss: 0.2510860562324524\nValidation loss: 0.24596765637397766\n\n\n\nStep 200\nTraining loss: 0.15993238985538483\nValidation loss: 0.1527174413204193\n\n\n\nStep 300\nTraining loss: 0.11942551285028458\nValidation loss: 0.1161203384399414\n\n\n\nStep 400\nTraining loss: 0.05994514748454094\nValidation loss: 0.06195114552974701\n\n\n\nStep 500\nTraining loss: 0.04363465681672096\nValidation loss: 0.04668991640210152\n\n\n\nStep 600\nTraining loss: 0.040037043392658234\nValidation loss: 0.042801398783922195\n\n\n\nStep 700\nTraining loss: 0.040146779268980026\nValidation loss: 0.042471904307603836\n\n\n\nStep 800\nTraining loss: 0.04032427817583084\nValidation loss: 0.04244818538427353\n\n\n\nStep 900\nTraining loss: 0.039380643516778946\nValidation loss: 0.041496243327856064\n\n\n\nStep 1000\nTraining loss: 0.03925921022891998\nValidation loss: 0.04148998484015465\n\n\n\nStep 1100\nTraining loss: 0.04000569134950638\nValidation loss: 0.042704202234745026\n\n\n\nTest loss after training: 0.03959168866276741\n\n\nThe resulting fit can be seen in the following video:\n\nLooks pretty good all things considered! We can also compare this with the fit on clean data to see how impressive the robustness to noise was:\n\nFinally, we construct the terms and check the results after forward selection:\n\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model2.apply(params2, new_x)[0]\n\n# Construct terms numerically\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))\nterm_values = construct_terms(diff_term_values)\n\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))\n\n# Forward selection\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\nforward_r2_select(term_matrix, h_t)\n\nR^2: 0.9983865021765934\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      0.999449\n    \n  \n\n\n\n\nR^2: 0.9984874802784794\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h_xxx(x, t)**3\n    \n  \n  \n    \n      Coefficients\n      1.006194\n      1.954267e-09\n    \n  \n\n\n\n\nR^2: 0.9985314607402614\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h_xxx(x, t)**3\n      h_x(x, t)**2*h_xxxx(x, t)**2\n    \n  \n  \n    \n      Coefficients\n      1.007312\n      1.972251e-09\n      -5.414004e-11\n    \n  \n\n\n\n\nR^2: 0.998539837060987\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h_x(x, t)**3\n      h_xxx(x, t)**3\n      h_x(x, t)**2*h_xxxx(x, t)**2\n    \n  \n  \n    \n      Coefficients\n      1.014444\n      -0.000186\n      1.638917e-09\n      -4.988270e-11\n    \n  \n\n\n\n\nBoom! Landed right on the money. This is a simple example with a straightforward answer, but example holds to show the overall procedure for handling data with additive noise (multiplicative noise, which is more structural, would be an altogether different challenge)."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-extracted",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#sec-extracted",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "Application to extracted wave data",
    "text": "Application to extracted wave data\nNow, applying this procedure to real data is as simple as replacing our original dataset with an experimental dataset. However, the extraction process has a strong influence on the quality of the data that we will be using, so it deserves to be treated with some detail.\n\nImage data extraction\nThe original video we will be using can be found on YouTube here.\nVideo\nWe can load this video into individual image frames via:\n\nimport skimage as img\nimport imageio.v3 as iio\n\nraw_frames = []\ncut = (160,200)\nfor i in range(200,232):\n    frame = iio.imread(\"youtube_video.mp4\",plugin=\"pyav\",index=i)\n\n    # Cut the image to focus only on the wave portion\n    raw_frame = frame[cut[0]:cut[1],:,:]\n    raw_frames.append(raw_frame)\nraw_frames = np.array(raw_frames)\nplt.figure(figsize=(8,1))\nplt.imshow(raw_frames[16])\nplt.axis(False); plt.show()\n\n\n\n\nWe then need to remove the background and isolate the wave portion of the image, which is facilitated by the green color of the water in this video:\n\nframes = []\nfor i in range(len(raw_frames)):\n    frame = raw_frames[i]\n\n    # Find where the image is more green than red or blue and very bright green\n    mean_green = np.mean(frame[:,:,1])\n    std_green = np.std(frame[:,:,1])\n    frame = (frame[:,:,1] > frame[:,:,0]) & (frame[:,:,1] > frame[:,:,2]) & (frame[:,:,1] > mean_green+std_green)\n    frames.append(frame)\nframes = np.array(frames)\nplt.figure(figsize=(8,1))\nplt.imshow(frames[16],cmap=\"gray\")\nplt.axis(False); plt.show()\n\n\n\n\nBy averaging these pixels across all vertical pixels in the image, we can get a rough wave outline:\n\nheights = []\nfor i in range(len(frames)):\n    frame = frames[i]\n    \n    # Approximate wave height by averaging y-locations of bright green areas\n    height = np.zeros(frame.shape[1])\n    for j in range(frame.shape[1]):\n        height[j] = np.mean(np.where(frame[:,j] == 1)[0])\n    heights.append(height)\nheights = np.array(heights)\nbase = heights[16, 0]\n\nplt.figure(figsize=(8,1))\nplt.imshow(frames[16],cmap=\"gray\")\nline = plt.plot(heights[16], color=\"red\",lw=3)[0]\nline2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\nplt.axis(False); plt.show()\n\n\n\n\nFinally, we can note that the video is not quite level to the wave surface, so we can use a linear adjustment to align the water boundary heights at the middle of the video:\n\n# Adjust images and heights for an un-leveled camera\nim_width = len(heights[16])\nslope = (heights[16][-1] - heights[16][0]) / im_width\nfor i in range(len(heights)):\n    frame = frames[i]\n    height = heights[i]\n\n    # Adjust\n    for j in range(len(height)):\n        shift = int(slope*(im_width-j))\n        # Move frame pixels per column\n        frame[:,j] = np.roll(frame[:,j], shift)\n        # Move height of wave\n        height[j] += shift\n    frames[i] = frame\n    heights[i] = height\n\nframes = np.array(frames)\nraw_frames = np.array(raw_frames)\nheights = np.array(heights)\n\nfig = plt.figure(figsize=(8,1))\nim = plt.imshow(frames[0],cmap=\"gray\")\nline = plt.plot(heights[0], color=\"red\",lw=3)[0]\nline2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\nplt.axis(False);\n\ndef animation_function(i):\n    im.set_array(frames[i])\n    line.set_ydata(heights[i])\n    return [im,line,line2]\n\nwave_animation = anim.FuncAnimation(fig, animation_function, frames=range(len(frames)), blit=True)\nwave_animation.save(\"extracted_wave.gif\")\nplt.close()\n\n\nWe can now save this data to be used with our previous procedure:\n\n# Video portion is about 2 seconds long\ntimes = np.linspace(0,2,len(heights))\n# No given space scale\nx_domain = np.arange(len(heights[0]))\nnp.save(\"video_wave_images.npy\",raw_frames)\nnp.savez(\"video_wave_heights.npz\",h=heights,x=x_domain,t=times)\n\n\n\nUsing our experimental dataset\nUsing the same methods as listed in Section 4, we can discover an equation for this particular dataset:\n\nx,t,ext_h = load_data(\"video_wave_heights.npz\")\n# Flip image wave to be more familiar\next_h = -ext_h\nanimation2 = animate_data(x,t,[ext_h], [\"extracted h\"])\nanimation2.save(\"extracted_h.gif\")\nplt.close()\n\n\n\n# Splitting data\nX,T = jnp.meshgrid(x,t)\nxt_ext = np.vstack((X.flatten(),T.flatten())).T\nh_ext = ext_h.flatten()\nxt_train, xt_test, h_train, h_test = ms.train_test_split(xt_ext,h_ext,test_size=.1,train_size=.9)\nxt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n\ntrain_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\nvalid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\ntest_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)\n\n# Initialize model\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel3 = MyNet()\nparams3 = model3.init(rng2,random_data)\n\n# Loss function\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model3.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\n# Optimizer\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params3)\n\n# Training (adjusted to use our validation data\nepochs = 1200\nfor i in range(epochs):\n    xt_batch = train_data[i%len(train_data)][0]\n    h_batch = train_data[i%len(train_data)][1]\n    loss_val, grads = loss_grad_fn(params3, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params3 = optax.apply_updates(params3, updates)\n    if i % 100 == 0:\n        train_loss = mse(params3,xt_train,h_train)\n        valid_loss = mse(params3,xt_valid,h_valid)\n        print(\"Step {}\".format(i))\n        print(\"Training loss: {}\".format(train_loss))\n        print(\"Validation loss: {}\".format(valid_loss))\n        print()\ntest_loss = mse(params3,xt_test,h_test)\nprint(\"Test loss after training: {}\".format(test_loss))\n\nhhat = model3.apply(params3,xt_ext).reshape(X.shape)\ndiff = np.sqrt((ext_h - hhat)**2)\nanimation3 = animate_data(x,t,[ext_h,hhat,diff],[\"$extracted h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation3.save(\"ext_h_compare.gif\")\nplt.close()\n\nStep 0\nTraining loss: 1.0461026430130005\nValidation loss: 1.1078636646270752\n\n\n\nStep 100\nTraining loss: 0.019222810864448547\nValidation loss: 0.020138416439294815\n\n\n\nStep 200\nTraining loss: 0.015582811087369919\nValidation loss: 0.016700129956007004\n\n\n\nStep 300\nTraining loss: 0.01490006223320961\nValidation loss: 0.015923861414194107\n\n\n\nStep 400\nTraining loss: 0.014932448044419289\nValidation loss: 0.01587551273405552\n\n\n\nStep 500\nTraining loss: 0.014114796184003353\nValidation loss: 0.01500153448432684\n\n\n\nStep 600\nTraining loss: 0.01351952739059925\nValidation loss: 0.01461494155228138\n\n\n\nStep 700\nTraining loss: 0.01352175697684288\nValidation loss: 0.014626342803239822\n\n\n\nStep 800\nTraining loss: 0.012838657945394516\nValidation loss: 0.013886799104511738\n\n\n\nStep 900\nTraining loss: 0.012310030870139599\nValidation loss: 0.013370105996727943\n\n\n\nStep 1000\nTraining loss: 0.012038346379995346\nValidation loss: 0.01310480572283268\n\n\n\nStep 1100\nTraining loss: 0.011337725445628166\nValidation loss: 0.012411274015903473\n\n\n\nTest loss after training: 0.010088494047522545\n\n\n\n\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model3.apply(params3, new_x)[0]\n\n# Construct terms numerically\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_ext))\nterm_values = construct_terms(diff_term_values)\n\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_ext))\n\n# Forward selection\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_ext),2).T, names=(\"x\",\"t\")))\nforward_r2_select(term_matrix, h_t)\n\nR^2: 0.9840890772168756\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      -0.960403\n    \n  \n\n\n\n\nR^2: 0.9843441895692854\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      -0.934937\n      -0.019499\n    \n  \n\n\n\n\nR^2: 0.9856124272907933\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n      h(x, t)**2*h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      -0.898256\n      -0.137659\n      0.050806\n    \n  \n\n\n\n\nR^2: 0.9872553049177294\n\n\n\n\n\n\n  \n    \n      \n      h_x(x, t)\n      h(x, t)*h_x(x, t)\n      h(x, t)**2*h_x(x, t)\n      h(x, t)**3*h_x(x, t)\n    \n  \n  \n    \n      Coefficients\n      -0.915401\n      -0.28248\n      0.264671\n      -0.065045\n    \n  \n\n\n\n\nFeel free to play with the parameters of each step to try to change/improve the results we have seen here."
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#appendix",
    "href": "other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html#appendix",
    "title": "Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html",
    "href": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html",
    "title": "Connor Robertson",
    "section": "",
    "text": "# Needed package for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# Needed package for getting the current working directory\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n# Needed package for builing the convolutional neural network\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nfrom tensorflow.keras.utils import plot_model\n\n# Needed package for computer vision problems\nimport cv2\n\n2022-10-18 14:16:48.058716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nimage_index = 0\nprint('The number for index = ' + str(image_index) + ' is ' + str(y_train[image_index])) \n\nThe number for index = 0 is 5\n\n\n\nplt.imshow(x_train[image_index], cmap='Greys')\n\n<matplotlib.image.AxesImage at 0x7fe3cf11db20>\n\n\n\n\n\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\nx_train[image_index].shape\n\n(28, 28)\n\n\n\n\n\n\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)\n# Making sure that the values are float so that we can get decimal points after division\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint('Number of images in x_train', x_train.shape[0])\nprint('Number of images in x_test', x_test.shape[0])\n\nx_train shape: (60000, 28, 28, 1)\nNumber of images in x_train 60000\nNumber of images in x_test 10000\n\n\n\n\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten()) \nmodel.add(Dense(256, activation=tf.nn.relu))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation=tf.nn.softmax))\n\n2022-10-18 14:16:49.743398: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n\n\n                                                                 \n\n\n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n\n\n )                                                               \n\n\n                                                                 \n\n\n conv2d_1 (Conv2D)           (None, 11, 11, 32)        9248      \n\n\n                                                                 \n\n\n max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n\n\n 2D)                                                             \n\n\n                                                                 \n\n\n flatten (Flatten)           (None, 800)               0         \n\n\n                                                                 \n\n\n dense (Dense)               (None, 256)               205056    \n\n\n                                                                 \n\n\n dropout (Dropout)           (None, 256)               0         \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 10)                2570      \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 217,194\n\n\nTrainable params: 217,194\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\n\n\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\nmodel.fit(x=x_train,y=y_train, epochs=5)\n\n\nmodel.evaluate(x_test, y_test)\n\n  1/313 [..............................] - ETA: 25s - loss: 4.0738e-04 - accuracy: 1.0000\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34/313 [==>...........................] - ETA: 0s - loss: 0.0621 - accuracy: 0.9835     \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67/313 [=====>........................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9799\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b102/313 [========>.....................] - ETA: 0s - loss: 0.0678 - accuracy: 0.9813\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b138/313 [============>.................] - ETA: 0s - loss: 0.0743 - accuracy: 0.9798\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b175/313 [===============>..............] - ETA: 0s - loss: 0.0649 - accuracy: 0.9818\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b212/313 [===================>..........] - ETA: 0s - loss: 0.0602 - accuracy: 0.9833\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b246/313 [======================>.......] - ETA: 0s - loss: 0.0527 - accuracy: 0.9854\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b284/313 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9870\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b313/313 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.9876\n\n\n[0.04519505798816681, 0.9876000285148621]\n\n\n\nimage_index = 59\nplt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\npred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\nprint(' For image_index = ' + str(image_index) + ' CNN predicted number = ' + str(pred.argmax()))\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 48ms/step\n\n\n For image_index = 59 CNN predicted number = 5"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#write-a-number-in-6-different-ways-on-a-paper-and-transfer-the-picture-into-the-same-directory-that-your-jupyter-notebook-is",
    "href": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#write-a-number-in-6-different-ways-on-a-paper-and-transfer-the-picture-into-the-same-directory-that-your-jupyter-notebook-is",
    "title": "Connor Robertson",
    "section": "1. Write a number in 6 different ways on a paper and transfer the picture into the same directory that your Jupyter Notebook is",
    "text": "1. Write a number in 6 different ways on a paper and transfer the picture into the same directory that your Jupyter Notebook is\n\ncwd = os.getcwd()\nprint('current working directory is = ' + cwd)\n\ncurrent working directory is = /home/connor/GDrive/Software/cnrrobertson.github.io/other/mlseminar/fall_2022/workshop4_cnn\n\n\n\nfile = r'test_images/original_image.jpeg'\ntest_image_original = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n\n\nplt.imshow(test_image_original, cmap = 'gray')\n\n<matplotlib.image.AxesImage at 0x7fe3c6c6c3a0>"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#take-a-picture-of-each-of-them-individually-you-can-have-only-one-picture-and-then-crop-it-into-6-different-pieces",
    "href": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#take-a-picture-of-each-of-them-individually-you-can-have-only-one-picture-and-then-crop-it-into-6-different-pieces",
    "title": "Connor Robertson",
    "section": "2. Take a picture of each of them individually – you can have only one picture and then crop it into 6 different pieces",
    "text": "2. Take a picture of each of them individually – you can have only one picture and then crop it into 6 different pieces\n\nfig, axs = plt.subplots(2, 3)\n\n\ncounter = 1\nfor row_number in range(0, 2):\n    for col_number in range(0,3):\n        \n        file = r'test_images/copy_' + str(counter) +'.jpeg'\n        copy_image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n        axs[row_number, col_number].imshow(copy_image, cmap = 'gray')\n        counter = counter + 1"
  },
  {
    "objectID": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#change-the-format-of-the-picture-into-a-readable-form-for-your-cnn-model",
    "href": "other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.html#change-the-format-of-the-picture-into-a-readable-form-for-your-cnn-model",
    "title": "Connor Robertson",
    "section": "4. Change the format of the picture into a readable form for your CNN model",
    "text": "4. Change the format of the picture into a readable form for your CNN model\n\nfile = r'test_images/copy_5.jpeg'\ncopy_1 = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\nplt.imshow(copy_1, cmap = 'gray')\n\n<matplotlib.image.AxesImage at 0x7fe3b851ff40>\n\n\n\n\n\n\n# copy_1_resized = cv2.resize(copy_1, (28, 28), interpolation = cv2.INTER_LINEAR)\ncopy_1_resized = cv2.resize(copy_1, (28, 28))\ncopy_1_resized = cv2.bitwise_not(copy_1_resized)\n\nplt.imshow(copy_1_resized, cmap = 'Greys')\n\n<matplotlib.image.AxesImage at 0x7fe3b8495430>\n\n\n\n\n\n\npred = model.predict(copy_1_resized.reshape(1, 28, 28, 1))\nprint('CNN predicted number = ' + str(pred.argmax()))\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 34ms/step\n\n\nCNN predicted number = 3"
  },
  {
    "objectID": "other/mlseminar/mlseminar.html",
    "href": "other/mlseminar/mlseminar.html",
    "title": "Machine Learning and Optimization Seminar",
    "section": "",
    "text": "The Machine Learning and Optimization is a student-led seminar in the Department of Mathematical Sciences at NJIT. Its goal is to expose the participants to topics in machine learning and optimization by:"
  },
  {
    "objectID": "other/mlseminar/mlseminar.html#fall-2022",
    "href": "other/mlseminar/mlseminar.html#fall-2022",
    "title": "Machine Learning and Optimization Seminar",
    "section": "Fall 2022",
    "text": "Fall 2022\n\nWorkshops\n\n9/15/22: Python set up, machine learning basics, gradient descent, and automatic differentiation - Connor Robertson\n\nDocument (with code)\nJupyter notebook\n\n10/6/22: Neural networks - structure, building, and training - Jake Brusca\n\nPresentation\nDocument (with code)\nJupyter notebook\nGoogle Colab notebook\n\n10/13/22: Data-driven model discovery - Connor Robertson\n\nDocument (with code)\nJupyter notebook\nVideo data\n\n10/20/22: Convolutional neural networks - Soheil Saghafi\n\nPresentation\nDocument (with code)\nJupyter notebook\nImage data\n\n11/3/22: Recurrent neural networks - Austin Juhl\n\nPresentation\nCode repository\n\n12/1/22: Reinforcement learning - Sepideh Nikookar\n\n\n\nPresentations\n\n11/10/22: Recurrent neural networks for beat prediction - Prianka Bose\n12/8/22: Generative adversarial networks - Soheil Saghafi"
  },
  {
    "objectID": "other/mlseminar/mlseminar.html#past-presentations",
    "href": "other/mlseminar/mlseminar.html#past-presentations",
    "title": "Machine Learning and Optimization Seminar",
    "section": "Past Presentations",
    "text": "Past Presentations\n\nSpring 2022\n\nWasserstein GANs Work Because They Fail - Axel Turnquist\nMean-field Theory: Drift and the Mean Drift - Binan Gu\n\n\n\nFall 2021\n\nComputing the Distance Between Probability Measures - Axel Turnquist\nFull Waveform Inversion Using the Wasserstein Metric - Brittany Hamfeldt\nImage Sharpening - Axel Turnquist\nNeural Networks for function approximation and data-driven modeling - Connor Robertson\nOptimal control of systems governed by PDEs with uncertainty - Georg Stadler\nStochastic Temporal Networks - Binan Gu\nTopological Data Analysis Applied to Interaction Networks in Particulate Systems - Lou Kondic\n\n\n\nSpring 2021\n\nSpin Glass and High Dimensional Energy Landscape - Binan Gu\nDiffusion Approximations and Applications in Non-convex Optimization - Binan Gu\nStochastic Gradient Descent and How I Learned to Love the Randomness - Joe McCann\n\n\n\nFall 2020\n\nWhy does stochastic gradient descent work so well? - Axel Turnquist\nInformation Geometry & Learning - Axel Turnquist\nGraph-based Learning Beyond the Paradigm of Neural Networks - Binan Gu\nEffective Dimension in High-Dimensional Problems - Axel Turnquist\nBayesian Statistics and Machine Learning - Gan Luan\nMatrix Completion and Sparse Recovery - Axel Turnquist\nWasserstein GAN - Yixuan Sun\nGraphical Model Selection - Binan Gu\nLearning Frameworks - Axel Turnquist"
  },
  {
    "objectID": "research/active_nematics.html",
    "href": "research/active_nematics.html",
    "title": "Data-driven model discovery for an active nematic system",
    "section": "",
    "text": "Video\n\nSeveral models have been proposed for this system via a continuum “Q-tensor” theory. The models consist of coupled partial differential time-evolution equations for the orientation and velocity of the microtubules (and occasionally the concentration). Though successful in recreating some of the most salient qualitative behavior, there has been some disagreement on the model form and the exact quantitative agreement [2–5].\nThis project aims to provide a data-driven model that can lend insight into the various proposed model forms using the Sparse Identification of Nonlinear Dynamics (SINDy) modeling framework [6]. It involves:\n\nAccurately extracting the orientation, concentration, and velocity of the microtubles in the above video\nGenerating a library of possible terms for each evolution equation via symbolic computation, data fitting, and numerical differentiation of noisy data\nUsing techniques in sparse regression and variable selection to identify the most probable form for the evolution equations\nSimulating the resulting models and comparing the results both qualitatively and quantitatively with the information extracted from the experimental images\n\nThis work was presented at the APS March meeting 2022 and will soon be presented at APS DFD meeting 2022.\n\n\n\n\nReferences\n\n[1] J. Pringle, A. Muthukumar, A. Tan, L. Crankshaw, L. Conway, and J. L. Ross, Microtubule Organization by Kinesin Motors and Microtubule Crosslinking Protein MAP65, Journal of Physics: Condensed Matter 25, 374103 (2013).\n\n\n[2] S. J. DeCamp, G. S. Redner, A. Baskaran, M. F. Hagan, and Z. Dogic, Orientational Order of Motile Defects in Active Nematics, Nature Materials 14, 1110 (2015).\n\n\n[3] A. Doostmohammadi, J. Ignés-Mullol, J. M. Yeomans, and F. Sagués, Active Nematics, Nature Communications 9, 1 (2018).\n\n\n[4] A. U. Oza and J. Dunkel, Antipolar Ordering of Topological Defects in Active Liquid Crystals, New Journal of Physics 18, 093006 (2016).\n\n\n[5] T. Gao, M. D. Betterton, A.-S. Jhang, and M. J. Shelley, Analytical Structure, Dynamics, and Coarse Graining of a Kinetic Model of an Active Fluid, Physical Review Fluids 2, 093302 (2017).\n\n\n[6] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems, Proceedings of the National Academy of Sciences 113, 3932 (2016)."
  },
  {
    "objectID": "research/bacteria.html",
    "href": "research/bacteria.html",
    "title": "Predicting bacterial growth in experimental images via recurrent neural networks",
    "section": "",
    "text": "This project focused on predicting the growth of a heterogeneous culture of bacterial strains via experimental images of their interactions and a spatiotemporal convolutional recurrent neural network called PredRNN [1]. Specifically, the network was trained on sequences of images from microwell arrays which show the growth of two mutant strains of Pseudomonas aeruginosa in a single well (one which can kill the other) [2]. A sample of one of these sequences can be seen below. The network then outputs a predicted sequence of subsequent images, which we compared with the true images via popular image metrics such as Learned Perceptual Image Patch Similarity (LPIPS) as well as with biological metrics such as the number and size of colonies of each strain. The results of this work can be found in this preprint [3].\n\n\n\nThis same technique was also applied to accelerate the results of an agent based model of the same system which can be found in this preprint [4].\n\n\n\n\nReferences\n\n[1] Y. Wang, H. Wu, J. Zhang, Z. Gao, J. Wang, P. Yu, and M. Long, PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning, IEEE Transactions on Pattern Analysis and Machine Intelligence 1 (2022).\n\n\n[2] A. C. Timm, M. C. Halsted, J. L. Wilmoth, and S. T. Retterer, Assembly and Tracking of Microbial Community Development Within a Microwell Array Platform, JoVE (Journal of Visualized Experiments) e55701 (2017).\n\n\n[3] C. Robertson, J. L. Wilmoth, S. Retterer, and M. Fuentes-Cabrera, Performing Video Frame Prediction of Microbial Growth with a Recurrent Neural Network, (2022).\n\n\n[4] J. Sakkos et al., Investigating the Growth of an Engineered Strain of Cyanobacteria with an Agent-Based Model and a Recurrent Neural Network, bioRxiv (2021)."
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Research",
    "section": "",
    "text": "This section contains various overviews of the research projects that I have worked on or which I am currently working on. They are high level overviews of the project scope as well as references to papers and results."
  },
  {
    "objectID": "research/research.html#projects",
    "href": "research/research.html#projects",
    "title": "Research",
    "section": "Projects",
    "text": "Projects\n\nData-driven discovery of a PDE model for an active nematic system\nPredicting bacterial growth in experimental images using recurrent neural networks"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz1.html",
    "href": "teaching/math111/quizzes/quiz1.html",
    "title": "Quiz 1 - Limits and rate of change",
    "section": "",
    "text": "Find \\[\n\\lim_{t\\rightarrow 0} \\frac{\\tan(t)\\sec(t)}{3t}\n\\]\n\n\n\n\\[\n\\begin{align*}\n\\lim_{t\\rightarrow 0} \\frac{\\tan(t)\\sec(t)}{3t} &= \\frac{1}{3}\\lim_{t\\rightarrow 0} \\frac{\\sin(t)}{\\cos(t)} \\times \\frac{1}{\\cos(t)} \\times \\frac{1}{t} \\\\\n&= \\frac{1}{3} \\lim_{t\\rightarrow 0} \\cancelto{1}{\\frac{\\sin(t)}{t}} \\times \\frac{1}{\\cos^2(t)} \\\\\n&= \\frac{1}{3} \\lim_{t\\rightarrow 0} \\frac{1}{\\cos^2(t)} \\\\\n&= \\boxed{\\frac{1}{3}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz1.html#problem-2",
    "href": "teaching/math111/quizzes/quiz1.html#problem-2",
    "title": "Quiz 1 - Limits and rate of change",
    "section": "Problem 2",
    "text": "Problem 2\n\nQuestion\nFor function \\(y = f(x) = 3x^2 + 1\\):\n\nFind the rate of change \\(\\frac{\\Delta y}{\\Delta x}\\)\nFind the average rate of change over intervals \\([2,3]\\) and \\([-1,1]\\)\n\n\n\nSolution\n\na.\n\\[\n\\begin{align*}\n\\frac{\\Delta y}{\\Delta x} &= \\frac{f(x + \\Delta x) - f(x)}{x + \\Delta x - x} \\\\\n&= \\frac{3(x+\\Delta x)^2 + 1 - (3x^2 + 1)}{\\Delta x} \\\\\n&= \\frac{\\cancel{3x^2} + 6x\\cancel{(\\Delta x)} + 3(\\Delta x)^\\cancel{2} + \\cancel{1}- \\cancel{ 3x^2} -\\cancel{1} }{\\cancel{\\Delta x}} \\\\\n&= \\boxed{6x + 3(\\Delta x)}\n\\end{align*}\n\\]\n\n\nb.\nFor \\([2,3]\\): \\[\n\\begin{align*}\n\\frac{f(x_2) - f(x_1)}{x_2 - x_1} &= \\frac{f(3) - f(2)}{3 - 2} \\\\\n&= \\frac{3(3)^2 + 1 - (3(2)^2 + 1)}{1} \\\\\n&= 27 - 12 = \\boxed{15}\\\\\n\\end{align*}\n\\] For \\([-1,1]\\): \\[\n\\begin{align*}\n\\frac{f(x_2) - f(x_1)}{x_2 - x_1} &= \\frac{f(1) - f(-1)}{1 + 1} \\\\\n&= \\frac{3(1)^2 + 1 - (3(-1)^2 + 1)}{2} \\\\\n&= \\frac{0}{2} = \\boxed{0}\\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz2.html",
    "href": "teaching/math111/quizzes/quiz2.html",
    "title": "Quiz 2 - Continuity",
    "section": "",
    "text": "Show that the graph of \\(f(x) = 3x^2 + 5x - 11 = 0\\) has a solution between \\(x=1\\) and \\(x=2\\). State which theorem you used.\n\n\n\nSince we know that \\(f(x)\\) is a polynomial and thus continous, we can use the intermediate value theorem to show that there exists a root between the points. To do so we need to show that the function is above the x-axis at one of the endpoints and below at the other: \\[\n\\begin{align*}\nf(1) &= 3(1)^2 + 5(1) - 11 = 3 + 5 - 11 = -3 < 0 \\\\\nf(2) &= 3(2)^2 + 5(2) - 11 = 12 + 10 - 11 = 11 > 0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz2.html#problem-2",
    "href": "teaching/math111/quizzes/quiz2.html#problem-2",
    "title": "Quiz 2 - Continuity",
    "section": "Problem 2",
    "text": "Problem 2\n\nQuestion\nFind constants \\(a,b\\) so that the function given below is continuous for all \\(x\\): \\[\nf(x) = \\begin{cases}\nx^2 + 3 & x<2 \\\\\na & x = 2 \\\\\nax+b & x > 2\n\\end{cases}\n\\]\n\n\nSolution\nFirst, we need to make sure that the left most part of the function connects with the center part: \\[\n\\begin{align*}\na &= x^2 + 3 \\text{ at }x=2 \\\\\n&= (2)^2 + 3 \\\\\n\\Aboxed{a &= 7}\n\\end{align*}\n\\] Next, we match the center part to the rightmost part: \\[\n\\begin{align*}\na &= ax + b\\text{ at }x=2 \\\\\n7 &= 7(2) + b \\\\\nb &= 7 - 14 \\\\\n\\Aboxed{b &= -7}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz3.html",
    "href": "teaching/math111/quizzes/quiz3.html",
    "title": "Quiz 3 - Definition of a derivative",
    "section": "",
    "text": "Use the definition of a derivative to find the derivative \\(f'(x)\\) of the function \\(f(x) = \\frac{1}{x}\\).\n\n\n\nUsing the definition: \\[\n\\begin{align*}\nf'(x) &= \\lim_{h\\rightarrow 0}\\left(\\frac{f(x + h) - f(x)}{h}\\right) \\\\\n&= \\lim_{h\\rightarrow 0}\\left(\\frac{\\frac{1}{x+h} - \\frac{1}{x}}{h}\\right) \\\\\n&= \\lim_{h\\rightarrow 0}\\left(\\frac{\\frac{x}{x(x+h)} - \\frac{x+h}{x(x+h)}}{h}\\right) \\\\\n&= \\lim_{h\\rightarrow 0}\\left(\\frac{\\frac{x - (x+h)}{x(x+h)}}{h}\\right) \\\\\n&= \\lim_{h\\rightarrow 0}\\left(\\frac{\\frac{ -h}{x(x+h)}}{h}\\right) \\\\\n&= \\lim_{h\\rightarrow 0}\\left(\\frac{-1}{x(x+h)}\\right) \\\\\n\\Aboxed{f'(x) &= -\\frac{1}{x^2}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz4.html",
    "href": "teaching/math111/quizzes/quiz4.html",
    "title": "Quiz 4 - Chain rule and implicit differentiation",
    "section": "",
    "text": "Find the derivative \\(\\frac{dy}{dx}\\) for:\na) \\(y = \\sqrt{x^2 + \\sec^2(x)}\\)\nb) \\(y = \\log_5(\\csc(x))\\)\nc) \\(y = \\tan^2(\\sin(3x))\\)\n\n\n\na) \\[\n\\begin{align}\ny &= \\sqrt{x^2 + \\sec^2(x)} = \\left(x^2 + \\sec^2(x)\\right)^{1/2} \\\\\n\\frac{dy}{dx} &= \\frac{1}{2}\\left(x^2 + \\sec^2(x)\\right)^{-1/2} \\times \\frac{d}{dx}\\left(x^2 + \\sec^2(x)\\right) \\\\\n&= \\frac{1}{2}\\left(x^2 + \\sec^2(x)\\right)^{-1/2} \\times \\left(2x + 2\\sec(x)\\times \\frac{d}{dx} \\sec(x)\\right) \\\\\n&= \\frac{1}{2}\\left(x^2 + \\sec^2(x)\\right)^{-1/2} \\times \\left(2x + 2\\sec^2(x)\\tan(x)\\right) \\\\\n\\Aboxed{\\frac{dy}{dx}&= \\frac{x + \\sec^2(x)\\tan(x)}{\\sqrt{x^2 + \\sec^2(x)}}}\n\\end{align}\n\\]\nb) \\[\n\\begin{align}\ny &= \\log_5(\\csc(x)) \\\\\n\\frac{dy}{dx} &= \\frac{1}{\\csc(x)\\ln(5)} \\times \\frac{d}{dx} \\csc(x) \\\\\n&= \\frac{1}{\\csc(x)\\ln(5)} \\times -\\cot(x)\\csc(x) \\\\\n\\Aboxed{\\frac{dy}{dx} &= -\\frac{\\cot(x)}{\\ln(5)}}\n\\end{align}\n\\]\nc) \\[\n\\begin{align}\ny &= \\tan^2(\\sin(3x)) \\\\\n\\frac{dy}{dx} &= 2\\tan(\\sin(3x)) \\times \\frac{d}{dx} \\tan(\\sin(3x)) \\\\\n&= 2\\tan(\\sin(3x)) \\times \\sec^2(\\sin(3x)) \\times \\frac{d}{dx} \\sin(3x) \\\\\n&= 2\\tan(\\sin(3x)) \\times \\sec^2(\\sin(3x)) \\times \\cos(3x) \\times \\frac{d}{dx} 3x \\\\\n&= 2\\tan(\\sin(3x)) \\times \\sec^2(\\sin(3x)) \\times \\cos(3x) \\times 3 \\\\\n\\Aboxed{\\frac{dy}{dx} &= 6\\tan(\\sin(3x))\\sec^2(\\sin(3x))\\cos(3x)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz5.html",
    "href": "teaching/math111/quizzes/quiz5.html",
    "title": "Quiz 5 - More chain rule and differentiation of trigonometric functions",
    "section": "",
    "text": "Differentiate \\(y = \\sqrt{x}^x\\).\n\n\n\n\\[\n\\begin{align}\ny &= \\sqrt{x}^x \\\\\n\\ln(y) &= \\ln(\\sqrt{x}^x) \\\\\n\\frac{d}{dx}\\ln(y) &= \\frac{d}{dx}\\ln(\\sqrt{x}^x) \\\\\n\\frac{y'}{y} &= \\frac{d}{dx}\\left(x\\ln(\\sqrt{x})\\right) \\\\\ny' &= y\\left(\\ln(\\sqrt{x}) + \\frac{x}{\\sqrt{x}}\\frac{d}{dx}\\sqrt{x}\\right) \\\\\n&= \\sqrt{x}^x\\left(\\ln(\\sqrt{x}) + \\frac{x}{\\sqrt{x}}\\left(\\frac{1}{2\\sqrt{x}}\\right)\\right) \\\\\n&= \\sqrt{x}^x\\left(\\frac{1}{2}\\ln(x) + \\frac{1}{2}\\right) \\\\\n\\Aboxed{y' &= \\frac{1}{2}\\sqrt{x}^x\\left(\\ln(x) + 1\\right)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz5.html#problem-2",
    "href": "teaching/math111/quizzes/quiz5.html#problem-2",
    "title": "Quiz 5 - More chain rule and differentiation of trigonometric functions",
    "section": "Problem 2",
    "text": "Problem 2\n\nQuestion\nDifferentiate \\(y = 10^{x + \\cos(x)}\\).\n\n\nSolution\n\\[\n\\begin{align}\ny &= 10^{x + \\cos(x)} \\\\\n\\ln(y) &= (x + \\cos(x))\\ln(10) \\\\\n\\frac{d}{dx}\\ln(y) &= \\ln(10)\\frac{d}{dx}(x + \\cos(x)) \\\\\n\\frac{y'}{y} &= \\ln(10)(1 - \\sin(x)) \\\\\ny' &= y\\ln(10)(1 - \\sin(x)) \\\\\n\\Aboxed{y' &= 10^{x+\\cos(x)}\\ln(10)(1 - \\sin(x))}\n\\end{align}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz5.html#problem-3",
    "href": "teaching/math111/quizzes/quiz5.html#problem-3",
    "title": "Quiz 5 - More chain rule and differentiation of trigonometric functions",
    "section": "Problem 3",
    "text": "Problem 3\n\nQuestion\nDifferentiate \\(y = \\frac{\\ln(x)}{x^e + e^e}\\).\n\n\nSolution\n\\[\n\\begin{align}\ny &= \\frac{\\ln(x)}{x^e + e^e} \\\\\ny' &= \\frac{\\frac{1}{x}(x^e + e^e) - \\ln(x)(ex^{e-1})}{(x^e+e^e)^2}\\\\\n&= \\frac{\\frac{1}{x}(x^e + e^e) - \\ln(x)(ex^{e-1})}{(x^e+e^e)^2}\\\\\n\\Aboxed{y' &= \\frac{x^e + e^e - \\ln(x)(ex^e)}{x(x^e+e^e)^2}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "teaching/math111/math111.html",
    "href": "teaching/math111/math111.html",
    "title": "Math 111 - Calculus I",
    "section": "",
    "text": "This section contains some resources for the Fall 2022 Math 111 Section 23 at NJIT. Hopefully, the notes, problems, and supplementary plots/videos/animations can help deepen understanding or act as a reference for review."
  },
  {
    "objectID": "teaching/math111/math111.html#quiz-solutions",
    "href": "teaching/math111/math111.html#quiz-solutions",
    "title": "Math 111 - Calculus I",
    "section": "Quiz Solutions",
    "text": "Quiz Solutions\n\nQuiz 1\nQuiz 2\nQuiz 3\nQuiz 4\nQuiz 5\nQuiz 6"
  },
  {
    "objectID": "teaching/math111/math111.html#a-couple-of-notes",
    "href": "teaching/math111/math111.html#a-couple-of-notes",
    "title": "Math 111 - Calculus I",
    "section": "A Couple of Notes",
    "text": "A Couple of Notes\n\nLimits and Continuity\nMotivation: People were interested in the speed and direction of planets at any moment in time. So, they wanted to know if they could calculate that by measuring the path of the planets. We can use a concept called “limits” to find this speed and direction.\n\nCalculus relates the rate of changes of things. i.e. how quickly does water empty out of a barrel is a ratio of the rate of change of the water relative to the rate of change of time.\n\n\nAverage and Instantaneous Rates of Change\n\nAverage speed: Divide the distance traveled by the time elapsed\nInstantaneous speed: The average speed over an infinitely small amount of time\nSecant line: A line connecting two points on a curve\nAs the two points of a secant line get closer and closer, the slope of the secant approaches the instantaneous velocity\nTangent line: A line that only touches one point on a line\nThe slope of a curve at a point \\(x\\) is the slope of the tangent line that passes through only \\(x\\)\nFor a curve that represents a quantity (y-axis) over time (x-axis), the slope of the tangent line at a point \\(t\\) in time is the instantaneous velocity (or rate of change of time) at \\(t\\)\nFormula for secant line / average velocity from time \\(t_1\\) to \\(t_2\\) where function \\(f(t)\\) is the position at time \\(t\\) and \\(\\Delta\\) represents “change in …”: \\[\n\\boxed{\\frac{\\Delta f}{\\Delta t}} = \\frac{f(t_2) - f(t_1)}{t_2 - t_1} = \\frac{f(t+h) - f(t)}{t+h - t} = \\boxed{\\frac{f(t+h) - f(t)}{h}}\n\\]\n\n\n\nHelpful animation\nAnimation of Figure 2.6 from the book."
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_Examples.html",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_Examples.html",
    "title": "Connor Robertson",
    "section": "",
    "text": "Lab Instructor: Connor Robertson\n\n\nHere are a few basic calculations and constructions that you will need to use throughout the course for building arrays of numbers. Take note of these constructions as they will be useful for the entire course\n\n% a row vector, X=(0 1 2 3) can be written three ways:\n\n% here it is written using brackets\nX = [0 1 2 3]\n\n\nX =\n\n     0     1     2     3\n\n\n\n\n% including ; at the end of the line will suppress the output (it will\n% still do the command but not print the variable:\nX = [0 1 2 3];\n\n% Here it is written in the form X=linspace(a,b,n) where\n% a is the first point, b is the last and n is the number\n% of equally spaced points\nX = linspace(0, 3, 4)\n\n\nX =\n\n     0     1     2     3\n\n\n\n\n% linspace is a function that Matlab includes by default (it is in the\n% Matlab standard library)\n\n% Here it is written in the form X=a:delta:b where\n% a is the first point, b is the last and delta is the \n% distance between each point\nX=0:1:3"
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "",
    "text": "Here are a few basic calculations and constructions that you will need to use throughout the course for building arrays of numbers. Take note of these constructions as they will be useful for the entire course.\n\n\nA row vector, X=(0 1 2 3) is commonly written in three ways:\n1. Using brackets:\nX=[0 1 2 3]\nIf ; is included at the end of the line, the output will be supressed. The command will still be executed, but nothing will be printed.\nX=[0 1 2 3];\n2. Using the linspace function:\nX=linspace(0, 3, 4)\nThis gives 4 equally spaced points between and including 0 and 3 as shown above. linspace is a function included in the Matlab standard library i.e. it is always available.\n3. In the form a:step:b:\nX=0:1:3\nThis will give all points between and including 0 to 3 with step size 1 between each point. It can also be written as X=0:3 which uses the default step size 1.\n\n\n\nOnce we have these arrays, we can apply an operation to each element in the array. For example, to compute \\(Y = [e^0, e^2, e^2, e^3]\\), we use the Matlab standard library function exp for exponential:\nY=exp(X)\nOr to apply sin to each element of the array:\nS=sin(X)\nOr to make an array Q with the square root to each element of the array X:\nQ=sqrt(X)\n\n\n\nTo create a \\(2\\times2\\) identity matrix:\nId=[1 0; 0 1]\nNote that the ; in the array definition marks the end of a row. There is also a Matlab standard library function eye that can be used as:\nId2=eye(2)\nA \\(4\\times 4\\) identity matrix is then:\nId4=eye(4)\nTo make a length 4 column vector (4 rows, 1 column) of all zeros, we can use the standard library zeros function:\nz = zeros(4,1)\nThis can be extended to a \\(4\\times4\\) matrix of all zeros:\nZ=zeros(4,4)\nOr a \\(3\\times3\\) matrix of all ones:\nW=ones(3,3)\n\n\n\nIf we wanted to compute the matrix-vector product of Id4 with our previous row vector X, we would first need to transpose X into a column vector. This can be done with either the standard library transpose function or the operator ':\nY=Id4 * X'\nor\nY=Id4 * transpose(X)\nFailing to transpose will give an error:\nId4 * X"
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-2-common-operations",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-2-common-operations",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "Example 2: Common operations",
    "text": "Example 2: Common operations\nIn Matlab, the * operator means “dot product” when working with matrices and vectors. If we instead wish to apply a multiplication operation to each element of an array, we preface the operation with a dot, i.e. .*. This elementwise operator syntax with an additional period extends to division and exponents as ./ and .^ respectively. Observe the difference between the products A*B and A.*B in the following examples:\nA=[1 0; 0 1]\nB=[0 1; 1 0]\nx = [1 ; 0]\nThe dot product is A*B:\nA*B\nA * x\nThe elementwise product is A.*B:\nA.*B\nThis product is the product of each element of A with each corresponding element of B."
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-3-plotting-in-matlab",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-3-plotting-in-matlab",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "Example 3: Plotting in Matlab",
    "text": "Example 3: Plotting in Matlab\nTo plot a line in Matlab, you will need to pass in arrays of \\(x\\) and \\(y\\) positions to the standard library plot function. These positions will be used as \\((x,y)\\) coordinates through which the line will pass. In this example we will define our function using an inline (or anonymous) function. More complicated functions can be found at the bottom of this page.\nFirst, create the function we would like to plot:\nf=@(x) x.^3;\nThis is now a function variable f to which we can pass in a value x and it will compute \\(x^3\\). We use .^ so that we can pass an array x into the function. We can now create a set of 1000 equally spaced x values between -2 and 2:\nX =linspace(-2, 2, 1000);\nand cube each of them using our function f to get an array of 1000 y points:\nY=f(X);\nWe can now pass X and Y as our \\(x\\) and \\(y\\) positions to the plot function to get a line that passes through each \\((x,y)\\) in order.\nplot(X,Y)\nThere are several additional parameters we can pass into the plot function to change the line style and color of the lines. Judgement should be used to determine which plot options to use to make your results of your work clear.\nWe can now create a new figure (a new plotting area with no lines on it) and plot our points with several additional decorations:\nfigure()\nplot(X,Y,'r','LineWidth',1); % 'r' => red line\n\nhold on\nplot(X,X);\n\ngrid on\n\naxis([-1 1 -2 2]);\n\nxlabel('x','fontsize',14); \nylabel('f(x)','fontsize',14);\n\ntitle('Plot example',...\n    'fontsize',14);\n\nlegend('x^3', 'x');\nThe 'LineWidth' marks that the width will be changed and 1 is the selected line width. The hold on command will keep the current figure for all new plots and plotting commands until a new figure is created with figure() or the hold is removed with hold off. This allows more lines to be added to the current figure. grid on turns on grid lines for the figure and axis is a function to determine the limits of the x and y axes. xlabel and ylabel are functions for labeling the figure axes and title is a function to give the figure a title. Note that the ... tells Matlab that the function inputs are continued on the next line. The legend function allows for annotating the plots in the figure in the order they were plotted."
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-4-control-flow",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-4-control-flow",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "Example 4: Control flow",
    "text": "Example 4: Control flow\nOne standard programming construct is that of a “for loop.” The following simple example shows how to implement a for loop to add the numbers between 1 and N. We will find the sum of the first 50 numbers\nN=50;\n\n% Initialize the sum variable as 0. We are going to add to this\nsumN = 0;\nfor i=1:N\n    \n    % In each iteration we will add the new number i to our sum\n    sumN = sumN+i;\nend\ndisp is the matlab function for “displaying” or printing a value or array. num2str is the matlab function to convert a number to a “string” which is a datatype meant for sequences of letters. This is especially useful for printing by combining different strings as shown below:\nresult = ['The sum of the first ' num2str(N) ' numbers is ' num2str(sumN)];\ndisp(result)\nThis algorithm is also packaged as a Matlab function SumN at the bottom of this file. In this course, all algorithms will be written as a function so they can be easily reused. However, all functions in Matlab scripts must be placed at the end of the file, so scroll down to see the function SumN.\ntotal=SumN(N);\nresult2 = ['The Sum is still ', num2str(total)];\ndisp(result2)"
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-5-a-simple-numerical-method-taylor-series",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#example-5-a-simple-numerical-method-taylor-series",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "Example 5: A simple numerical method (Taylor series)",
    "text": "Example 5: A simple numerical method (Taylor series)\nAs an application of the examples above, the following shows how to approximate the \\(\\sin\\) function at a point \\(x\\)using the Taylor series of \\(\\sin\\) (centered at 0). This makes use of for loops as demonstrated above.\nIf we consider \\(x=1\\) and use the first \\(N=20\\) terms in the Taylor series, we initialize our variables:\nx = 1;\nN = 20;\nWe then initialize a variable my_sin1 in which we will store the result and use the for loop to add each term evaluated at \\(x=1\\) to the variable:\nmy_sin1 = 0;\nfor n=0:N\n    new_term = (-1)^n * (x^(2*n+1)) / factorial(2*n + 1);\n    my_sin1 = my_sin1 + new_term;\nend\nWe can compare our approximation with Matlab’s approximation of \\(\\sin(1)\\) by comparing it with the standard library function sin:\ndisp(\"My answer:\")\ndisp(my_sin1)\ndisp(\"Matlab's answer:\")\ndisp(sin(1))\nA function that takes as inputs x and N and computes the taylor series approximation of \\(\\sin\\) as we just have for x=1,N=20 can be seen at the bottom of this file under the name my_sin. We can verify that it gives the same result:\ndisp(my_sin(1, 20))"
  },
  {
    "objectID": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#functions",
    "href": "teaching/math340/Lab1_matlab_taylor_series/Lab1_matlab_intro.html#functions",
    "title": "Lab 1 - Brief Introduction to Matlab",
    "section": "Functions",
    "text": "Functions\nfunction [output] = SumN(input)\n    % The names \"input\" and \"output\" are chosen to make it clear how this\n    % function works. The names don't matter as long as they are consistent.\n    N=input;\n    sum=0;\n    for i=1:N\n        % In each iteration we will add the new number to our sum\n        sum=sum+i;\n    end\n    output=sum;\nend;\nfunction [answer] = my_sin(x, N)\n    answer = 0;\n    for n=0:N\n        new_term = (-1)^n * (x^(2*n+1)) / factorial(2*n + 1);\n        answer = answer + new_term;\n    end\nend"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "This section contains various supplemental resources for the students in my classes in the Department of Mathematical Sciences at NJIT. They are organized by class."
  },
  {
    "objectID": "teaching/teaching.html#classes",
    "href": "teaching/teaching.html#classes",
    "title": "Teaching",
    "section": "Classes",
    "text": "Classes\n\nMath 111 - Calculus 1\nMath 340 - Applied Numerical Methods"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connor Robertson",
    "section": "",
    "text": "On this site you can find my CV, my LinkedIn and Github profiles, and some other little tidbits for teaching, research, seminars, and general exploration."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Connor Robertson",
    "section": "",
    "text": "If you are not redirected, click here."
  },
  {
    "objectID": "teaching/math111/quizzes/quiz6.html",
    "href": "teaching/math111/quizzes/quiz6.html",
    "title": "Quiz 6 - More chain rule and differentiation of trigonometric functions",
    "section": "",
    "text": "Differentiate \\(y = \\sin^{-1}(e^{-x}) + \\sec^{-1}(e^x)\\).\n\n\n\n\\[\n\\begin{align}\ny &= \\sin^{-1}(e^{-x}) + \\sec^{-1}(e^x) \\\\\n\\frac{d}{dx}y &= \\frac{d}{dx}\\left(\\sin^{-1}(e^{-x}) + \\sec^{-1}(e^x)\\right) \\\\\n&= \\frac{1}{\\sqrt{1-e^{-2x}}}\\frac{d}{dx}e^{-x} + \\frac{1}{e^{2x}\\sqrt{1-e^{-2x}}}\\frac{d}{dx}e^x \\\\\n&= -\\frac{e^{-x}}{\\sqrt{1-e^{-2x}}} + \\frac{e^x}{e^{2x}\\sqrt{1-e^{-2x}}} \\\\\n&= -\\frac{1}{e^x\\sqrt{1-e^{-2x}}} + \\frac{1}{e^{x}\\sqrt{1-e^{-2x}}} \\\\\n\\Aboxed{\\frac{d}{dx}y&= 0}\n\\end{align}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz7.html",
    "href": "teaching/math111/quizzes/quiz7.html",
    "title": "Quiz 7 - First derivative test and linearization",
    "section": "",
    "text": "Find the absolute maximum and minimum for the function \\(y = 6\\sqrt{x}-2x^{2/3}\\) on the interval \\(0\\leq x \\leq 4\\).\n\n\n\nFirst, we find the critical points on the interval: \\[\n\\begin{align}\ny' = 0 &=3(x)^{-1/2} - \\frac{4}{3}x^{-1/3} \\\\\n\\frac{4}{3}x^{-1/3} &= 3(x)^{-1/2} \\\\\n4 &= 9(x)^{-1/2 + 1/3} \\\\\n\\frac{4}{9} &= (x)^{-1/6} \\\\\n(x)^{1/6} &= \\frac{9}{4} \\\\\nx &= \\left(\\frac{9}{4}\\right)^6\n\\end{align}\n\\] This is obviously outside our interval. So, we check the endpoints: \\[\n\\begin{align}\ny(0) &= 0 \\\\\ny(4) &= 12 - 4\\sqrt[3]{2} \\\\\n\\end{align}\n\\] Thus, our minimum is \\(\\boxed{x=0}\\) and maximum is \\(\\boxed{x=4}\\)."
  },
  {
    "objectID": "teaching/math111/quizzes/quiz7.html#problem-2",
    "href": "teaching/math111/quizzes/quiz7.html#problem-2",
    "title": "Quiz 7 - First derivative test and linearization",
    "section": "Problem 2",
    "text": "Problem 2\n\nQuestion\nFind the linearization of \\(f(x) = e^{2x}\\) about the point \\(a=1\\) the use that to approximate \\(f(2)\\).\n\n\nSolution\nThe linearization form presented in the book is: \\(f(x) \\approx f(a) + f'(a)(x-a)\\): \\[\n\\begin{align}\nf(x) &\\approx f(a) + f'(a)(x-a) \\\\\n&= f(1) + f'(1)(x-1) \\\\\n\\Aboxed{f(x) &\\approx e^2 + 2e^2(x-1)}\n\\end{align}\n\\]\nUsing this, we have: \\[\nf(2) \\approx e^2 + 2e^2 = \\boxed{3e^2}\n\\]"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz8.html",
    "href": "teaching/math111/quizzes/quiz8.html",
    "title": "Quiz 8 - Curve sketching",
    "section": "",
    "text": "Given the function \\(y = 3x^4 - 4x^3\\),\n\n\nWhere is it increasing or decreasing?\n\n\n\nWhere is it concave up or down?\n\n\n\nWhat are the local minima and maxima?\n\n\n\nSketch the graph"
  },
  {
    "objectID": "teaching/math111/quizzes/quiz8.html#problem-2",
    "href": "teaching/math111/quizzes/quiz8.html#problem-2",
    "title": "Quiz 8 - Curve sketching",
    "section": "Problem 2",
    "text": "Problem 2\n\nQuestion\nGiven the function \\(y = 2 + 3x^2 - x^3\\),\n\na)\nWhere is it increasing or decreasing?\n\n\nb)\nWhere is it concave up or down?\n\n\nc)\nWhat are the local minima and maxima?\n\n\nd)\nSketch the graph\n\n\n\nSolution\n\\[\n\\begin{align}\n\\end{align}\n\\]"
  }
]