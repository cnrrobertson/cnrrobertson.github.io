{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Introduction to machine learning: set up and basics'\n",
        "csl: american-physics-society.csl\n",
        "bibliography: workshops.bib\n",
        "author: Connor Robertson\n",
        "execute:\n",
        "  keep-ipynb: true\n",
        "---"
      ],
      "id": "574018d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-tip collapse=\"true\"}\n",
        "## Other formats\n",
        "Download the workshop as a Jupyter notebook <a href=\"workshop1_intro.ipynb\" download>here</a>.\n",
        "\n",
        "Open the workshop as a Jupyter notebook in Google Colab [here](https://colab.research.google.com/drive/1rG9j0eWVt4ORoNcEKa89ccnZmYBLeDYb?usp=sharing) (with your NJIT Google account).\n",
        ":::\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "### Machine Learning and Optimization Seminar\n",
        "The goal of the Machine Learning and Optimization seminar this year is to expose the participants to topics in machine learning and optimization by:\n",
        "\n",
        "1. Facilitating hands-on workshops and group discussions to explore and gain experience\n",
        "2. Inviting speakers to introduce machine learning and optimization concepts\n",
        "\n",
        "We are excited to get started on this but recognize that since neither machine learning nor optimization are standardized in the department, participants will have a varied level of exposure to different topics.\n",
        "Our hope is that we can use this disparity of experience to increase collaboration during the workshops in a way that can't be achieved during the talks.\n",
        "All are encouraged to share their knowledge and experience with one another openly during the workshops and to give feedback to the organizers after.\n",
        "\n",
        "All workshop material will be available [here](machinelearning_optimization_seminar.qmd) for later reference.\n",
        "\n",
        "This first workshop is focused on tooling and the basic concepts of machine learning and optimization with the goal that everyone can be on the same footing for later workshops.\n",
        "\n",
        "## Your Python environment \n",
        "It is safe to say that Python is the language of choice for machine learning.\n",
        "This interpreted language has a very clear and high-level syntax, is extremely convenient for interactive programming and debugging, and has an enormous user base of enterprises, researchers, and hobbyists who have built an almost infinite collection of open-source packages for every topic.\n",
        "For these three reasons the workshops for this seminar will use Python.\n",
        "\n",
        "To get started, we will give the basics of the Python programming language. \n",
        "Just kidding!\n",
        "That would take too long.\n",
        "We will instead guide you on how to install Python most conveniently, teach you how to get started learning about Python, and then point you to a curated library of much more high-quality instruction for using Python.\n",
        "A list of some such references as well as documentation for setup and important Python packages can be found in the Appendix [here](#python-resources-and-packages).\n",
        "\n",
        "### Setting up\n",
        ":::{.callout-tip}\n",
        "If you are looking for the easiest and most immediate way to get going, check out the [Google Colab](#google-colab) section.\n",
        ":::\n",
        ":::{.callout-note}\n",
        "This section will require use of a terminal emulator for installing and running Python.\n",
        "If you are not familiar with the terminal, check out [this quick tutorial](https://mrkaluzny.com/blog/terminal-101-getting-started-with-terminal/) to get started.\n",
        "\n",
        "If you are using a computer with Windows, the terminal instructions may not apply.\n",
        ":::\n",
        "\n",
        "Python is installed by default on MacOS and most Linux distributions.\n",
        "However, it can be challenging to navigate between the versions and packages that your operating system uses and those needed for other projects.\n",
        "Thus, there are a variety of version, package, and environment management tools:\n",
        "\n",
        "- **Version management**: Which version of Python are you using? Can you change versions to run a specific Python program if it requires?\n",
        "    - `pyenv`\n",
        "    - `conda`/`mamba`\n",
        "- **Package management**: How can you install the many amazing Python packages people have created?\n",
        "    - `pip`\n",
        "    - `conda`/`mamba`\n",
        "- **Environment management**: If you have two projects that require different packages (or different versions of the same package), can you switch which packages are available depending on which project you are working on?\n",
        "    - `venv`\n",
        "    - `virtualenv`\n",
        "    - `poetry`\n",
        "    - `conda`/`mamba`\n",
        "    - many more\n",
        "    \n",
        "The `conda` package manager is the only one that fills all three roles.\n",
        "It is formally a part of the Anaconda Python distribution which is a favorite in the fields of data science and machine learning.\n",
        "`mamba` is a newer and faster rewrite used in exactly the same way and which is highly recommended.\n",
        "\n",
        "The best way to get started with `mamba` is to install `mambaforge`.\n",
        "You can find installer downloads for Windows, MacOS, or Linux [here](https://github.com/conda-forge/miniforge#mambaforge).\n",
        "\n",
        "For Windows, run the `.exe` file once it is downloaded.\n",
        "\n",
        "For MacOS and Linux, open a terminal and navigate to the download location:\n",
        "```bash\n",
        "cd ~/Downloads\n",
        "```\n",
        "Then run the installer as follows:\n",
        "```bash\n",
        "./Mambaforge-Linux-x86_64.sh\n",
        "```\n",
        "The installer will walk you through a few steps and end by asking if you'd like to \"initialize Mambaforge by running conda init?\"\n",
        "Answer yes and restart your terminal.\n",
        "This final command will have added `conda` and `mamba` to your system `$PATH` variable, which means it is available to your terminal.\n",
        "Once restarted, run `mamba -V` to print the version and to verify that the installation worked.\n",
        "\n",
        "### Environments\n",
        "The idea of a `conda`/`mamba` environment is that once an environment is created and activated, all new packages installed will be added to that environment and will be accessible to any Python program run while the environment is active.\n",
        "As an example, let's create an environment called `workshop` with a specific version of Python installed.\n",
        "The following will create the environment and install a specific version of `python`:\n",
        "```bash\n",
        "mamba create -n workshop python=3.9\n",
        "```\n",
        "Once created, we can list our environments via the command\n",
        "```bash\n",
        "mamba env list\n",
        "```\n",
        "```\n",
        "# conda environments:\n",
        "#\n",
        "base                     /home/user/mambaforge\n",
        "workshop                 /home/user/mambaforge/envs/workshop\n",
        "```\n",
        "Note that there is a \"base\" environment which is where `conda` and `mamba` themselves are installed as well as their dependencies.\n",
        "The best practice is to create an environment for each of your projects to minimize dependency issues (when packages require separate versions of the same package).\n",
        "\n",
        "To activate our new environment:\n",
        "```bash\n",
        "mamba activate workshop\n",
        "```\n",
        "Running `mamba env list` will now show our active environment via an asterisk:\n",
        "```\n",
        "base                     /home/user/mambaforge\n",
        "workshop              *  /home/user/mambaforge/envs/workshop\n",
        "```\n",
        "\n",
        "### Installing packages\n",
        "Now that we have activated the `workshop` `conda` environment, let's install some common machine learning packages in Python.\n",
        "It is as easy as writing:\n",
        "```bash\n",
        "mamba install numpy matplotlib pandas jupyter scipy scikit-learn scikit-image\n",
        "```\n",
        "This command will search the [conda-forge](https://conda-forge.org/#page-top) repository of packages and install the most up-to-date versions (the `forge` in `mambaforge`).\n",
        "\n",
        ":::{.callout-tip}\n",
        "Either `conda` or `mamba` could be used for all the commands discussed in this section.\n",
        "However, `mamba` is significantly faster when installing packages.\n",
        ":::\n",
        "\n",
        "Now that these packages have been installed, we can easily use them in an interactive `ipython` prompt (installed with the `jupyter` package):\n",
        "```bash\n",
        "ipython\n",
        "```\n",
        "```default\n",
        "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)\n",
        "Type 'copyright', 'credits' or 'license' for more information\n",
        "IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n",
        "\n",
        "In [1]: import numpy as np\n",
        "\n",
        "In [2]: import matplotlib.pyplot as plt\n",
        "\n",
        "In [3]: x = np.linspace(0,10,100)\n",
        "\n",
        "In [4]: y = np.sin(x)\n",
        "\n",
        "In [5]: plt.plot(x,y); plt.show()\n",
        "```\n",
        "This should plot a simple $\\sin$ curve.\n",
        "\n",
        "### Cleaning up\n",
        "After we are done using the environment that has our desired version of Python and the needed packages, we can go back to our regular terminal by deactivating the environment:\n",
        "```bash\n",
        "mamba deactivate workshop\n",
        "```\n",
        "If we have somehow broken our environment and need to remove it:\n",
        "```bash\n",
        "mamba env remove -n workshop\n",
        "```\n",
        "There are many more commands and functionalities that `conda` and `mamba` provide that can be found in the [python resources and packages](#python-resources-and-packages) section of the Appendix.\n",
        "\n",
        "### Google Colab\n",
        "As an alternative to the entire procedure above, you can use an online [Jupyter Notebook](https://jupyter.org) service hosted by Google called [Colab](https://colab.research.google.com).\n",
        "This service will get you up and running immediately but cannot save your environment between notebooks and has limited functionality to run scripts or save data.\n",
        "Thus, if your notebook requires a package that is not installed by default, you will need to add the installation command in one of the first notebook cells.\n",
        "For example, to install the [reservoirpy]() package, we would write in a notebook cell:\n",
        "```default\n",
        "!pip install reservoirpy\n",
        "```\n",
        "In a notebook, the `!` denotes a terminal command.\n",
        "The package will now be ready for import and use within the current notebook session:\n",
        "```default\n",
        "from reservoirpy.nodes import Input,Reservoir,Ridge\n",
        "```\n",
        "\n",
        "## Basic concepts of machine learning \n",
        "\n",
        "Machine learning is, at its most basic, automated data analysis, usually with the goal of finding patterns or making predictions.\n",
        "The \"machines\" in this analysis are equations or algorithms and the \"learning\" is usually some form of parameter selection and/or fitting.\n",
        "Due to the uncertain nature of most data, the majority of these models are probabilistic in nature.\n",
        "In fact, it can be hard to distinguish the methodological lines between what is termed \"machine learning\" and the field of statistics.\n",
        "However, there are some important distinctions between the tools, goals, and terminology of the two areas.\n",
        "Today, machine learning has emerged as a broad description of almost any data-driven computing which may or may not include classical descriptive and inferential statistics [@murphy2012machine].\n",
        "\n",
        "At first glance, machine learning can be separated into three main classes: \n",
        "\n",
        "- **Supervised learning**: Given dependent and independent variable data, train a model which effectively maps the independent variable data to produce the dependent variable data.\n",
        "    - Generalized linear models (linear, logistic, etc. regression)\n",
        "    - Naive Bayes\n",
        "    - Neural networks (most)\n",
        "    - Support vector machine (SVM)\n",
        "    - Random forests\n",
        "    - etc.\n",
        "- **Unsupervised learning**: Given data, find patterns (no specified output, though there is still a measure of success)\n",
        "    - Clustering\n",
        "    - Mixture models\n",
        "    - Dimensionality reduction\n",
        "    - Association rules\n",
        "    - etc.\n",
        "- **Reinforcement learning**: Given input data and desired outcomes, simulate and use the results to update a model to improve the simulation's ability to achieve those outcomes\n",
        "    - Q-learning\n",
        "    - SARSA\n",
        "    - etc.\n",
        "\n",
        "There is an enormous amount of interest in machine learning methods currently, thus there is also an enormous amount of high-quality material discussing it.\n",
        "We will end our introduction here and direct you to established textbooks [@james2013introduction;@hastie2009elements;@murphy2012machine], NJIT classes (Math 478, Math 678, Math 680, CS 675, CS 677), and online resources (too many to even start listing).\n",
        "\n",
        "### General procedure\n",
        "In practice, machine learning algorithms often boil down to an optimization problem.\n",
        "To characterize this in a few steps, consider a problem with data $x$:\n",
        "\n",
        "1. Select a model representation $f$ with parameters $p$ for the problem:\n",
        "$$\n",
        "y = f(x;p)\n",
        "$$\n",
        "2. Determine an appropriate objective function $\\mathcal{L}$:\n",
        "$$\n",
        "\\mathcal{L}(f(x;p),x)\n",
        "$$\n",
        "3. Use an optimization method $\\mathcal{O}$ with parameters $d$ to find parameters $p$ that minimize or maximize the objective for the model:\n",
        "$$\n",
        "p^* = \\mathcal{O}(\\mathcal{L},f,x;d)\n",
        "$$\n",
        "\n",
        "In some sense, this is the same procedure as _inverse problems_ in traditional applied mathematics but with a broader set of models $f$ that may or may not be based on first-principles understanding of the problem.\n",
        "\n",
        "### Incorporating data\n",
        "There are a variety of choices for models $f$ and objectives $\\mathcal{L}$ depending on the class of problem being considered (supervised, unsupervised, or reinforcement).\n",
        "For supervised learning (the most common), the objective is often to predict or generate the output or dependent variable data of some process.\n",
        "For this, data is usually separated into three sets:\n",
        "\n",
        "1. **Training data** ($x$): used to tune the parameters $p$\n",
        "2. **Validation data** ($x^v$): used to evaluate the generalization of the model $f$ to data not in the training set during training\n",
        "3. **Testing data** ($x^t$): used to benchmark the predictive or generative ability of the model after training is completed\n",
        "\n",
        "## A first machine learning problem\n",
        "These workshops are about learning by doing, so let's build understanding by fitting a simple \"machine\" to some data as a supervised problem.\n",
        "Consider some data $(x,y)$:"
      ],
      "id": "61cc66ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def f_known(x):\n",
        "    part1 = np.exp(-x)*(np.sin((x)**3) + np.sin((x)**2) - x)\n",
        "    part2 = 1/(1 + np.exp(-1*(x-1)))\n",
        "    return part1 + part2\n",
        "xsamples = np.random.uniform(-1/2,5,100)\n",
        "ysamples = f_known(xsamples)\n",
        "plt.scatter(xsamples,ysamples)\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Data\")\n",
        "plt.show()"
      ],
      "id": "15e80757",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We would like to fit a model of the following form to this data:\n",
        "$$\n",
        "f(x;p_0,p_1) = e^{-p_0x}(\\sin((p_0x)^3) + \\sin((p_0x)^2) - p_0x) + \\frac{1}{1 + e^{-p_1(x-1)}}\n",
        "$$\n",
        "To formulate this as a machine learning/optimization problem, we can consider a simple $L^2$ objective/loss in which we would like to minimize the $L^2$ norm distance between the model output $f(x)$ and the true data $y$:\n",
        "$$\n",
        "\\mathcal{L(f(x;\\vec{p}),y)} = ||f(x) - y||_2^2\n",
        "$$\n",
        "The problem can then be written as the unconstrained optimization problem:\n",
        "$$\n",
        "p^* = \\underset{\\vec{p}}{\\text{minimize }} \\mathcal{L}(f(x;\\vec{p}),y)\n",
        "$$\n",
        "We then expect our model $f(x;p^*)$ to represent a \"machine\" that has accurately \"learned\" the relationship between $x$ and $y$.\n",
        "\n",
        "There are several ways to approach this problem, but a simple and popular approach for a continuous and unconstrained problem is to use an iterative gradient method.\n",
        "\n",
        "### Gradient descent\n",
        "_Gradient descent_ is a straightforward method taught early in an undergraduate numerical methods class.\n",
        "Its simplicity and cheap computational cost has made it popular for machine learning methods (which can contain so many parameters that computing the Hessian for second-order methods like Newton's method becomes infeasible).\n",
        "Beginning with an initial parameter guess $\\vec{p}_0$, its update procedure can be written as:\n",
        "$$\n",
        "\\begin{align*}\n",
        "v^i &= -\\alpha \\nabla_p \\mathcal{L} \\\\\n",
        "\\vec{p}^{i+1} &= \\vec{p}^i + v^i\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\alpha^i$ controls the step size in the direction of the gradient (usually called a \"learning rate\" in machine learning).\n",
        "This method will follow the gradient of the objective/loss $\\mathcal{L}$ until the objective is sufficiently small, or until it reaches a steady state.\n",
        "\n",
        "Simply implemented in Python, this method can be written as:"
      ],
      "id": "981a5d55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(fp,x0,lr=.2,tol=1e-12,steps=10000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    for s in range(steps):\n",
        "        xnew = x - lr*fp(x)\n",
        "        if np.linalg.norm(fp(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "2824ab6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, this method contains a troublesome parameter $\\alpha^i$ which, if chosen too large, could prevent convergence of the solution or, if chosen too small, could require an unreasonable number of steps to converge.\n",
        "For this reason, \"vanilla\" (or normal) gradient descent is almost always replaced with a modified method in learning problems[@sebastian_ruder_2020;@john_chen_2020].\n",
        "\n",
        "The following demonstrates an animation of the above gradient descent method applied to our data with two different learning rates, one successful, one not.\n",
        "It uses the following animation code and the `autograd` automatic differentiation library that will be further discussed later:"
      ],
      "id": "2a0144ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Animation code\n",
        "from matplotlib import animation as anim\n",
        "from matplotlib import gridspec\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def animate_steps_2d(xs,func,xmin=-.1,xmax=2.5,ymin=-1,ymax=3,interval=50,path=True):\n",
        "    def anim_func(i):\n",
        "        ax.clear()\n",
        "        ax1.clear()\n",
        "        ax2.clear()\n",
        "        # Add surface plot\n",
        "        ax.plot_surface(X,Y,Z,cmap=\"gist_earth\")\n",
        "        x_loss = loss(xs[i])\n",
        "        ax.scatter(xs[i][0],xs[i][1],x_loss,zorder=100,color=\"red\",s=100)\n",
        "        ax.set_xlabel(\"$p_1$\")\n",
        "        ax.set_ylabel(\"$p_2$\")\n",
        "        ax.set_zlabel(\"loss\")\n",
        "        if path:\n",
        "            temp_x1 = [xs[j][0] for j in range(i)]\n",
        "            temp_x2 = [xs[j][1] for j in range(i)]\n",
        "            temp_losses = [loss(xs[j]) for j in range(i)]\n",
        "            ax.plot(temp_x1,temp_x2,temp_losses,color=\"orange\")\n",
        "        loss_fx = [func([fxs[j],xs[i][1]]) for j in range(len(fxs))]\n",
        "        loss_fy = [func([xs[i][0],fys[j]]) for j in range(len(fys))]\n",
        "        # Add flat plots for perspective\n",
        "        ax1.plot(fxs,loss_fx)\n",
        "        ax1.scatter(xs[i][0],x_loss,color=\"red\",s=100,zorder=100)\n",
        "        ax1.set_xlabel(\"$p_1$\")\n",
        "        ax1.set_ylabel(\"loss\")\n",
        "        ax1.set_xlim(np.min(X),np.max(X))\n",
        "        ax1.set_ylim(np.min(Z),np.max(Z))\n",
        "        ax2.plot(fys,loss_fy)\n",
        "        ax2.scatter(xs[i][1],x_loss,color=\"red\",s=100,zorder=100)\n",
        "        ax2.set_xlabel(\"$p_2$\")\n",
        "        ax2.set_ylabel(\"loss\")\n",
        "        ax2.set_xlim(np.min(Y),np.max(Y))\n",
        "        ax2.set_ylim(np.min(Z),np.max(Z))\n",
        "\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    gs = gridspec.GridSpec(12,20)\n",
        "    ax = fig.add_subplot(gs[0:12,0:16],projection=\"3d\",computed_zorder=False)\n",
        "    ax1 = fig.add_subplot(gs[0:5,16:20])\n",
        "    ax2 = fig.add_subplot(gs[7:12,16:20])\n",
        "    ax.view_init(47,47)\n",
        "\n",
        "    fxs = np.linspace(xmin,xmax,100)\n",
        "    fys = np.linspace(ymin,ymax,100)\n",
        "    X,Y = np.meshgrid(fxs,fys)\n",
        "    Z = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Z[i,j] = func([X[i,j],Y[i,j]])\n",
        "\n",
        "    tanim = anim.FuncAnimation(fig,anim_func,interval=50,frames=len(xs))\n",
        "    plt.show()"
      ],
      "id": "7465d506",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| panel: fill\n",
        "from autograd import grad\n",
        "import autograd.numpy as anp\n",
        "def f_model(p):\n",
        "    part1 = anp.exp(-p[0]*xsamples)*(anp.sin((p[0]*xsamples)**3) + anp.sin((p[0]*xsamples)**2) - p[0]*xsamples) \n",
        "    part2 = 1/(1 + anp.exp(-p[1]*(xsamples-1)))\n",
        "    return part1 + part2\n",
        "loss = lambda p: anp.sum((f_model(p) - ysamples)**2)\n",
        "grad_loss = grad(loss) # automatically differentiated\n",
        "\n",
        "p0 = np.array([.7,.2])\n",
        "xs = gradient_descent(grad_loss,p0,.01,tol=1e-8,steps=1000)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "55e3574f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adaptive steps (adagrad, adadelta, RMSprop)\n",
        "\n",
        "### With momentum (momentum, Nesterov acceleration)\n",
        "\n",
        "### Combining ideas (Adam and more)\n",
        "<!-- TODO: Simple overview of adding momentum to gradient descent -->\n",
        "<!-- TODO: nesterov acceleration https://www.codingninjas.com/codestudio/library/nesterov-accelerated-gradient-->\n",
        "<!-- TODO: RMSProp https://optimization.cbe.cornell.edu/index.php?title=RMSProp -->\n",
        "<!-- TODO: ADAM (combo of momentum and RMSProp averaging) https://medium.com/analytics-vidhya/momentum-rmsprop-and-adam-optimizer-5769721b4b19 -->\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "### Future workshop schedule\n",
        "To keep your interest as we begin on this fairly generic beginning, the schedule for workshops for the semester is planned as follows:\n",
        "\n",
        "## Appendix\n",
        "\n",
        "### Python resources and packages\n",
        "**Python**\n",
        "\n",
        "- [Python documentation (especially sections 3-9)](https://docs.python.org/3.9/tutorial/introduction.html)\n",
        "- [Quick cheatsheet of general Python knowledge](https://www.pythoncheatsheet.org)\n",
        "- [Quicker introduction](https://learnxinyminutes.com/docs/python/)\n",
        "\n",
        "**Conda/mamba**\n",
        "\n",
        "- [Conda user guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html)\n",
        "- [Mamba website](https://mamba.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "**Essential packages**\n",
        "\n",
        "- [`numpy`](https://numpy.org) - Creating, manipulating, and operating (linear algebra, fft, etc.) on multi-dimensional arrays. A list of packages built on `numpy` for a variety of domains can be found on the homepage under the _ECOSYSTEM_ heading.\n",
        "- [`scipy`](https://scipy.org) - Fundamentals in optimization, integration, interpolation, differential equations, statistics, signal processing, etc.\n",
        "- [`matplotlib`](https://matplotlib.org) - Static or interactive plots and animations\n",
        "- [`scikit-learn`](https://scikit-learn.org/stable/index.html) - Standard machine learning tools and algorithms built on `numpy`, `scipy`, and `matplotlib`\n",
        "- [`pandas`](https://pandas.pydata.org) - Easily represent, manipulate, and visualize structured datasets (matrices with names for columns and rows)\n",
        "- [`keras`](https://keras.io) - High level neural network framework built on `tensorflow`\n",
        "- [`tensorflow`](https://www.tensorflow.org) - In depth neural network framework focused on ease and production\n",
        "- [`pytorch`](https://pytorch.org) - In depth neural network framework focused on facilitating the path from research to production\n",
        "- [`scikit-image`](https://scikit-image.org) - Image processing algorithms and tools\n",
        "- [`jupyter`](https://jupyter.org) - Interactive \"notebook\" style programming\n",
        "\n",
        "### Julia as an alternative to Python\n",
        "Julia is a fairly new language that has been mainly proposed as an alternative to Python and Matlab, though it is general use.\n",
        "Its strength and its weakness is that it is \"just-in-time\" compiled (meaning your code is automatically analyzed and compiled just before it is run).\n",
        "A clever language design combined with just-in-time compilation makes Julia as clear to read and write as Python while being much faster.\n",
        "It can even approach the speed of C when written carefully.\n",
        "However, the just-in-time compilation and type system remove a chunk of the interactive convenience of Python and its young age also means that it does not have the volume of packages that Python does.\n",
        "\n",
        "Nonetheless, it is an elegant and high-performance language to use and has shown rapid growth recently.\n",
        "Concise, simple, and easy to read and contribute to packages have been quickly emerging and it already provides many useful tools.\n",
        "As a result, it is worth describing it's installation process, environment management, and noteable packages.\n",
        "\n",
        "#### Installation\n",
        "The officially supported method of installation for Julia is now using the `juliaup` version manager.\n",
        "The [installer](https://github.com/JuliaLang/juliaup#windows) can be downloaded from the Windows store on Windows or run on MacOS or Linux with:\n",
        "\n",
        "```bash\n",
        "curl -fsSL https://install.julialang.org | sh\n",
        "```\n",
        "#### Environments\n",
        "Julia comes with a standard environment and package manager named [`Pkg`](https://pkgdocs.julialang.org/v1/).\n",
        "Interestingly, the easiest way to use it is to run the Julia REPL (read-eval-print-loop), i.e. to run `julia` interactively.\n",
        "You can do so by typing `julia` into the terminal.\n",
        "You will then be presented with a terminal interface such as:\n",
        "```default\n",
        "   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n",
        "  (_)     | (_) (_)    |\n",
        "   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n",
        "  | | | | | | |/ _` |  |\n",
        "  | | |_| | | | (_| |  |  Version 1.8.0 (2022-08-17)\n",
        " _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n",
        "|__/                   |\n",
        "\n",
        "julia>\n",
        "```\n",
        "Typing `]` will put you into \"`Pkg` mode\":\n",
        "```default\n",
        "(@v1.8) pkg>\n",
        "```\n",
        "Type `?` and hit enter to get options in this mode.\n",
        "We can create and activate a new environment called workshop with the command:\n",
        "```default\n",
        "(@v1.8) pkg> activate --shared workshop\n",
        "```\n",
        "Note that the `--shared` flag will make a \"global\" environment that can be accessed from any directory.\n",
        "If we were to leave out this flag, `Pkg` would put a `Project.toml` and `Manifest.toml` file in the current directory that contain the name of the environment, its installed packages, and their dependencies.\n",
        "This can be useful to easily isolate and share environments.\n",
        "After running this command, our `Pkg` mode will have changed to represent the active environment:\n",
        "```default\n",
        "(@workshop) pkg>\n",
        "```\n",
        "\n",
        "#### Installing packages\n",
        "To install some packages in the active environment, write:\n",
        "```default\n",
        "(@workshop) pkg> add Plots MLJ DataFrames Flux Pluto\n",
        "```\n",
        "These packages will install and precompile.\n",
        "To test one of them, press backspace to leave `Pkg` mode and input:\n",
        "```default\n",
        "julia> using Plots\n",
        "[ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
        "julia> x = range(0,10,100);\n",
        "julia> y = sin.(x);\n",
        "julia> plot(x,y)\n",
        "```\n",
        "This should show a plot of a simple $\\sin$ curve.\n",
        "Note that the precompilation of `Plots` took some time.\n",
        "However, this will not need to occur again until the package is updated.\n",
        "Also note that the call to `plot(x,y)` took some time.\n",
        "This is due to the just-in-time compilation.\n",
        "Now that the compilation has been done for inputs of the types of `x` and `y`, if you run `plot(x,y)` again, it should be almost instantaneous.\n",
        "\n",
        "#### Cleaning up\n",
        "To deactivate the environment, enter the `Pkg` mode again by pressing `]` on an empty line, then enter:\n",
        "\n",
        "```default\n",
        "(@workshop) pkg> activate\n",
        "```\n",
        "To delete the environment we created you can delete the environment folder at the listed location at creation.\n",
        "This is usually `/home/user/.julia/environments/workshop` on MacOS or Linux.\n",
        "\n",
        "#### References\n",
        "\n",
        "**Julia**\n",
        "\n",
        "- [Julia documentation](https://docs.julialang.org/en/v1/)\n",
        "- [Quick cheatsheet of Julia](https://juliadocs.github.io/Julia-Cheat-Sheet/)\n",
        "- [Comparison of the syntax of Julia, Python, and Matlab](https://cheatsheets.quantecon.org)\n",
        "\n",
        "**Packages**\n",
        "\n",
        "As compared to Python, Julia has many scientific computing tools built into its standard library.\n",
        "Thus, a lot of the functionality found in `numpy` are loaded by default.\n",
        "On the other hand, because of the interoperability of the language and the reduced need for a polyglot codebase (i.e. needing C and Fortran code for a Python package to be fast), packages are usually much smaller modules in Julia.\n",
        "For example, the functionality of the `scipy` package in Python can be found spread across possibly a dozen different packages in Julia.\n",
        "This is convenient to only load and use what you need, but inconvenient in that it may require more searching to find and the interfaces may not be standardized.\n",
        "The following are some packages that roughly recreate the essential Python packages [here](#python-resources-and-packages). \n",
        "\n",
        "- `numpy` - [Standard library](https://docs.julialang.org/en/v1/manual/arrays/),[`FFTW.jl`](https://juliamath.github.io/FFTW.jl/latest/)\n",
        "- `scipy` - [`Statistics.jl`]()\n",
        "- `matplotlib` - [`Plots.jl`](https://docs.juliaplots.org/latest/)\n",
        "- `scikit-learn` - [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
        "- `pandas` - [`DataFrames.jl`](https://dataframes.juliadata.org/stable/)\n",
        "- `keras`,`tensorflow`,`pytorch` - [`Flux.jl`](https://fluxml.ai/Flux.jl/stable/)\n",
        "- `scikit-image` - [`Images.jl`](https://juliaimages.org/latest/install/)\n",
        "- `jupyter` - [`Pluto.jl`](https://github.com/fonsp/Pluto.jl) although you can use Julia with Jupyter via [`IJulia.jl`](https://julialang.github.io/IJulia.jl/stable/)"
      ],
      "id": "2543032f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}