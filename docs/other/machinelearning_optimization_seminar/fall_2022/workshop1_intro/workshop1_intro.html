<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Connor Robertson">

<title>Connor Robertson - Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../assets/circle-svgrepo-com.svg" rel="icon" type="image/svg+xml">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Connor Robertson</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../assets/CV_Connor_Robertson_5_22.pdf">CV</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../teaching/teaching.html">Teaching</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../research/research.html">Research</a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other" role="button" data-bs-toggle="dropdown" aria-expanded="false">Other</a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-other">    
        <li>
    <a class="dropdown-item" href="../../../../other/fun/fun.html">
 <span class="dropdown-text">Just for Fun</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../other/machinelearning_optimization_seminar/machinelearning_optimization_seminar.html">
 <span class="dropdown-text">Machine Learning and Optimization Seminar</span></a>
  </li>  
    </ul>
  </li>
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../other/machinelearning_optimization_seminar/machinelearning_optimization_seminar.html" class="sidebar-item-text sidebar-link">Machine Learning and Optimization Seminar</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/machinelearning_optimization_seminar/fall_2022/workshop1_intro/workshop1_intro.html" class="sidebar-item-text sidebar-link active">Workshop 1: Python set up, basics, gradient descent, automatic differentiation</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#machine-learning-and-optimization-seminar" id="toc-machine-learning-and-optimization-seminar" class="nav-link" data-scroll-target="#machine-learning-and-optimization-seminar">Machine Learning and Optimization Seminar</a></li>
  </ul></li>
  <li><a href="#your-python-environment" id="toc-your-python-environment" class="nav-link" data-scroll-target="#your-python-environment">Your Python environment</a>
  <ul class="collapse">
  <li><a href="#setting-up" id="toc-setting-up" class="nav-link" data-scroll-target="#setting-up">Setting up</a></li>
  <li><a href="#environments" id="toc-environments" class="nav-link" data-scroll-target="#environments">Environments</a></li>
  <li><a href="#installing-packages" id="toc-installing-packages" class="nav-link" data-scroll-target="#installing-packages">Installing packages</a></li>
  <li><a href="#cleaning-up" id="toc-cleaning-up" class="nav-link" data-scroll-target="#cleaning-up">Cleaning up</a></li>
  <li><a href="#google-colab" id="toc-google-colab" class="nav-link" data-scroll-target="#google-colab">Google Colab</a></li>
  </ul></li>
  <li><a href="#basic-concepts-of-machine-learning" id="toc-basic-concepts-of-machine-learning" class="nav-link" data-scroll-target="#basic-concepts-of-machine-learning">Basic concepts of machine learning</a>
  <ul class="collapse">
  <li><a href="#general-procedure" id="toc-general-procedure" class="nav-link" data-scroll-target="#general-procedure">General procedure</a></li>
  <li><a href="#incorporating-data" id="toc-incorporating-data" class="nav-link" data-scroll-target="#incorporating-data">Incorporating data</a></li>
  </ul></li>
  <li><a href="#a-first-machine-learning-problem" id="toc-a-first-machine-learning-problem" class="nav-link" data-scroll-target="#a-first-machine-learning-problem">A first machine learning problem</a>
  <ul class="collapse">
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#adaptive-steps" id="toc-adaptive-steps" class="nav-link" data-scroll-target="#adaptive-steps">Adaptive steps</a></li>
  <li><a href="#with-momentum" id="toc-with-momentum" class="nav-link" data-scroll-target="#with-momentum">With momentum</a></li>
  <li><a href="#combining-ideas" id="toc-combining-ideas" class="nav-link" data-scroll-target="#combining-ideas">Combining ideas</a></li>
  <li><a href="#automatic-differentiation" id="toc-automatic-differentiation" class="nav-link" data-scroll-target="#automatic-differentiation">Automatic differentiation</a></li>
  </ul></li>
  <li><a href="#further-exploration" id="toc-further-exploration" class="nav-link" data-scroll-target="#further-exploration">Further Exploration</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#python-resources-and-packages" id="toc-python-resources-and-packages" class="nav-link" data-scroll-target="#python-resources-and-packages">Python resources and packages</a></li>
  <li><a href="#julia-as-an-alternative-to-python" id="toc-julia-as-an-alternative-to-python" class="nav-link" data-scroll-target="#julia-as-an-alternative-to-python">Julia as an alternative to Python</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Connor Robertson </p>
          </div>
  </div>
    
    
  </div>
  

</header>

<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Other formats
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Download the workshop as a Jupyter notebook <a href="workshop1_intro.ipynb" download="">here</a>.</p>
<p>After downloading the workshop Jupyter notebook, you can upload it to <a href="https://colab.research.google.com/">Google Colab</a> to get a quick start, but you will not be able to see the animations.</p>
</div>
</div>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<section id="machine-learning-and-optimization-seminar" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning-and-optimization-seminar">Machine Learning and Optimization Seminar</h3>
<p>The goal of the Machine Learning and Optimization seminar this year is to expose the participants to topics in machine learning and optimization by:</p>
<ol type="1">
<li>Facilitating hands-on workshops and group discussions to explore and gain experience</li>
<li>Inviting speakers to introduce machine learning and optimization concepts</li>
</ol>
<p>We are excited to get started on this but recognize that since neither machine learning nor optimization are standardized in the department, participants will have a varied level of exposure to different topics. Our hope is that we can use this disparity of experience to increase collaboration during the workshops in a way that can’t be achieved during the talks. All are encouraged to share their knowledge and experience with one another openly during the workshops and to give feedback to the organizers after.</p>
<p>All workshop material will be available <a href="machinelearning_optimization_seminar.qmd">here</a> for later reference.</p>
<p>This first workshop is focused on tooling and the basic concepts of machine learning and optimization with the goal that everyone can be on the same footing for later workshops.</p>
</section>
</section>
<section id="your-python-environment" class="level2">
<h2 class="anchored" data-anchor-id="your-python-environment">Your Python environment</h2>
<p>It is safe to say that Python is the language of choice for machine learning. This interpreted language has a very clear and high-level syntax, is extremely convenient for interactive programming and debugging, and has an enormous user base of enterprises, researchers, and hobbyists who have built an almost infinite collection of open-source packages for every topic. For these three reasons the workshops for this seminar will use Python.</p>
<p>To get started, we will give the basics of the Python programming language. Just kidding! That would take too long. We will instead guide you on how to install Python most conveniently, teach you how to get started learning about Python, and then point you to a curated library of much more high-quality instruction for using Python. A list of some such references as well as documentation for setup and important Python packages can be found in the Appendix <a href="#python-resources-and-packages">here</a>.</p>
<section id="setting-up" class="level3">
<h3 class="anchored" data-anchor-id="setting-up">Setting up</h3>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are looking for the easiest and most immediate way to get going with a Python Jupyter Notebook, check out the <a href="#google-colab">Google Colab</a> section.</p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section will require use of a terminal emulator for installing and running Python. If you are not familiar with the terminal, check out <a href="https://mrkaluzny.com/blog/terminal-101-getting-started-with-terminal/">this quick tutorial</a> to get started.</p>
<p>If you are using a computer with Windows, the terminal instructions may not apply.</p>
</div>
</div>
<p>Python is installed by default on MacOS and most Linux distributions. However, it can be challenging to navigate between the versions and packages that your operating system uses and those needed for other projects. Thus, there are a variety of version, package, and environment management tools:</p>
<ul>
<li><strong>Version management</strong>: Which version of Python are you using? Can you change versions to run a specific Python program if it requires?
<ul>
<li><code>pyenv</code></li>
<li><code>conda</code>/<code>mamba</code></li>
</ul></li>
<li><strong>Package management</strong>: How can you install the many amazing Python packages people have created?
<ul>
<li><code>pip</code></li>
<li><code>conda</code>/<code>mamba</code></li>
</ul></li>
<li><strong>Environment management</strong>: If you have two projects that require different packages (or different versions of the same package), can you switch which packages are available depending on which project you are working on?
<ul>
<li><code>venv</code></li>
<li><code>virtualenv</code></li>
<li><code>poetry</code></li>
<li><code>conda</code>/<code>mamba</code></li>
<li>many more</li>
</ul></li>
</ul>
<p>The <code>conda</code> package manager is the only one that fills all three roles. It is formally a part of the Anaconda Python distribution which is a favorite in the fields of data science and machine learning. <code>mamba</code> is a newer and faster rewrite used in exactly the same way and which is highly recommended.</p>
<p>The best way to get started with <code>mamba</code> is to install <code>mambaforge</code>. You can find installer downloads for Windows, MacOS, or Linux <a href="https://github.com/conda-forge/miniforge#mambaforge">here</a>.</p>
<p>For Windows, run the <code>.exe</code> file once it is downloaded.</p>
<p>For MacOS and Linux, open a terminal and navigate to the download location:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~/Downloads</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then run the installer as follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./Mambaforge-Linux-x86_64.sh</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The installer will walk you through a few steps and end by asking if you’d like to “initialize Mambaforge by running conda init?” Answer yes and restart your terminal. This final command will have added <code>conda</code> and <code>mamba</code> to your system <code>$PATH</code> variable, which means it is available to your terminal. Once restarted, run <code>mamba -V</code> to print the version and to verify that the installation worked.</p>
</section>
<section id="environments" class="level3">
<h3 class="anchored" data-anchor-id="environments">Environments</h3>
<p>The idea of a <code>conda</code>/<code>mamba</code> environment is that once an environment is created and activated, all new packages installed will be added to that environment and will be accessible to any Python program run while the environment is active. As an example, let’s create an environment called <code>workshop</code> with a specific version of Python installed. The following will create the environment and install a specific version of <code>python</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">-n</span> workshop python=3.9</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once created, we can list our environments via the command</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> env list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code># conda environments:
#
base                     /home/user/mambaforge
workshop                 /home/user/mambaforge/envs/workshop</code></pre>
<p>Note that there is a “base” environment which is where <code>conda</code> and <code>mamba</code> themselves are installed as well as their dependencies. The best practice is to create an environment for each of your projects to minimize dependency issues (when packages require separate versions of the same package).</p>
<p>To activate our new environment:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate workshop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running <code>mamba env list</code> will now show our active environment via an asterisk:</p>
<pre><code>base                     /home/user/mambaforge
workshop              *  /home/user/mambaforge/envs/workshop</code></pre>
</section>
<section id="installing-packages" class="level3">
<h3 class="anchored" data-anchor-id="installing-packages">Installing packages</h3>
<p>Now that we have activated the <code>workshop</code> <code>conda</code> environment, let’s install some common machine learning packages in Python. It is as easy as writing:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> install numpy matplotlib pandas jupyter scipy scikit-learn scikit-image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This command will search the <a href="https://conda-forge.org/#page-top">conda-forge</a> repository of packages and install the most up-to-date versions (the <code>forge</code> in <code>mambaforge</code>).</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Either <code>conda</code> or <code>mamba</code> could be used for all the commands discussed in this section. However, <code>mamba</code> is significantly faster when installing packages.</p>
</div>
</div>
<p>Now that these packages have been installed, we can easily use them in an interactive <code>ipython</code> prompt (installed with the <code>jupyter</code> package):</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ipython</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Type 'copyright', 'credits' or 'license' for more information</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>In [1]: import numpy as np</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>In [2]: import matplotlib.pyplot as plt</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>In [3]: x = np.linspace(0,10,100)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>In [4]: y = np.sin(x)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>In [5]: plt.plot(x,y); plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This should plot a simple <span class="math inline">\(\sin\)</span> curve.</p>
</section>
<section id="cleaning-up" class="level3">
<h3 class="anchored" data-anchor-id="cleaning-up">Cleaning up</h3>
<p>After we are done using the environment that has our desired version of Python and the needed packages, we can go back to our regular terminal by deactivating the environment:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> deactivate workshop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we have somehow broken our environment and need to remove it:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> env remove <span class="at">-n</span> workshop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>There are many more commands and functionalities that <code>conda</code> and <code>mamba</code> provide that can be found in the <a href="#python-resources-and-packages">python resources and packages</a> section of the Appendix.</p>
</section>
<section id="google-colab" class="level3">
<h3 class="anchored" data-anchor-id="google-colab">Google Colab</h3>
<p>As an alternative to the entire procedure above, you can use an online <a href="https://jupyter.org">Jupyter Notebook</a> service hosted by Google called <a href="https://colab.research.google.com">Colab</a>. This service will get you up and running immediately but cannot save your environment between notebooks and has limited functionality to run scripts, save data, view animations, change package versions, etc. Thus, if your notebook requires a package that is not installed by default, you will need to add the installation command in one of the first notebook cells. For example, to install the <a href="">reservoirpy</a> package, we would write in a notebook cell:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>!pip install reservoirpy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In a notebook, the <code>!</code> denotes a terminal command. The package will now be ready for import and use within the current notebook session:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>from reservoirpy.nodes import Input,Reservoir,Ridge</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="basic-concepts-of-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="basic-concepts-of-machine-learning">Basic concepts of machine learning</h2>
<p>Machine learning is, at its most basic, automated data analysis, usually with the goal of finding patterns or making predictions. The “machines” in this analysis are equations or algorithms and the “learning” is usually some form of parameter selection and/or fitting. Due to the uncertain nature of most data, the majority of these models are probabilistic in nature. In fact, it can be hard to distinguish the methodological lines between what is termed “machine learning” and the field of statistics. However, there are a few important distinctions between the tools, goals, and terminology of the two areas. Today, machine learning has emerged as a broad description of almost any data-driven computing which may or may not include classical descriptive and inferential statistics<span class="citation" data-cites="murphy2012machine">&nbsp;[<a href="#ref-murphy2012machine" role="doc-biblioref">1</a>]</span>.</p>
<p>At first glance, machine learning can be separated into three main classes:</p>
<ul>
<li><strong>Supervised learning</strong>: Given dependent and independent variable data, train a model which effectively maps the independent variable data to produce the dependent variable data.
<ul>
<li>Generalized linear models (linear, logistic, etc. regression)</li>
<li>Naive Bayes</li>
<li>Neural networks (most)</li>
<li>Support vector machine (SVM)</li>
<li>Random forests</li>
<li>etc.</li>
</ul></li>
<li><strong>Unsupervised learning</strong>: Given data, find patterns (no specified output, though there is still a measure of success)
<ul>
<li>Clustering</li>
<li>Mixture models</li>
<li>Dimensionality reduction</li>
<li>Association rules</li>
<li>etc.</li>
</ul></li>
<li><strong>Reinforcement learning</strong>: Given input data and desired outcomes, simulate and use the results to update a model to improve the simulation’s ability to achieve those outcomes
<ul>
<li>Q-learning</li>
<li>SARSA</li>
<li>etc.</li>
</ul></li>
</ul>
<p>There is an enormous amount of current interest in machine learning methods and there is a corresponding amount of high-quality material discussing it. We will end the introduction here and direct you to established textbooks<span class="citation" data-cites="james2013introduction hastie2009elements murphy2012machine">&nbsp;[<a href="#ref-murphy2012machine" role="doc-biblioref">1</a>–<a href="#ref-hastie2009elements" role="doc-biblioref">3</a>]</span>, NJIT classes (Math 478, Math 678, Math 680, CS 675, CS 677), and online resources (too many to even start listing).</p>
<section id="general-procedure" class="level3">
<h3 class="anchored" data-anchor-id="general-procedure">General procedure</h3>
<p>In practice, machine learning algorithms often boil down to an optimization problem. To characterize this in a few steps, consider a problem with data <span class="math inline">\(x\)</span>:</p>
<ol type="1">
<li>Select a model representation <span class="math inline">\(f\)</span> with parameters <span class="math inline">\(p\)</span> for the problem: <span class="math display">\[
y = f(x;p)
\]</span></li>
<li>Determine an appropriate objective function <span class="math inline">\(\mathcal{L}\)</span>: <span class="math display">\[
\mathcal{L}(f(x;p),x)
\]</span></li>
<li>Use an optimization method <span class="math inline">\(\mathcal{O}\)</span> with parameters <span class="math inline">\(d\)</span> to find parameters <span class="math inline">\(p\)</span> that minimize or maximize the objective for the model: <span class="math display">\[
p^* = \mathcal{O}(\mathcal{L},f,x;d)
\]</span></li>
</ol>
<p>In some sense, this is the same procedure used for <em>inverse problems</em> in traditional applied mathematics but with a broader set of models <span class="math inline">\(f\)</span> that may or may not be based on first-principles understanding of the problem.</p>
</section>
<section id="incorporating-data" class="level3">
<h3 class="anchored" data-anchor-id="incorporating-data">Incorporating data</h3>
<p>Depending on the class of problem considered (supervised, unsupervised, or reinforcement), there are a variety of choices for models <span class="math inline">\(f\)</span> and objectives <span class="math inline">\(\mathcal{L}\)</span>. For supervised learning (the most common), the objective is often to predict or generate the output or dependent variable data of some process. For this, data can be separated into three sets:</p>
<ol type="1">
<li><strong>Training data</strong> (<span class="math inline">\(x\)</span>): used to tune the parameters <span class="math inline">\(p\)</span></li>
<li><strong>Validation data</strong> (<span class="math inline">\(x^v\)</span>): used to evaluate the generalization of the model <span class="math inline">\(f\)</span> to data not in the training set during training</li>
<li><strong>Testing data</strong> (<span class="math inline">\(x^t\)</span>): used to benchmark the predictive or generative ability of the model after training is completed</li>
</ol>
</section>
</section>
<section id="a-first-machine-learning-problem" class="level2">
<h2 class="anchored" data-anchor-id="a-first-machine-learning-problem">A first machine learning problem</h2>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code for this problem will require the following packages: <code>numpy</code>, <code>matplotlib</code>, <code>autograd</code></p>
</div>
</div>
<p>These workshops are about learning by doing, so let’s build understanding by fitting a simple “machine” to some data as a supervised problem. Consider some data <span class="math inline">\((x,y)\)</span>: <!-- Consider some data $(x,y)$ generated with the function: --> <!-- $$ --> <!-- y = e^{-x}(\sin(x^3) + \sin(x^2) - x) + \frac{1}{1 + e^{-(x-1)}} --> <!-- $$ --></p>
<div class="cell" data-execution_count="1">
<details>
<summary>Data generation</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># To see animations in a Jupyter notebook, uncomment the following line:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># %matplotlib notebook</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_known(x):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    part1 <span class="op">=</span> np.exp(<span class="op">-</span>x)<span class="op">*</span>(np.sin((x)<span class="op">**</span><span class="dv">3</span>) <span class="op">+</span> np.sin((x)<span class="op">**</span><span class="dv">2</span>) <span class="op">-</span> x)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    part2 <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">1</span><span class="op">*</span>(x<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> part1 <span class="op">+</span> part2</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>xsamples <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">100</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>ysamples <span class="op">=</span> f_known(xsamples)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(xsamples,ysamples)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)<span class="op">;</span> plt.ylabel(<span class="st">"$y$"</span>)<span class="op">;</span> plt.title(<span class="st">"Data"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="workshop1_intro_files/figure-html/cell-2-output-1.png" width="589" height="449"></p>
</div>
</div>
<p>We would like to fit a model of the following form to this data: <span class="math display">\[
f(x;p_0,p_1) = e^{-p_0x}(\sin((p_0x)^3) + \sin((p_0x)^2) - p_0x) + \frac{1}{1 + e^{-p_1(x-1)}}
\]</span> <!-- which looks like the following for $p_0=1,p_1=1$: --></p>
<p>To formulate this as a machine learning/optimization problem, we can consider an objective/loss to minimize the <span class="math inline">\(L^2\)</span> norm distance between the model output <span class="math inline">\(f(x)\)</span> and the true data <span class="math inline">\(y\)</span>: <span class="math display">\[
\mathcal{L(f(x;\vec{p}),y)} = ||f(x) - y||_2^2
\]</span> The problem can then be written as the unconstrained optimization problem: <span class="math display">\[
p^* = \underset{\vec{p}}{\text{minimize }} \mathcal{L}(f(x;\vec{p}),y)
\]</span> We then expect our model <span class="math inline">\(f(x;p^*)\)</span> to represent a “machine” that has accurately “learned” the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>There are several ways to approach this problem, but a simple and popular approach for a continuous and unconstrained problem is to use an iterative gradient method.</p>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h3>
<p><em>Gradient descent</em> is a straightforward method taught early in an undergraduate numerical methods class. Its simplicity and relatively low computational cost has made it popular for machine learning methods (which can contain enough parameters that second-order methods, like Newton’s method, are infeasibly expensive because of the Hessian computation). Beginning with an initial parameter guess <span class="math inline">\(\vec{p}_0\)</span>, its update procedure can be written as: <span class="math display">\[
\begin{align*}
\vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
v^i &amp;= -\alpha \nabla_p \mathcal{L}
\end{align*}
\]</span> where <span class="math inline">\(\alpha\)</span> controls the step size in the direction of the gradient (usually called a “learning rate” in machine learning). This method will follow the gradient of the objective/loss <span class="math inline">\(\mathcal{L}\)</span> until the objective is sufficiently small, or until it reaches a steady state.</p>
<p>Simply implemented in Python, this method can be written as:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(f_p,x0,alpha<span class="op">=</span><span class="fl">.2</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>f_p(x)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>However, this method contains a troublesome parameter <span class="math inline">\(\alpha\)</span> which, if chosen too large, could prevent convergence of the solution or, if chosen too small, could require an unreasonable number of steps to converge. The method itself is also prone to terminate in local minima rather than in a global minimum unless the correct initial guess and learning rate are chosen. For this reason, “vanilla” (or normal) gradient descent is almost always replaced with a modified method in learning problems<span class="citation" data-cites="sebastian_ruder_2020 john_chen_2020">&nbsp;[<a href="#ref-sebastian_ruder_2020" role="doc-biblioref">4</a>,<a href="#ref-john_chen_2020" role="doc-biblioref">5</a>]</span>.</p>
<p>The following demonstrates an animation of the above gradient descent method applied to our data with two different learning rates, one successful, one not. It uses the following animation code and the <code>autograd</code> automatic differentiation library that will be further discussed later:</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Animation code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> animation <span class="im">as</span> anim</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> gridspec</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># To display the animations in a jupyer notebook uncomment the following line:</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># %matplotlib notebook</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> animate_steps_2d(xs,loss,xmin<span class="op">=-</span><span class="fl">.1</span>,xmax<span class="op">=</span><span class="fl">2.5</span>,ymin<span class="op">=-</span><span class="dv">1</span>,ymax<span class="op">=</span><span class="dv">3</span>,interval<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">6</span>),constrained_layout<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    gs <span class="op">=</span> gridspec.GridSpec(ncols<span class="op">=</span><span class="dv">6</span>,nrows<span class="op">=</span><span class="dv">2</span>,figure<span class="op">=</span>fig)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(gs[:,<span class="dv">0</span>:<span class="dv">4</span>],projection<span class="op">=</span><span class="st">"3d"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    ax1 <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>,<span class="dv">4</span>:])</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    ax2 <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>,<span class="dv">4</span>:])</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    ax.view_init(<span class="dv">47</span>,<span class="dv">47</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$p_0$"</span>)<span class="op">;</span> ax.set_ylabel(<span class="st">"$p_2$"</span>)<span class="op">;</span> ax.set_zlabel(<span class="st">"loss"</span>,rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"$p_0$"</span>)<span class="op">;</span> ax1.set_ylabel(<span class="st">"loss"</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"$p_1$"</span>)<span class="op">;</span> ax2.set_ylabel(<span class="st">"loss"</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    xs_arr <span class="op">=</span> np.array(xs)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    fxs <span class="op">=</span> np.linspace(xmin,xmax,<span class="dv">100</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    fys <span class="op">=</span> np.linspace(ymin,ymax,<span class="dv">100</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    loss_fx <span class="op">=</span> [loss([fxs[j],xs[<span class="dv">0</span>][<span class="dv">1</span>]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fxs))]</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    loss_fy <span class="op">=</span> [loss([xs[<span class="dv">0</span>][<span class="dv">0</span>],fys[j]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fys))]</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    X,Y <span class="op">=</span> np.meshgrid(fxs,fys)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.zeros_like(X)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>            Z[i,j] <span class="op">=</span> loss([X[i,j],Y[i,j]])</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add surface plot</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    surf <span class="op">=</span> ax.plot_surface(X,Y,Z,cmap<span class="op">=</span><span class="st">"gist_earth"</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlim(np.<span class="bu">min</span>(X),np.<span class="bu">max</span>(X))<span class="op">;</span> ax1.set_ylim(np.<span class="bu">min</span>(Z),np.<span class="bu">max</span>(Z))</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(np.<span class="bu">min</span>(Y),np.<span class="bu">max</span>(Y))<span class="op">;</span> ax2.set_ylim(np.<span class="bu">min</span>(Z),np.<span class="bu">max</span>(Z))</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    plot1 <span class="op">=</span> ax.plot(xs[<span class="dv">0</span>][<span class="dv">0</span>],xs[<span class="dv">0</span>][<span class="dv">1</span>],loss(xs[<span class="dv">0</span>]),zorder<span class="op">=</span><span class="dv">100</span>,color<span class="op">=</span><span class="st">"red"</span>,linestyle<span class="op">=</span><span class="st">""</span>,marker<span class="op">=</span><span class="st">"o"</span>)[<span class="dv">0</span>]</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    plot2 <span class="op">=</span> ax.plot([],[],[],color<span class="op">=</span><span class="st">"orange"</span>)[<span class="dv">0</span>]</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add flat plots for perspective</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    plot3 <span class="op">=</span> ax1.plot(fxs,loss_fx)[<span class="dv">0</span>]</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    plot4 <span class="op">=</span> ax1.scatter(xs[<span class="dv">0</span>][<span class="dv">0</span>],loss(xs[<span class="dv">0</span>]),color<span class="op">=</span><span class="st">"red"</span>,s<span class="op">=</span><span class="dv">100</span>,zorder<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    plot5 <span class="op">=</span> ax2.plot(fys,loss_fy)[<span class="dv">0</span>]</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    plot6 <span class="op">=</span> ax2.scatter(xs[<span class="dv">0</span>][<span class="dv">1</span>],loss(xs[<span class="dv">0</span>]),color<span class="op">=</span><span class="st">"red"</span>,s<span class="op">=</span><span class="dv">100</span>,zorder<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> anim_func(i):</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        x_loss <span class="op">=</span> loss(xs[i])</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        plot1.set_data_3d(xs[i][<span class="dv">0</span>],xs[i][<span class="dv">1</span>],x_loss)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>        temp_x1 <span class="op">=</span> [xs[j][<span class="dv">0</span>] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i)]</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        temp_x2 <span class="op">=</span> [xs[j][<span class="dv">1</span>] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i)]</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        temp_losses <span class="op">=</span> [loss(xs[j]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i)]</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        plot2.set_data_3d(temp_x1,temp_x2,temp_losses)</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        loss_fx <span class="op">=</span> [loss([fxs[j],xs[i][<span class="dv">1</span>]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fxs))]</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>        loss_fy <span class="op">=</span> [loss([xs[i][<span class="dv">0</span>],fys[j]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(fys))]</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        plot3.set_data(fxs,loss_fx)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        plot4.set_offsets([xs[i][<span class="dv">0</span>],x_loss])</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        plot5.set_data(fys,loss_fy)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        plot6.set_offsets([xs[i][<span class="dv">1</span>],x_loss])</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        plots <span class="op">=</span> [plot1,plot2,plot3,plot4,plot5,plot6]</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> plots</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>    tanim <span class="op">=</span> anim.FuncAnimation(fig,anim_func,interval<span class="op">=</span><span class="dv">50</span>,frames<span class="op">=</span><span class="bu">len</span>(xs),blit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tanim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> anp</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_model(p):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    part1 <span class="op">=</span> anp.exp(<span class="op">-</span>p[<span class="dv">0</span>]<span class="op">*</span>xsamples)<span class="op">*</span>(anp.sin((p[<span class="dv">0</span>]<span class="op">*</span>xsamples)<span class="op">**</span><span class="dv">3</span>) <span class="op">+</span> anp.sin((p[<span class="dv">0</span>]<span class="op">*</span>xsamples)<span class="op">**</span><span class="dv">2</span>) <span class="op">-</span> p[<span class="dv">0</span>]<span class="op">*</span>xsamples) </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    part2 <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> anp.exp(<span class="op">-</span>p[<span class="dv">1</span>]<span class="op">*</span>(xsamples<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> part1 <span class="op">+</span> part2</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="kw">lambda</span> p: anp.<span class="bu">sum</span>((f_model(p) <span class="op">-</span> ysamples)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>grad_loss <span class="op">=</span> grad(loss) <span class="co"># automatically differentiated</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> gradient_descent(grad_loss,p0,<span class="fl">.005</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/gradient_descent1.gif" class="img-fluid"></p>
<p>Although this behavior is somewhat typical of vanilla gradient descent, this model was pathologically chosen to be challenging. The curvature of the loss function for each parameter at the correct parameter values are as follows:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Plotting parameter loss curves</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">.1</span>,<span class="fl">2.5</span>,<span class="dv">1000</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>fig,axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,sharey<span class="op">=</span><span class="va">True</span>,figsize<span class="op">=</span>(<span class="dv">9</span>,<span class="dv">4</span>))</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>ax1,ax2 <span class="op">=</span> axs</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>ax1.plot(ps,[loss(np.array([p_i,<span class="dv">1</span>])) <span class="cf">for</span> p_i <span class="kw">in</span> ps])</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">"$p_0$"</span>)<span class="op">;</span> ax1.set_ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>ax2.plot(ps,[loss(np.array([<span class="dv">1</span>,p_i])) <span class="cf">for</span> p_i <span class="kw">in</span> ps])</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">"$p_1$"</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.xlabel("$x$"); plt.ylabel("$y$"); plt.title("Model")</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="workshop1_intro_files/figure-html/cell-7-output-1.png" width="733" height="356"></p>
</div>
</div>
<p>In this loss, <span class="math inline">\(p_0\)</span> demonstrates a plethora of local minima which could trap the descent algorithm while <span class="math inline">\(p_1\)</span> has a small gradient which will slow down the convergence. The following methods are meant to simplify the choice of a learning rate while overcoming these specific convergence issues.</p>
</section>
<section id="adaptive-steps" class="level3">
<h3 class="anchored" data-anchor-id="adaptive-steps">Adaptive steps</h3>
<p>Due to the challenges of determining a good learning rate (especially in models with many parameters and large variance in loss gradients), many methods have been developed to automatically adjust the <span class="math inline">\(\alpha\)</span> parameter with each step and for each parameter. One of the most common adaptive algorithms is called <code>adagrad</code> (for adaptive gradient. very creative). Originally developed to provide parameter specific learning rates in sparse problems (in the case that some parameters are only occasionally important) it scales the learning rate by a squared sum of previous gradients:</p>
<p><strong>adagrad:</strong> <span class="math display">\[
\begin{align*}
\vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
v^i &amp;= -\frac{\alpha}{\sqrt{G^i}} \nabla_p \mathcal{L}(x;\vec{p}^i) \\
G^i &amp;= \sum_{j=0}^i (\nabla_p \mathcal{L}(x;\vec{p}^j))^2
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="7">
<details>
<summary>adagrad code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adagrad(f_p,x0,alpha<span class="op">=</span><span class="fl">.2</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------- NEW -----------</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    sum_sq_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        sum_sq_grad <span class="op">=</span> f_p(x)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> sum_sq_grad</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>f_p(x)<span class="op">/</span>np.sqrt(sum_sq_grad)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> adagrad(grad_loss, p0,<span class="fl">.2</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/adagrad.gif" class="img-fluid"> However, depending on the problem, this scaled learning rate may slow down convergence considerably. As an adjustment to remove this monotone decreasing learning rate, <code>RMSprop</code> attempts to balance the current gradient with a dampened version of the sum of squares of previous gradients from <code>adagrad</code>:</p>
<p><strong>RMSprop:</strong> <span class="math display">\[
\begin{align*}
\vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
v^i &amp;= -\frac{\alpha}{\sqrt{G^i}} \nabla_p \mathcal{L}(x;\vec{p}^i) \\
G^i &amp;= \gamma G^{i-1} + (1-\gamma)(\nabla_p \mathcal{L}(x;\vec{p}^i))^2
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="9">
<details>
<summary>RMSprop code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rmsprop(f_p,x0,gamma<span class="op">=</span><span class="fl">0.9</span>,alpha<span class="op">=</span><span class="fl">0.001</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------- NEW -----------</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    sum_sq_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        sum_sq_grad <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>gamma)<span class="op">*</span>(f_p(x)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> gamma<span class="op">*</span>sum_sq_grad</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>f_p(x)<span class="op">/</span>np.sqrt(sum_sq_grad)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> rmsprop(grad_loss, p0,<span class="fl">0.2</span>,<span class="fl">.05</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/rmsprop.gif" class="img-fluid"> This helps with the challenge of a monotone decreasing learning rate, but it introduces a dampening parameter <span class="math inline">\(\gamma\)</span> that must be chosen (recommended values are <span class="math inline">\(\alpha = 0.001\)</span> and <span class="math inline">\(\gamma = 0.9\)</span>).</p>
</section>
<section id="with-momentum" class="level3">
<h3 class="anchored" data-anchor-id="with-momentum">With momentum</h3>
<p>Though the previous adaptive methods address the challenge of determining a learning rate, these methods are still likely to terminate in local minima. To address this issue, there are several methods which utilize a concept of “momentum” to propel iterations out of local minima with the hope of landing in the global minimum. This momentum is most simply added by incorporating previous gradients into the current update:</p>
<p><strong>Gradient descent with momentum:</strong> <span class="math display">\[
\begin{align*}
    \vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
    v^i &amp;= -\alpha G^i \\
    G^i &amp;= \nabla_p \mathcal{L}(x;\vec{p}^i) + \gamma G^{i-1}
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="11">
<details>
<summary>Gradient descent with momentum code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_momentum(f_p,x0,gamma,alpha<span class="op">=</span><span class="fl">0.01</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------- NEW -----------</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    sum_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        sum_grad <span class="op">=</span> f_p(x) <span class="op">+</span> gamma<span class="op">*</span>sum_grad</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>sum_grad</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> gradient_descent_momentum(grad_loss, p0,<span class="fl">.9</span>,<span class="fl">.005</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/momentum.gif" class="img-fluid"> where <span class="math inline">\(\gamma\)</span> is a momentum parameter that determines how much of previous updates are kept for the current step (<span class="math inline">\(0 &lt;= \gamma &lt;=1\)</span> where <span class="math inline">\(\gamma = 0\)</span> includes no momentum).</p>
<p>However, iterations that include this momentum may jump right out of global minima and/or delay convergence. To incorporate a counterbalance to the momentum based on current success (to slow down in the right places), <code>Nesterov acceleration</code> adjusts the gradient according to an approximated step (to see how successful it may be in the future). By so doing, it can effectively reduce or increase the momentum according to the next future iteration:</p>
<p><strong>Nesterov accelerated gradient descent:</strong> <span class="math display">\[
\begin{align*}
    \vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
    v^i &amp;= -\alpha G^i \\
    G^i &amp;= \nabla_p \mathcal{L}(x;\vec{p}^i - \gamma v^{i-1}) + \gamma G^{i-1}
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="13">
<details>
<summary>Gradient descent with Nesterov acceleration code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_nesterov(f_p,x0,gamma,alpha<span class="op">=</span><span class="fl">0.01</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------- NEW -----------</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    sum_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        sum_grad <span class="op">=</span> f_p(x<span class="op">-</span>gamma<span class="op">*</span>v_i) <span class="op">+</span> gamma<span class="op">*</span>sum_grad</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>sum_grad</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> gradient_descent_nesterov(grad_loss, p0,<span class="fl">.8</span>,<span class="fl">.002</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/nesterov.gif" class="img-fluid"> where again <span class="math inline">\(\gamma\)</span> is a momentum parameter. Notice that the only adjustment compared to vanilla gradient descent with momentum is calculating the gradient at an approximation of the next parameter values rather than at the current parameters.</p>
</section>
<section id="combining-ideas" class="level3">
<h3 class="anchored" data-anchor-id="combining-ideas">Combining ideas</h3>
<p>Of course, the ideas of adaptive gradients and momentum both address important issues and are not mutually exclusive, so they can both be used simultaneously (with the downside of adding more parameters to tune). Note that the adaptive step size methods (<code>adagrad</code>,<code>RMSprop</code>) use a sum of squares of the previous gradient while the momentum methods (gradient descent with momentum or Nesterov acceleration) use a sum of the previous gradient. These can be seen as the second moment and first moment of the previous gradients respectively. The <code>Adam</code> (adaptive moment estimation) method uses both of these gradient moments to incorporate both momentum and adaptivity:</p>
<p><strong>adam:</strong> <span class="math display">\[
\begin{align*}
    \vec{p}^{i+1} &amp;= \vec{p}^i + v^i \\
    v^i &amp;= -\frac{\alpha}{\sqrt{b^i}}m^i \\
    m^i &amp;= \beta_1m^{i-1} + (1-\beta_1)\nabla_p \mathcal{L}(x;\vec{p}^i) \\
    b^i &amp;= \beta_2b^{i-1} + (1-\beta_2)(\nabla_p \mathcal{L}(x;\vec{p}^i))^2
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="15">
<details>
<summary>Adam code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(f_p,x0,beta1,beta2,alpha<span class="op">=</span><span class="fl">0.01</span>,tol<span class="op">=</span><span class="fl">1e-12</span>,steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> [x]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------- NEW -----------</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    sum_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    sum_sq_grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,steps):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        sum_grad <span class="op">=</span> beta1<span class="op">*</span>sum_grad <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta1)<span class="op">*</span>f_p(x)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        sum_sq_grad <span class="op">=</span> beta2<span class="op">*</span>sum_sq_grad <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta2)<span class="op">*</span>(f_p(x)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        v_i <span class="op">=</span> <span class="op">-</span>alpha<span class="op">*</span>sum_grad<span class="op">/</span>np.sqrt(sum_sq_grad)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        xnew <span class="op">=</span> x <span class="op">+</span> v_i</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(f_p(xnew)) <span class="op">&lt;</span> tol:</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to objective loss gradient below </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.linalg.norm(xnew <span class="op">-</span> x) <span class="op">&lt;</span> tol:</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Converged to steady state of tolerance </span><span class="sc">{}</span><span class="st"> in </span><span class="sc">{}</span><span class="st"> steps."</span>.<span class="bu">format</span>(tol,s))</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x,xs</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnew</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        xs.append(x)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Did not converge after </span><span class="sc">{}</span><span class="st"> steps (tolerance </span><span class="sc">{}</span><span class="st">)."</span>.<span class="bu">format</span>(steps,tol))</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>p0 <span class="op">=</span> np.array([<span class="fl">2.1</span>,<span class="fl">.2</span>])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> adam(grad_loss, p0,<span class="fl">0.9</span>,<span class="fl">0.99</span>,<span class="fl">.05</span>,tol<span class="op">=</span><span class="fl">1e-8</span>,steps<span class="op">=</span><span class="dv">100</span>)[<span class="dv">1</span>]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>animate_steps_2d(xs,loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="videos/adam.gif" class="img-fluid"> Parameters <span class="math inline">\(\beta_1,\beta_2\)</span> represent decay rates in incorporating previous moments into the update step.</p>
<p>Although this list is not comprehensive, it demonstrates the reasoning behind the common solutions for the challenges of using gradient descent in learning methods. For a more comprehensive list of recently proposed methods, see<span class="citation" data-cites="john_chen_2020">&nbsp;[<a href="#ref-john_chen_2020" role="doc-biblioref">5</a>]</span>.</p>
</section>
<section id="automatic-differentiation" class="level3">
<h3 class="anchored" data-anchor-id="automatic-differentiation">Automatic differentiation</h3>
<p>Gradient descent methods rely on computing the gradient of the loss <span class="math inline">\(\nabla_p \mathcal{L}\)</span> for a given parameter set <span class="math inline">\(\vec{p}^i\)</span>. For simple models, the gradients can be calculated by hand. However, for models with many nested functions and parameters (neural networks in particular) or methods whose form depends on hyperparameters, we will need an automated method. The “automatic differentiation” method is the solution to these challenges. Though it was developed for neural networks and mainly used there, its breadth of applications is only just becoming apparent. (Consider the Julia language which has worked hard to make automatic differentiation possible everywhere).</p>
<p>There are both “forward” and “backward” automatic differentiation methods, but we will consider only the forward which best demonstrates the “automatic” moniker. To do so, consider the first order expansion of two functions at a given point <span class="math inline">\(a\)</span>: <span class="math display">\[
\begin{align*}
f(a + \epsilon) &amp;= f(a) + \epsilon f'(a) \\
g(a + \epsilon) &amp;= g(a) + \epsilon g'(a)
\end{align*}
\]</span> where <span class="math inline">\(\epsilon\)</span> is a small perturbation. Basic operations with these functions at this point can then be written as: <span class="math display">\[
\begin{align*}
f + g &amp;= [f(a) + g(a)] + \epsilon[f'(a) + g'(a)] \\
f - g &amp;= [f(a) - g(a)] + \epsilon[f'(a) - g'(a)] \\
f \cdot g &amp;= [f(a) \cdot g(a)] + \epsilon[f(a)\cdot g'(a) + g(a)\cdot f'(a)] \\
f \div g &amp;= [f(a) \div g(a)] + \epsilon\left[\frac{f'(a)\cdot g(a) + f(a)\cdot g'(a)}{g(a)^2}\right]
\end{align*}
\]</span> The derivatives are then represented by the <span class="math inline">\(\epsilon\)</span> terms in the above equalities.</p>
<p>To implement this on a computer, you can create a new “Dual” number that performs additions, subtractions, multiplications, and divisions with the above operating rules (and can be extended for other functions: <span class="math inline">\(\sin,\cos,\exp\)</span>,etc).</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is cumbersome and slow to implement in Python which is why large and complicated libraries have been written in C (<code>tensorflow</code>,<code>pytorch</code>,<code>jax</code>, and others).</p>
<p>It can be cleanly and easily implemented in Julia. To see a demonstration of this, see<span class="citation" data-cites="chris_rackauckas_2020">&nbsp;[<a href="#ref-chris_rackauckas_2020" role="doc-biblioref">6</a>]</span>.</p>
</div>
</div>
<p>A quick demonstration of a simple implementation of this is as follows<span class="citation" data-cites="mostafa_samir_2020">&nbsp;[<a href="#ref-mostafa_samir_2020" role="doc-biblioref">7</a>]</span>. Define a dual number class:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DualNumber:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, val, der):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val <span class="op">=</span> val</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.der <span class="op">=</span> der</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># DualNumber + DualNumber</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.val <span class="op">+</span> other.val, <span class="va">self</span>.der <span class="op">+</span> other.der)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># DualNumber + a number</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.val <span class="op">+</span> other, <span class="va">self</span>.der)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__radd__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a number + DualNumber</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__add__</span>(other)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>,other):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># DualNumber * DualNumber</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.val <span class="op">*</span> other.val, <span class="va">self</span>.val<span class="op">*</span>other.der <span class="op">+</span> <span class="va">self</span>.der<span class="op">*</span>other.val)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># DualNumber * a number</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.val <span class="op">*</span> other, <span class="va">self</span>.der <span class="op">*</span> other)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>,other):</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a number * DualNumber</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__mul__</span>(other)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__pow__</span>(<span class="va">self</span>,power):</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># DualNumber ^ a number</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.val <span class="op">**</span> power, <span class="va">self</span>.der <span class="op">*</span> power <span class="op">*</span> (<span class="va">self</span>.val <span class="op">**</span> (power <span class="op">-</span> <span class="dv">1</span>)))</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Printing a DualNumber</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"DualNumber(</span><span class="sc">{}</span><span class="st">,</span><span class="sc">{}</span><span class="st">)"</span>.<span class="bu">format</span>(<span class="va">self</span>.val,<span class="va">self</span>.der)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>dual1 <span class="op">=</span> DualNumber(<span class="dv">1</span>,<span class="dv">2</span>)<span class="op">;</span> dual2 <span class="op">=</span> DualNumber(<span class="dv">3</span>,<span class="dv">4</span>)<span class="op">;</span> other <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dual1,<span class="st">"+"</span>,dual2,<span class="st">"="</span>,dual1<span class="op">+</span>dual2)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dual1,<span class="st">"+"</span>,other,<span class="st">"="</span>,dual1<span class="op">+</span>other)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dual1,<span class="st">"*"</span>,dual2,<span class="st">"="</span>,dual1<span class="op">*</span>dual2)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dual1,<span class="st">"*"</span>,other,<span class="st">"="</span>,dual1<span class="op">*</span>other)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dual1,<span class="st">"^"</span>,<span class="dv">2</span>,<span class="st">"="</span>,dual1<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>DualNumber(1,2) + DualNumber(3,4) = DualNumber(4,6)
DualNumber(1,2) + 5 = DualNumber(6,2)
DualNumber(1,2) * DualNumber(3,4) = DualNumber(3,10)
DualNumber(1,2) * 5 = DualNumber(5,10)
DualNumber(1,2) ^ 2 = DualNumber(1,4)</code></pre>
</div>
</div>
<p>This seems simple, but by using a few rules like this, we can compute simple polynomial derivatives at a point automatically. For example, computing the derivative of <span class="math inline">\(f(x) = 3x^3 + 2x + 4\)</span> at <span class="math inline">\(x=2\)</span>. We first initialize the function and the dual number <span class="math inline">\((2,1)\)</span> (<span class="math inline">\(1\)</span> because the derivative of <span class="math inline">\(x\)</span> is <span class="math inline">\(1\)</span>).</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">9</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> dualx <span class="op">=</span> DualNumber(x,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then pass the dual number through the polynomial function, unpack the results, and compare the result with the true derivative <span class="math inline">\(f'(2)\)</span>:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>poly_dual <span class="op">=</span> f(dualx)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>dual_val <span class="op">=</span> poly_dual.val<span class="op">;</span> dual_der <span class="op">=</span> poly_dual.der</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>true_der <span class="op">=</span> df(x)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"True derivative at x=2: "</span>, true_der)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dual number derivative: "</span>, dual_der)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True derivative at x=2:  38
Dual number derivative:  38</code></pre>
</div>
</div>
<p>Using the rules for dual numbers we provided above, the derivative “automatically” popped out of the polynomial evaluation.</p>
<p>In contrast with the forward method, the backward method keeps a log of the operations performed and then uses defined chain rules (similar to how we defined rules for the forward pass) to backtrack from the final output back to the start. This approach is more efficient when the dimension of the gradient is large and that of the output small. Forward evaluation is more efficient when the dimension of the gradient is small and that of the output large.</p>
<p>All in all, automatic differentiation is programming with hard-coded differential rules for primitive operations. It can also require keeping track of which operations happen and in what order. Implementing such a method relies heavily on computer science techniques since it boils down to parsing operation calls. This represents one of the clearest differences in the approaches of machine learning to those of statistics or optimization due to its computer language-heavy principles. Through this lens, machine learning could be viewed as “inferential statistics using new tools from computer science for non-traditional problems.”</p>
</section>
</section>
<section id="further-exploration" class="level2">
<h2 class="anchored" data-anchor-id="further-exploration">Further Exploration</h2>
<ol type="1">
<li>Each of the parameters for the gradient descent methods was chosen by hand for the above animations. Try adjusting the starting point and parameters for each to get a feel for how they behave.</li>
<li>Try splitting the sample data into a training and a testing set. Use the training set and one of the gradient descent methods to fit some parameters, then see how well the model generalizes to the testing set with those parameters.</li>
<li>Try implementing another gradient descent method from<span class="citation" data-cites="sebastian_ruder_2020">&nbsp;[<a href="#ref-sebastian_ruder_2020" role="doc-biblioref">4</a>]</span> using the given methods as a template.</li>
<li>Add the missing subtraction (<code>__sub__</code>, <code>__rsub__</code>) and division (<code>__truediv__</code>, <code>__rtruediv__</code>) methods in the <code>DualNumber</code> class and play around with automatically differentiation functions using those operators (you can use <a href="https://github.com/Mostafa-Samir/Hands-on-Intro-to-Auto-Diff/blob/master/dualnumbers/dualnumbers.py">this</a> as a reference). You can also make functions such as <code>sin</code>,<code>cos</code>, or <code>exp</code> that are meant to compute with <code>DualNumber</code>s.</li>
</ol>
</section>


<div id="quarto-appendix" class="default"><section id="appendix" class="level2 appendix"><h2 class="quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">



</div></section><section id="python-resources-and-packages" class="level3 appendix"><h2 class="quarto-appendix-heading">Python resources and packages</h2><div class="quarto-appendix-contents">

<p><strong>Python help</strong></p>
<ul>
<li><a href="https://docs.python.org/3.9/tutorial/introduction.html">Python documentation (especially sections 3-9)</a></li>
<li><a href="https://www.pythoncheatsheet.org">Quick cheatsheet of general Python knowledge</a></li>
<li><a href="https://learnxinyminutes.com/docs/python/">Quicker introduction</a></li>
</ul>
<p><strong>Conda/mamba help</strong></p>
<ul>
<li><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/index.html">Conda user guide</a></li>
<li><a href="https://mamba.readthedocs.io/en/latest/index.html">Mamba website</a></li>
</ul>
<p><strong>Python essential packages</strong></p>
<ul>
<li><a href="https://numpy.org"><code>numpy</code></a> - Creating, manipulating, and operating (linear algebra, fft, etc.) on multi-dimensional arrays. A list of packages built on <code>numpy</code> for a variety of domains can be found on the homepage under the <em>ECOSYSTEM</em> heading.</li>
<li><a href="https://scipy.org"><code>scipy</code></a> - Fundamentals in optimization, integration, interpolation, differential equations, statistics, signal processing, etc.</li>
<li><a href="https://matplotlib.org"><code>matplotlib</code></a> - Static or interactive plots and animations</li>
<li><a href="https://scikit-learn.org/stable/index.html"><code>scikit-learn</code></a> - Standard machine learning tools and algorithms built on <code>numpy</code>, <code>scipy</code>, and <code>matplotlib</code></li>
<li><a href="https://pandas.pydata.org"><code>pandas</code></a> - Easily represent, manipulate, and visualize structured datasets (matrices with names for columns and rows)</li>
<li><a href="https://keras.io"><code>keras</code></a> - High level neural network framework built on <code>tensorflow</code></li>
<li><a href="https://www.tensorflow.org"><code>tensorflow</code></a> - In depth neural network framework focused on ease and production</li>
<li><a href="https://pytorch.org"><code>pytorch</code></a> - In depth neural network framework focused on facilitating the path from research to production</li>
<li><a href="https://scikit-image.org"><code>scikit-image</code></a> - Image processing algorithms and tools</li>
<li><a href="https://jupyter.org"><code>jupyter</code></a> - Interactive “notebook” style programming</li>
</ul>
</div></section><section id="julia-as-an-alternative-to-python" class="level3 appendix"><h2 class="quarto-appendix-heading">Julia as an alternative to Python</h2><div class="quarto-appendix-contents">

<p>Julia is a fairly new language that has been mainly proposed as an alternative to Python and Matlab, though it is general use. Its strength and its weakness is that it is “just-in-time” compiled (meaning your code is automatically analyzed and compiled just before it is run). A clever language design combined with just-in-time compilation makes Julia as clear to read and write as Python while being much faster. It can even approach the speed of C when written carefully. However, the just-in-time compilation and type system remove a chunk of the interactive convenience of Python and its young age also means that it does not have the volume of packages that Python does.</p>
<p>Nonetheless, it is an elegant and high-performance language to use and has shown rapid growth recently. Concise, simple, and easy to read and contribute to packages have been quickly emerging and it already provides many useful tools. As a result, it is worth describing it’s installation process, environment management, and noteable packages.</p>




</div></section><section id="installation" class="level4 appendix"><h2 class="quarto-appendix-heading">Installation</h2><div class="quarto-appendix-contents">

<p>The officially supported method of installation for Julia is now using the <code>juliaup</code> version manager. The <a href="https://github.com/JuliaLang/juliaup#windows">installer</a> can be downloaded from the Windows store on Windows or run on MacOS or Linux with:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://install.julialang.org <span class="kw">|</span> <span class="fu">sh</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></section><section id="environments-1" class="level4 appendix"><h2 class="quarto-appendix-heading">Environments</h2><div class="quarto-appendix-contents">

<p>Julia comes with a standard environment and package manager named <a href="https://pkgdocs.julialang.org/v1/"><code>Pkg</code></a>. Interestingly, the easiest way to use it is to run the Julia REPL (read-eval-print-loop), i.e.&nbsp;to run <code>julia</code> interactively. You can do so by typing <code>julia</code> into the terminal. You will then be presented with a terminal interface such as:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>   _       _ _(_)_     |  Documentation: https://docs.julialang.org</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  (_)     | (_) (_)    |</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>   _ _   _| |_  __ _   |  Type "?" for help, "]?" for Pkg help.</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  | | | | | | |/ _` |  |</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  | | |_| | | | (_| |  |  Version 1.8.0 (2022-08-17)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a> _/ |\__'_|_|_|\__'_|  |  Official https://julialang.org/ release</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>|__/                   |</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>julia&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Typing <code>]</code> will put you into “<code>Pkg</code> mode”:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>(@v1.8) pkg&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Type <code>?</code> and hit enter to get options in this mode. We can create and activate a new environment called workshop with the command:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>(@v1.8) pkg&gt; activate --shared workshop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that the <code>--shared</code> flag will make a “global” environment that can be accessed from any directory. If we were to leave out this flag, <code>Pkg</code> would put a <code>Project.toml</code> and <code>Manifest.toml</code> file in the current directory that contain the name of the environment, its installed packages, and their dependencies. This can be useful to easily isolate and share environments. After running this command, our <code>Pkg</code> mode will have changed to represent the active environment:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>(@workshop) pkg&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></section><section id="installing-packages-1" class="level4 appendix"><h2 class="quarto-appendix-heading">Installing packages</h2><div class="quarto-appendix-contents">

<p>To install some packages in the active environment, write:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>(@workshop) pkg&gt; add Plots MLJ DataFrames Flux Pluto</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These packages will install and precompile. To test one of them, press backspace to leave <code>Pkg</code> mode and input:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>julia&gt; using Plots</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>[ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>julia&gt; x = range(0,10,100);</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>julia&gt; y = sin.(x);</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>julia&gt; plot(x,y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This should show a plot of a simple <span class="math inline">\(\sin\)</span> curve. Note that the precompilation of <code>Plots</code> took some time. However, this will not need to occur again until the package is updated. Also note that the call to <code>plot(x,y)</code> took some time. This is due to the just-in-time compilation. Now that the compilation has been done for inputs of the types of <code>x</code> and <code>y</code>, if you run <code>plot(x,y)</code> again, it should be almost instantaneous.</p>
</div></section><section id="cleaning-up-1" class="level4 appendix"><h2 class="quarto-appendix-heading">Cleaning up</h2><div class="quarto-appendix-contents">

<p>To deactivate the environment, enter the <code>Pkg</code> mode again by pressing <code>]</code> on an empty line, then enter:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>(@workshop) pkg&gt; activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To delete the environment we created you can delete the environment folder at the listed location at creation. This is usually <code>/home/user/.julia/environments/workshop</code> on MacOS or Linux.</p>
<p><strong>Julia help</strong></p>
<ul>
<li><a href="https://docs.julialang.org/en/v1/">Julia documentation</a></li>
<li><a href="https://juliadocs.github.io/Julia-Cheat-Sheet/">Quick cheatsheet of Julia</a></li>
<li><a href="https://cheatsheets.quantecon.org">Comparison of the syntax of Julia, Python, and Matlab</a></li>
</ul>
<p><strong>Julia packages</strong></p>
<p>As compared to Python, Julia has many scientific computing tools built into its standard library. Thus, a lot of the functionality found in <code>numpy</code> are loaded by default. On the other hand, because of the interoperability of the language and the reduced need for a polyglot codebase (i.e.&nbsp;needing C and Fortran code for a Python package to be fast), packages are usually much smaller modules in Julia. For example, the functionality of the <code>scipy</code> package in Python can be found spread across possibly a dozen different packages in Julia. This is convenient to only load and use what you need, but inconvenient in that it may require more searching to find and the interfaces may not be standardized. The following are some packages that roughly recreate the essential Python packages <a href="#python-resources-and-packages">here</a>.</p>
<ul>
<li><code>numpy</code> - <a href="https://docs.julialang.org/en/v1/manual/arrays/">Standard library</a>,<a href="https://juliamath.github.io/FFTW.jl/latest/"><code>FFTW.jl</code></a></li>
<li><code>scipy</code> - <a href=""><code>Statistics.jl</code></a></li>
<li><code>matplotlib</code> - <a href="https://docs.juliaplots.org/latest/"><code>Plots.jl</code></a></li>
<li><code>scikit-learn</code> - <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/"><code>MLJ.jl</code></a></li>
<li><code>pandas</code> - <a href="https://dataframes.juliadata.org/stable/"><code>DataFrames.jl</code></a></li>
<li><code>keras</code>,<code>tensorflow</code>,<code>pytorch</code> - <a href="https://fluxml.ai/Flux.jl/stable/"><code>Flux.jl</code></a></li>
<li><code>scikit-image</code> - <a href="https://juliaimages.org/latest/install/"><code>Images.jl</code></a></li>
<li><code>jupyter</code> - <a href="https://github.com/fonsp/Pluto.jl"><code>Pluto.jl</code></a> although you can use Julia with Jupyter via <a href="https://julialang.github.io/IJulia.jl/stable/"><code>IJulia.jl</code></a></li>
</ul>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-murphy2012machine" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">K. P. Murphy, <em>Machine Learning: A Probabilistic Perspective</em> (MIT press, 2012).</div>
</div>
<div id="ref-james2013introduction" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">G. James, D. Witten, T. Hastie, and R. Tibshirani, <em>An Introduction to Statistical Learning</em>, Vol. 112 (Springer, 2013).</div>
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>, Vol. 2 (Springer, 2009).</div>
</div>
<div id="ref-sebastian_ruder_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Ruder, <em><a href="https://ruder.io/optimizing-gradient-descent/index.html#momentum">An Overview of Gradient Descent Optimization Algorithms</a></em>, Sebastian Ruder (2020).</div>
</div>
<div id="ref-john_chen_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">J. Chen, <em><a href="https://johnchenresearch.github.io/demon/">An Updated Overview of Recent Gradient Descent Algorithms</a></em>, John Chen (2020).</div>
</div>
<div id="ref-chris_rackauckas_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">C. Rackauckas, <em><a href="https://book.sciml.ai/notes/08/">Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras</a></em>, Chris Rackauckas (2020).</div>
</div>
<div id="ref-mostafa_samir_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">M. Samir, <em><a href="https://mostafa-samir.github.io/auto-diff-pt1/">A Hands-on Introduction to Automatic Differentiation</a></em>, Mostafa Samir (2020).</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/connor-robertson-773ba8b1/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cnrrobertson">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-right">Connor Robertson, Copyright 2022</div>
  </div>
</footer>



</body></html>