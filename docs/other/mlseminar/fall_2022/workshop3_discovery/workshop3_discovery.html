<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Connor Robertson">

<title>Connor Robertson - Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../assets/circle-svgrepo-com.svg" rel="icon" type="image/svg+xml">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Connor Robertson</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../cv.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../teaching/teaching.html">
 <span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../research/research.html">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Other</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-other">    
        <li>
    <a class="dropdown-item" href="../../../../other/fun/fun.html">
 <span class="dropdown-text">Just for Fun</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../other/mlseminar/mlseminar.html">
 <span class="dropdown-text">Machine Learning and Optimization Seminar</span></a>
  </li>  
    </ul>
  </li>
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../other/mlseminar/mlseminar.html" class="sidebar-item-text sidebar-link">Machine Learning and Optimization Seminar</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html" class="sidebar-item-text sidebar-link">Workshop 1: Python set up, basics, gradient descent, automatic differentiation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop2_nn/workshop2_nn.pdf" class="sidebar-item-text sidebar-link">Workshop 2: Neural networks - structure, building, and training</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop3_discovery/workshop3_discovery.html" class="sidebar-item-text sidebar-link active">Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop4_cnn/workshop4_cnn.pdf" class="sidebar-item-text sidebar-link">Workshop 4: Convolutional neural networks</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop5_rnn/workshop5_rnn.pdf" class="sidebar-item-text sidebar-link">Workshop 5: Recurrent neural networks</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../other/mlseminar/fall_2022/workshop6_reinforcement/workshop6_reinforcement.pdf" class="sidebar-item-text sidebar-link">Workshop 6: Reinforcement learning</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#sparse-identification-of-nonlinear-dynamics" id="toc-sparse-identification-of-nonlinear-dynamics" class="nav-link" data-scroll-target="#sparse-identification-of-nonlinear-dynamics">Sparse Identification of Nonlinear Dynamics</a>
  <ul class="collapse">
  <li><a href="#setting-up-linear-problem" id="toc-setting-up-linear-problem" class="nav-link" data-scroll-target="#setting-up-linear-problem">Setting up linear problem</a></li>
  <li><a href="#determining-nonlinear-library-of-terms" id="toc-determining-nonlinear-library-of-terms" class="nav-link" data-scroll-target="#determining-nonlinear-library-of-terms">Determining nonlinear “library” of terms</a></li>
  <li><a href="#numerical-differentiation-of-the-terms" id="toc-numerical-differentiation-of-the-terms" class="nav-link" data-scroll-target="#numerical-differentiation-of-the-terms">Numerical differentiation of the terms</a></li>
  <li><a href="#sparse-regression" id="toc-sparse-regression" class="nav-link" data-scroll-target="#sparse-regression">Sparse regression</a></li>
  <li><a href="#summary-of-the-method" id="toc-summary-of-the-method" class="nav-link" data-scroll-target="#summary-of-the-method">Summary of the method</a></li>
  </ul></li>
  <li><a href="#sec-simulated" id="toc-sec-simulated" class="nav-link" data-scroll-target="#sec-simulated">Application to simulated wave data</a>
  <ul class="collapse">
  <li><a href="#generating-nonlinear-library" id="toc-generating-nonlinear-library" class="nav-link" data-scroll-target="#generating-nonlinear-library">Generating nonlinear library</a></li>
  <li><a href="#approximating-data" id="toc-approximating-data" class="nav-link" data-scroll-target="#approximating-data">Approximating data</a></li>
  <li><a href="#numerically-differentiating-the-neural-network-model" id="toc-numerically-differentiating-the-neural-network-model" class="nav-link" data-scroll-target="#numerically-differentiating-the-neural-network-model">Numerically differentiating the neural network model</a></li>
  <li><a href="#solving-the-sparse-regression-problem" id="toc-solving-the-sparse-regression-problem" class="nav-link" data-scroll-target="#solving-the-sparse-regression-problem">Solving the sparse regression problem</a></li>
  </ul></li>
  <li><a href="#sec-noisy" id="toc-sec-noisy" class="nav-link" data-scroll-target="#sec-noisy">Application to noisy simulated wave data</a></li>
  <li><a href="#sec-extracted" id="toc-sec-extracted" class="nav-link" data-scroll-target="#sec-extracted">Application to extracted wave data</a>
  <ul class="collapse">
  <li><a href="#image-data-extraction" id="toc-image-data-extraction" class="nav-link" data-scroll-target="#image-data-extraction">Image data extraction</a></li>
  <li><a href="#using-our-experimental-dataset" id="toc-using-our-experimental-dataset" class="nav-link" data-scroll-target="#using-our-experimental-dataset">Using our experimental dataset</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#the-benefits-of-jax" id="toc-the-benefits-of-jax" class="nav-link" data-scroll-target="#the-benefits-of-jax">The benefits of JAX</a></li>
  <li><a href="#training-without-normalizing-the-data" id="toc-training-without-normalizing-the-data" class="nav-link" data-scroll-target="#training-without-normalizing-the-data">Training without normalizing the data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Connor Robertson </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>One common hallmark of popular machine learning methods is their “black-box” nature. Since many of these methods are meant solely for prediction, this has not been too much of an issue. After all, a black box method can be as complex as needed since it does not need to be analyzed after the fact. This mentality has given birth to increasingly complex but effective models (just take a look at <a href="https://nikcheerla.github.io/deeplearningschool//media/alphago_arch.png">the model</a> that defeated the worlds best Go player).</p>
<p>However, there has been some recent interest in models that can be understood and analyzed. This is particularly true in the scientific realm, where practicioners looking to use machine learning would like to get an idea of the mechanisms underlying their system of study. In order to do so, new tools have been created and old, interpretable tools, such as linear regression, have been adapted to meet this challenge.</p>
<p>Many of these new, interpretable, models have been named “data-driven model discovery.” Their goals is to model collected data from a system with machine learning tools to determine a human-readable model.</p>
</section>
<section id="sparse-identification-of-nonlinear-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="sparse-identification-of-nonlinear-dynamics">Sparse Identification of Nonlinear Dynamics</h2>
<p>One method for model discovery as described above is called Sparse Identification of Nonlinear Dynamics (SINDy)<span class="citation" data-cites="brunton2016discovering">&nbsp;[<a href="#ref-brunton2016discovering" role="doc-biblioref">1</a>]</span>. The goal of this method is to extract the most probable differential equation directly from data of the important state variables of a continuum system.</p>
<section id="setting-up-linear-problem" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-linear-problem">Setting up linear problem</h3>
<p>As its name suggests, this method works discover models for linear or nonlinear systems. It is based on a simple idea that nonlinear differential equations can be expressed as a linear combination of nonlinear terms<span class="citation" data-cites="williams2015data">&nbsp;[<a href="#ref-williams2015data" role="doc-biblioref">2</a>]</span>. Assuming we are looking at the nonlinear time evolution of some quantity, this could then be written as the sum of <span class="math inline">\(K\)</span> nonlinear terms: <span class="math display">\[
u_t(x,t) = \xi_1\mathcal{N}_1(u,x,t) + \ldots + \xi_K\mathcal{N}_K(u,x,t)
\]</span> If we can then determine what nonlinear terms are possible <span class="math inline">\(\mathcal{N}_i(u,x,t)\)</span>, we can sift through these terms to determine which best contribute to the time evolution of the system.</p>
<p>Ultimately, this boils down to a regression problem. Given some space and time samples of our state variable: <span class="math inline">\(u(x_i,t_j)\)</span> for <span class="math inline">\(i \leq N\)</span> and <span class="math inline">\(j \leq M\)</span>, we can consider the linear system: <span class="math display">\[
u_t(x_i,t_j) = \xi_1\mathcal{N}_1(u_{ij},x_i,t_j) + \ldots + \xi_K\mathcal{N}_K(u_{ij},x_i,t_j)
\]</span> Expanded for all the data samples (flattened across space and time), this can be written as the system: <span id="eq-linear-system"><span class="math display">\[
\begin{bmatrix}
u_t(x_1, t_1) \\
\vdots \\
u_t(x_N, t_1) \\
\vdots \\
u_t(x_N, t_M) \\
\end{bmatrix}
=
\begin{bmatrix}
\mathcal{N}_1(x_1, t_1) &amp; \ldots &amp; \mathcal{N}_K(x_1, t_1) \\
\vdots &amp;  &amp; \vdots \\
\mathcal{N}_1(x_N, t_1) &amp; \ldots &amp; \mathcal{N}_K(x_1, t_1) \\
\vdots &amp;  &amp; \vdots \\
\mathcal{N}_1(x_N, t_M) &amp; \ldots &amp; \mathcal{N}_K(x_1, t_1)
\end{bmatrix}
\vec{\xi}
\tag{1}\]</span></span></p>
<p>Solving this system is then a straightforward linear regression.</p>
</section>
<section id="determining-nonlinear-library-of-terms" class="level3">
<h3 class="anchored" data-anchor-id="determining-nonlinear-library-of-terms">Determining nonlinear “library” of terms</h3>
<p>Determining what <span class="math inline">\(\mathcal{N}_i(u,x,t)\)</span> are reasonable for the system is somewhat of a traditional modeling problem. Are there any symmetries in the system that need to be satisfied? Is there periodic behavior that might warrant inclusion of trignometric terms? What order of polynomial interactions are possible for the system?</p>
<p>The most common library of terms for a 1D function is to put together polynomial interactions with spatial derivatives. Such a library up to 3rd order polynomials and derivatives could be written: <span class="math display">\[
\begin{align*}
\mathcal{N}_1(u,x,t) &amp;= u\\
\mathcal{N}_2(u,x,t) &amp;= u^2\\
&amp;\vdots \\
\mathcal{N}_i(u,x,t) &amp;= u_x\\
\mathcal{N}_{i+1}(u,x,t) &amp;= u_x^2\\
&amp;\vdots \\
\mathcal{N}_K(u,x,t) &amp;= u^3u_{xxx}\\
\end{align*}
\]</span></p>
</section>
<section id="numerical-differentiation-of-the-terms" class="level3">
<h3 class="anchored" data-anchor-id="numerical-differentiation-of-the-terms">Numerical differentiation of the terms</h3>
<p>In order to actually compute the values in the linear system written in <a href="#eq-linear-system">Equation&nbsp;1</a>, we must compute numerical derivatives in both <span class="math inline">\(t\)</span> and in <span class="math inline">\(x\)</span>. This isn’t an issue if we have smooth, reliable data and can be quickly computed with finite differences.</p>
<p>However, the intent of this method is to use data samples <span class="math inline">\(u(x_i,t_j)\)</span> that are collected from the real world, implying that they will each be polluted with some level of noise. There have been several classical methods presented for dealing with numerical differentiation of noisy data that could be used, but generally the methods revolve around an approximate fitting of a differentiable function basis to the data. Notable among these are:</p>
<ul>
<li>Local polynomial regression (LOESS<span class="citation" data-cites="cleveland1988locally">&nbsp;[<a href="#ref-cleveland1988locally" role="doc-biblioref">3</a>]</span>, Savitsky-Golay filter<span class="citation" data-cites="press1990savitzky">&nbsp;[<a href="#ref-press1990savitzky" role="doc-biblioref">4</a>]</span>, etc.)</li>
<li>Radial basis functions (Gaussian kernel)</li>
<li>Smoothing splines</li>
<li>Least squares spectral analysis (LSSA)</li>
</ul>
<p>These can be written along the lines of: <span class="math display">\[
\underset{\vec{c}}{\text{argmin}} \; \sum_{i,j}^{N,M}\|u(x_i,t_j) - F(x_i,t_j,\vec{c})\|_2
\]</span> where <span class="math display">\[
F(x_i,t_j,\vec{c}) = \sum_l^L c_l \phi_l(x_i,t_j)
\]</span> and <span class="math inline">\(\phi\)</span> represents our chosen basis function. Once computed, we can easily approximate derivatives of <span class="math inline">\(u\)</span> via: <span class="math display">\[
u_x(x_i,t_j) \approx F_x(x_i,t_j,\vec{c}) = \sum_l^L c_l \frac{d}{dx}\phi_l(x_i,t_j)
\]</span></p>
<p>Each of these has the goal of smoothing the given data while simultaneously providing an exact derivative of the approximation. This is a similar idea as we have discussed with automatic differentiation of neural networks. In fact, you could consider fitting a neural network to be the same as fitting a randomly initialized nested basis of nonlinear functions (since they are dense according to the universal approximation theorem). We will explore this idea in the example problem in <a href="#sec-simulated">Section&nbsp;3</a>.</p>
</section>
<section id="sparse-regression" class="level3">
<h3 class="anchored" data-anchor-id="sparse-regression">Sparse regression</h3>
<p>Once the matrix in <a href="#eq-linear-system">Equation&nbsp;1</a> has been created using numerical differentiation, it remains to sift through the nonlinear terms to determine which, if any, contribute to the time evolution of our state variable of interest. It is usually reasonable to consider that not all the nonlinear terms should be included in the equation, so we would like to determine the most parsimonious (smallest) combination of them that will capture our desired qualitative and quantitative behavior in the system.</p>
<p>There are two main families of sparse regression methods:</p>
<p><strong>Greedy methods</strong>: Iterative add/remove terms that best match the time derivative in some metric (<span class="math inline">\(R^2\)</span> coefficient of determination, Akaike Information Criteria (AIC), etc.).</p>
<ul>
<li>Forward selection: Start with no terms, add one by one according to which maximizes <span class="math inline">\(R^2\)</span> or AIC at each step</li>
<li>Backward selection: Start with all terms, remove one by one according to which least reduces <span class="math inline">\(R^2\)</span> or AIC</li>
<li>(Orthogonal) Matching pursuit: Start with no terms, add one by one according to which maximizes correlation (orthogonalizing after each step)</li>
</ul>
<p><strong>Regularization methods</strong>: Add a penalty to the regression for having too many terms or large coefficients <span class="math inline">\(\xi_i\)</span>. These can be written roughly as: <span class="math display">\[
\underset{\vec{\xi}}{\text{argmin}}\; \|u_t(x_i,t_j) - \mathbf{\mathcal{N}}(u_{ij},x_i,t_j) \cdot \vec{\xi}\|_2^2 + \lambda \|\xi\|_C
\]</span></p>
<ul>
<li>Ridge regression: Let <span class="math inline">\(C=2\)</span> forcing coefficients <span class="math inline">\(\vec{\xi}\)</span> to be smaller. We hope that important coefficients will remain larger while unimportant ones shrink.</li>
<li>Lasso regression: Let <span class="math inline">\(C=1\)</span> forcing coefficients <span class="math inline">\(\vec{\xi}\)</span> to be smaller and various to be set to 0 (due to the geometry of the 1-norm).</li>
<li>0-norm regression: Let <span class="math inline">\(C=0\)</span> which is a measure that counts the number of nonzero coefficients in <span class="math inline">\(\vec{\xi}\)</span>. Computing this usually requires a combination of regularization and relaxation best captured by the SR3 method<span class="citation" data-cites="zheng2018unified">&nbsp;[<a href="#ref-zheng2018unified" role="doc-biblioref">5</a>]</span>.</li>
</ul>
<p>Combinations of these two methods which iterative perform regularization methods removing terms with small coefficients according to a given threshold have also been proposed (Sequential Threshold Ridge Regression<span class="citation" data-cites="rudy2017data">&nbsp;[<a href="#ref-rudy2017data" role="doc-biblioref">6</a>]</span> or the original SINDy algorithm<span class="citation" data-cites="brunton2016discovering">&nbsp;[<a href="#ref-brunton2016discovering" role="doc-biblioref">1</a>]</span>).</p>
</section>
<section id="summary-of-the-method" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-the-method">Summary of the method</h3>
<p>In summary, the procedure to use SINDy is as follows:</p>
<ol type="1">
<li>Collect sample points of a continuum state variable of interest <span class="math inline">\(u(x_i,t_j)\)</span></li>
<li>Form a “library” of possible terms for the differential model of the system <span class="math inline">\(\mathcal{N}_k(u,x,t)\)</span></li>
<li>Compute the libary at sample points using noise robust numerical differentiation to compute both <span class="math inline">\(u_t(x_i,t_j)\)</span> and <span class="math inline">\(\mathcal{N}_k(u_{ij},x_i,t_j)\)</span></li>
<li>Use sparse regression to determine a sparse vector <span class="math inline">\(\vec{\xi}\)</span> which closely approximates <span class="math inline">\(u_t(x_i,t_j) = \xi_1\mathcal{N}_1(u_{ij},x_i,t_j) + \ldots + \xi_K\mathcal{N}_K(u_{ij},x_i,t_j)\)</span></li>
</ol>
<p>To really explore this method, we will walk through this process using simulated traveling wave data in <a href="#sec-simulated">Section&nbsp;3</a> and using real extracted data in <a href="#sec-extracted">Section&nbsp;5</a>.</p>
</section>
</section>
<section id="sec-simulated" class="level2">
<h2 class="anchored" data-anchor-id="sec-simulated">Application to simulated wave data</h2>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For this workshop you will need to install the following packages:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> install numpy matplotlib py-pde sympy jax optax flax scikit-learn scikit-image av</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Given some data generated via finite differences of the simple advection equation: <span class="math display">\[
h_t(x,t) = h_x(x,t)
\]</span> with periodic boundaries and a Gaussian initial condition, we have the following measurement of state variable <span class="math inline">\(h\)</span> (height of the wave):</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Generate simple wave data</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pde</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Domain</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>xmax <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>nx <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>tmax <span class="op">=</span> <span class="fl">1.0</span><span class="op">-</span><span class="dv">2</span><span class="op">*</span>dt</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>save_dt <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>init_cond <span class="op">=</span> <span class="st">".1*exp(-(1/.01)*(x-0.3)**2)"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> pde.CartesianGrid([(<span class="fl">0.0</span>,xmax)],nx,periodic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> pde.ScalarField.from_expression(grid,init_cond,label<span class="op">=</span><span class="st">"h(x,t)"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>eq <span class="op">=</span> pde.PDE({<span class="st">"h"</span>: <span class="st">"-d_dx(h)"</span>})</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>storage <span class="op">=</span> pde.MemoryStorage()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> eq.solve(h,t_range<span class="op">=</span>tmax,dt<span class="op">=</span>dt,tracker<span class="op">=</span>storage.tracker(save_dt),ret_info<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># pde.plot_kymograph(storage)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>movie <span class="op">=</span> pde.visualization.movie(storage,<span class="st">"simple_wave.gif"</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>h<span class="op">=</span>np.array(storage.data)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>storage.grid.coordinate_arrays[<span class="dv">0</span>]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>t<span class="op">=</span>np.array(storage.times)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>np.savez(<span class="st">"simple_wave.npz"</span>,h<span class="op">=</span>h,x<span class="op">=</span>x,t<span class="op">=</span>t)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"026e1da955724f31b6258ea93ff878d6","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p><img src="simple_wave.gif" class="img-fluid"></p>
<section id="generating-nonlinear-library" class="level3">
<h3 class="anchored" data-anchor-id="generating-nonlinear-library">Generating nonlinear library</h3>
<p>Generating a library can be most easily accomplished using the <code>sympy</code> symbolic math Python library. To be overly thorough, we will generate up to 4th order polynomial combinations of up to 4th order spatial derivatives.</p>
<p>We can first initialize our spatial and state variables:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x_sym,t_sym <span class="op">=</span> sp.symbols(<span class="st">"x t"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>h_sym <span class="op">=</span> sp.Function(<span class="st">"h"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Given a specified order, we can now create symbolic derivative terms (constructed to be most legible):</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Library parameters</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>max_poly_order <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>max_diff_order <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>diff_terms <span class="op">=</span> [h_sym(x_sym,t_sym)]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>diff_terms <span class="op">+=</span> [sp.Function(<span class="bu">str</span>(h_sym)<span class="op">+</span><span class="st">"_"</span><span class="op">+</span>(i<span class="op">*</span><span class="bu">str</span>(x_sym)))(x_sym,t_sym) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,max_diff_order<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(diff_terms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t)]</code></pre>
</div>
</div>
<p>Now, combining these into polynomials up to 4th order (again, this is overkill, but for a system you don’t fully understand, you may want to have a very complete library):</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> combinations_with_replacement</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>terms <span class="op">=</span> []</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> po <span class="kw">in</span> <span class="bu">range</span>(max_poly_order<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> po <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        term <span class="op">=</span> sp.core.numbers.One()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        combos <span class="op">=</span> combinations_with_replacement(diff_terms,po)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> combo <span class="kw">in</span> combos:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            term <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> combo_term <span class="kw">in</span> combo:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                term <span class="op">*=</span> combo_term</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            terms.append(term)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(terms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t), h(x, t)**2, h(x, t)*h_x(x, t), h(x, t)*h_xx(x, t), h(x, t)*h_xxx(x, t), h(x, t)*h_xxxx(x, t), h_x(x, t)**2, h_x(x, t)*h_xx(x, t), h_x(x, t)*h_xxx(x, t), h_x(x, t)*h_xxxx(x, t), h_xx(x, t)**2, h_xx(x, t)*h_xxx(x, t), h_xx(x, t)*h_xxxx(x, t), h_xxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t), h_xxxx(x, t)**2, h(x, t)**3, h(x, t)**2*h_x(x, t), h(x, t)**2*h_xx(x, t), h(x, t)**2*h_xxx(x, t), h(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_xxx(x, t)**2, h(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xxxx(x, t)**2, h_x(x, t)**3, h_x(x, t)**2*h_xx(x, t), h_x(x, t)**2*h_xxx(x, t), h_x(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxxx(x, t)**2, h_xx(x, t)**3, h_xx(x, t)**2*h_xxx(x, t), h_xx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)*h_xxxx(x, t)**2, h_xxx(x, t)**3, h_xxx(x, t)**2*h_xxxx(x, t), h_xxx(x, t)*h_xxxx(x, t)**2, h_xxxx(x, t)**3, h(x, t)**4, h(x, t)**3*h_x(x, t), h(x, t)**3*h_xx(x, t), h(x, t)**3*h_xxx(x, t), h(x, t)**3*h_xxxx(x, t), h(x, t)**2*h_x(x, t)**2, h(x, t)**2*h_x(x, t)*h_xx(x, t), h(x, t)**2*h_x(x, t)*h_xxx(x, t), h(x, t)**2*h_x(x, t)*h_xxxx(x, t), h(x, t)**2*h_xx(x, t)**2, h(x, t)**2*h_xx(x, t)*h_xxx(x, t), h(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxx(x, t)**2, h(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxxx(x, t)**2, h(x, t)*h_x(x, t)**3, h(x, t)*h_x(x, t)**2*h_xx(x, t), h(x, t)*h_x(x, t)**2*h_xxx(x, t), h(x, t)*h_x(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t)**2, h(x, t)*h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xx(x, t)**3, h(x, t)*h_xx(x, t)**2*h_xxx(x, t), h(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxx(x, t)**3, h(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxxx(x, t)**3, h_x(x, t)**4, h_x(x, t)**3*h_xx(x, t), h_x(x, t)**3*h_xxx(x, t), h_x(x, t)**3*h_xxxx(x, t), h_x(x, t)**2*h_xx(x, t)**2, h_x(x, t)**2*h_xx(x, t)*h_xxx(x, t), h_x(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxx(x, t)**2, h_x(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxxx(x, t)**2, h_x(x, t)*h_xx(x, t)**3, h_x(x, t)*h_xx(x, t)**2*h_xxx(x, t), h_x(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxx(x, t)**3, h_x(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxxx(x, t)**3, h_xx(x, t)**4, h_xx(x, t)**3*h_xxx(x, t), h_xx(x, t)**3*h_xxxx(x, t), h_xx(x, t)**2*h_xxx(x, t)**2, h_xx(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)**2*h_xxxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)**3, h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_xx(x, t)*h_xxxx(x, t)**3, h_xxx(x, t)**4, h_xxx(x, t)**3*h_xxxx(x, t), h_xxx(x, t)**2*h_xxxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t)**3, h_xxxx(x, t)**4]</code></pre>
</div>
</div>
</section>
<section id="approximating-data" class="level3">
<h3 class="anchored" data-anchor-id="approximating-data">Approximating data</h3>
<p>In order to provide numerical derivatives of our data, we will use a neural network approximation.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is far beyond what is necessary for this particular setting, but is a method that can generalize to data not on a uniform grid and in high dimension, which can be useful. The lack of requirement for a grid can also help with robustly fitting to noisy data by using a train-test methodology in <a href="#sec-noisy">Section&nbsp;4</a> which classical basis functions do not handle well. Using neural networks in this way as a combination with SINDy is explored more in<span class="citation" data-cites="xu2019dl">&nbsp;[<a href="#ref-xu2019dl" role="doc-biblioref">7</a>]</span>.</p>
</div>
</div>
<p>To begin, we will be using the Google developed <a href="https://flax.readthedocs.io/en/latest/"><code>flax</code></a> neural network framework which is built on their <a href="https://jax.readthedocs.io/en/latest/index.html"><code>jax</code></a> automatic differentiation library and the <a href="https://optax.readthedocs.io/en/latest/optax-101.html"><code>optax</code></a> optimization library. The reason for this will become clearer when we consider taking a fourth order derivative in <span class="math inline">\(x\)</span> of the network, a task which many other popular frameworks (<code>pytorch</code>, <code>keras</code>, <code>tensorflow</code>, etc.) cannot do (at least not nearly as concisely). However, the <code>jax</code> library is state-of-the-art for automatic differentiation and is used heavily for differentiable programming and neural network research today (see Appendix for more information).</p>
<section id="creating-the-neural-network-model" class="level4">
<h4 class="anchored" data-anchor-id="creating-the-neural-network-model">Creating the neural network model</h4>
<p>First, we will create a simple dense neural network model using the <span class="math inline">\(\tanh\)</span> activation (to ensure a smooth approximation):</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> flax.linen <span class="im">as</span> nn</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyNet(nn.Module):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">@nn.compact</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.Dense(<span class="dv">60</span>)(x)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.tanh(x)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.Dense(<span class="dv">12</span>)(x)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.tanh(x)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> nn.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This model will take an input of <span class="math inline">\((x_i,t_j)\)</span> (a dimension 2 array), linearly map it to a dimension 60 space, apply a tanh activation, linearly map to a dimension 12 space, apply a tanh activation, then linearly map to a dimension 1 output (this particular width and depth was chosen arbitrarily).</p>
<p>We next initialize the parameters of the network (each of the linear transformation matrices) and print out the dimensions of the corresponding arrays:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>jax.config.update(<span class="st">"jax_platform_name"</span>, <span class="st">"cpu"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Random generator seed</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>rng1,rng2 <span class="op">=</span> jax.random.split(jax.random.PRNGKey(<span class="dv">42</span>))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>random_data <span class="op">=</span> jax.random.normal(rng1,(<span class="dv">2</span>,))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> MyNet()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>params1 <span class="op">=</span> model1.init(rng2,random_data)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(jax.tree_util.tree_map(<span class="kw">lambda</span> x: x.shape, params1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>FrozenDict({
    params: {
        Dense_0: {
            bias: (60,),
            kernel: (2, 60),
        },
        Dense_1: {
            bias: (12,),
            kernel: (60, 12),
        },
        Dense_2: {
            bias: (1,),
            kernel: (12, 1),
        },
    },
})</code></pre>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confusing <code>tree_util.tree_map</code> command is a convenience function for mapping a function (in this case <code>lambda x: x.shape</code>) across a set of different objects. This is useful because these objects can be arrays, dictionaries, lists, classes (i.e.&nbsp;other neural networks), etc.</p>
</div>
</div>
</section>
<section id="loading-and-processing-data" class="level4">
<h4 class="anchored" data-anchor-id="loading-and-processing-data">Loading and processing data</h4>
<p>In order to fit this model to the data, we must load the data into batches of <span class="math inline">\((x_i,t_j,u(x_i,t_j))\)</span> points. Since our data is known to be quite smooth and we want to maximize the fit, we will use batches of size 10000:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_data(data_path,noise_scale<span class="op">=</span><span class="dv">0</span>,norm<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    raw_data <span class="op">=</span> np.load(data_path)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> raw_data[<span class="st">"h"</span>].astype(jnp.float32)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> raw_data[<span class="st">"x"</span>].astype(jnp.float32)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> raw_data[<span class="st">"t"</span>].astype(jnp.float32)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add noise if needed</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    h <span class="op">+=</span> noise_scale<span class="op">*</span>jnp.std(h)<span class="op">*</span>np.random.normal(size<span class="op">=</span>h.shape)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mean center, std center data</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> (h <span class="op">-</span> jnp.mean(h)) <span class="op">/</span> jnp.std(h)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> (x <span class="op">-</span> jnp.mean(x)) <span class="op">/</span> jnp.std(x)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> (t <span class="op">-</span> jnp.mean(t)) <span class="op">/</span> jnp.std(t)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x,t,h</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_data(x,t,h,batch_size):</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split data into batches</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(x),batch_size):</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        temp_xt <span class="op">=</span> jnp.vstack((x[i:i<span class="op">+</span>batch_size], t[i:i<span class="op">+</span>batch_size])).T</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        temp_h <span class="op">=</span> h[i:i<span class="op">+</span>batch_size].reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        data.append((temp_xt,temp_h))</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>x,t,h <span class="op">=</span> load_data(<span class="st">"simple_wave.npz"</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>X,T <span class="op">=</span> jnp.meshgrid(x,t)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> batch_data(X.flatten(),T.flatten(),h.flatten(),<span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the data needed to be centered and scaled to have a mean of <span class="math inline">\(\bar{h}=0\)</span> and standard deviation of <span class="math inline">\(\overline{(h - \bar{h})}=1\)</span> in order to best use the <span class="math inline">\(\tanh\)</span> activation (which extends from -1 to 1).</p>
</section>
<section id="training-the-model" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model">Training the model</h4>
<p>We will use the mean squared error fit of the data to our neural network output (just in time compiled with <code>@jax.jit</code> for maximum speed):</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(params,<span class="bu">input</span>,targets):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> squared_error(x,y):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model1.<span class="bu">apply</span>(params,x)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.mean((y <span class="op">-</span> pred)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.mean(jax.vmap(squared_error)(<span class="bu">input</span>,targets),axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>loss_grad_fn <span class="op">=</span> jax.value_and_grad(mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this loss defined, we initialize an ADAM optimizer and optimizer state and wrap the loss function to return both the output and gradient:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>tx <span class="op">=</span> optax.adam(learning_rate)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> tx.init(params1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now train the model to take in <span class="math inline">\((x_i,t_j)\)</span> and output <span class="math inline">\(u(x_i,t_j)\)</span>. Performing 1000 iterations over the data, we will print the mean squared error on the data as we proceed with the training:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>all_xt <span class="op">=</span> jnp.array([data[i][<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))])</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>all_h <span class="op">=</span> jnp.array([data[i][<span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))])</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    xt_batch <span class="op">=</span> data[i<span class="op">%</span><span class="bu">len</span>(data)][<span class="dv">0</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    h_batch <span class="op">=</span> data[i<span class="op">%</span><span class="bu">len</span>(data)][<span class="dv">1</span>]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    loss_val, grads <span class="op">=</span> loss_grad_fn(params1, xt_batch, h_batch)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    updates, opt_state <span class="op">=</span> tx.update(grads, opt_state)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    params1 <span class="op">=</span> optax.apply_updates(params1, updates)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> mse(params1,all_xt,all_h)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training loss step </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i,train_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 0: 1.0898101329803467</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 100: 0.16434669494628906</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 200: 0.09155044704675674</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 300: 0.01834404096007347</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 400: 0.0012183969374746084</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 500: 0.0004402332124300301</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 600: 0.000266294606262818</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 700: 0.00018376208026893437</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 800: 0.00013974899775348604</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 900: 0.00011441211245255545</code></pre>
</div>
</div>
<p>As you can tell, this procedure is somewhat more manual than other libraries such as <code>keras</code> but keep you closer to the details, allowing for more flexibility in implementation.</p>
</section>
<section id="validating-fit" class="level4">
<h4 class="anchored" data-anchor-id="validating-fit">Validating fit</h4>
<p>The fit to the model can be visualized as follows:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> anim</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>X,T <span class="op">=</span> jnp.meshgrid(x,t)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>xt_points <span class="op">=</span> jnp.vstack([X.flatten(),T.flatten()]).T</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>hhat1 <span class="op">=</span> model1.<span class="bu">apply</span>(params1,xt_points).reshape(X.shape)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.sqrt((h <span class="op">-</span> hhat1)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> animate_data(x,t,data_list,labels):</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure()</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    plots <span class="op">=</span> []</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data_list)):</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        plot <span class="op">=</span> plt.plot(x,data_list[i][<span class="dv">0</span>,:],label<span class="op">=</span>labels[i])[<span class="dv">0</span>]</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        plots.append(plot)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> anim_func(j):</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(plots)):</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>            plots[i].set_ydata(data_list[i][j,:])</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> plots</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    approx_anim <span class="op">=</span> anim.FuncAnimation(fig, anim_func, <span class="bu">range</span>(<span class="bu">len</span>(t)))</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> approx_anim</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>animation1 <span class="op">=</span> animate_data(x,t,[h,hhat1,diff],[<span class="st">"$h$"</span>,<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$"</span>,<span class="st">"$L^2$ error"</span>])</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>animation1.save(<span class="st">"clean_h_compare.gif"</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="clean_h_compare.gif" class="img-fluid"></p>
</section>
</section>
<section id="numerically-differentiating-the-neural-network-model" class="level3">
<h3 class="anchored" data-anchor-id="numerically-differentiating-the-neural-network-model">Numerically differentiating the neural network model</h3>
<p>The original reason to fit this model to the data was to be able to construct each of the terms in our nonlinear libary for the system. In order to differentiate the model, we must wrap it in a function that takes our inputs and returns the output.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_for_diff(x,t):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    new_x <span class="op">=</span> jnp.array([x,t])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model1.<span class="bu">apply</span>(params1, new_x)[<span class="dv">0</span>]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a derivative with respect to the first input (x) at point (x_i,t_j)</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>x_i <span class="op">=</span> <span class="fl">0.3</span><span class="op">;</span> t_j <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>jax.grad(model_for_diff,<span class="dv">0</span>)(x_i,t_j)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>DeviceArray(0.01142633, dtype=float32, weak_type=True)</code></pre>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we were to differentiate the model directly, we would compute derivatives for all the parameters! This is the main challenge with using other neural network frameworks for this kind of function approximation.</p>
</div>
</div>
<p>Applying this iteratively, we can construct derivatives <span class="math inline">\(h_x(x,t), \ldots, h_{xxxx}(x,t)\)</span> as is required by our library:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>diff_term_values <span class="op">=</span> {}</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_diff_order<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    diff_func <span class="op">=</span> model_for_diff</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteratively apply derivatives</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(i):</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        diff_func <span class="op">=</span> jax.grad(diff_func, <span class="dv">0</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        new_x,new_t <span class="op">=</span> x</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> diff_func(new_x,new_t)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    diff_term_values[diff_terms[i]] <span class="op">=</span> np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_points))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then reconstruct our terms attaching them to their corresponding values on our <span class="math inline">\((x,t)\)</span> grid:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> construct_terms(diff_term_values):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    term_values <span class="op">=</span> {}</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    term_shape <span class="op">=</span> np.shape(diff_term_values[<span class="bu">list</span>(diff_term_values.keys())[<span class="dv">0</span>]])</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> order <span class="kw">in</span> <span class="bu">range</span>(max_poly_order<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> order <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>            term <span class="op">=</span> sp.core.numbers.One()</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>            term_values[term] <span class="op">=</span> np.ones(term_shape)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>            combos <span class="op">=</span> combinations_with_replacement(diff_terms,order)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> combo <span class="kw">in</span> combos:</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>                term <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>                temp_term_value <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> combo_term <span class="kw">in</span> combo:</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>                    term <span class="op">*=</span> combo_term</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>                    temp_term_value <span class="op">*=</span> diff_term_values[combo_term]</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>                term_values[term] <span class="op">=</span> temp_term_value</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> term_values</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>term_values <span class="op">=</span> construct_terms(diff_term_values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we compute the derivative of the network with respect to time:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    new_x,new_t <span class="op">=</span> x</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.grad(model_for_diff,<span class="dv">1</span>)(new_x,new_t)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>h_t_term <span class="op">=</span> sp.Function(<span class="st">"h_t"</span>)(x_sym,t_sym)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>h_t <span class="op">=</span> <span class="op">-</span>np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_points))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="solving-the-sparse-regression-problem" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-sparse-regression-problem">Solving the sparse regression problem</h3>
<p>In order to cleanly work with our term library, we will use a very popular Python data science package called <code>pandas</code>. Simply put, this library allows you to easily load, manipulate, and save tabular data. Here is our library as a <code>pandas</code> <code>DataFrame</code>:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>term_matrix <span class="op">=</span> pd.DataFrame(term_values,index<span class="op">=</span>pd.MultiIndex.from_arrays(np.<span class="bu">round</span>(np.array(xt_points),<span class="dv">2</span>).T, names<span class="op">=</span>(<span class="st">"x"</span>,<span class="st">"t"</span>)))</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>term_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th>1</th>
      <th>h(x, t)</th>
      <th>h_x(x, t)</th>
      <th>h_xx(x, t)</th>
      <th>h_xxx(x, t)</th>
      <th>h_xxxx(x, t)</th>
      <th>h(x, t)**2</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h(x, t)*h_xx(x, t)</th>
      <th>h(x, t)*h_xxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
    <tr>
      <th>x</th>
      <th>t</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>-1.71</th>
      <th>-1.71</th>
      <td>1.0</td>
      <td>-0.585482</td>
      <td>0.032621</td>
      <td>0.299091</td>
      <td>3.230638</td>
      <td>30.858641</td>
      <td>0.342789</td>
      <td>-0.019099</td>
      <td>-0.175112</td>
      <td>-1.891480</td>
      <td>...</td>
      <td>85.184204</td>
      <td>10.084811</td>
      <td>96.328819</td>
      <td>920.120422</td>
      <td>8.788872e+03</td>
      <td>108.931458</td>
      <td>1040.499268</td>
      <td>9.938715e+03</td>
      <td>9.493333e+04</td>
      <td>9.067909e+05</td>
    </tr>
    <tr>
      <th>-1.68</th>
      <th>-1.71</th>
      <td>1.0</td>
      <td>-0.584148</td>
      <td>0.045153</td>
      <td>0.431625</td>
      <td>4.487586</td>
      <td>42.299286</td>
      <td>0.341229</td>
      <td>-0.026376</td>
      <td>-0.252133</td>
      <td>-2.621416</td>
      <td>...</td>
      <td>333.333466</td>
      <td>39.007198</td>
      <td>367.675781</td>
      <td>3465.654785</td>
      <td>3.266672e+04</td>
      <td>405.556274</td>
      <td>3822.710205</td>
      <td>3.603227e+04</td>
      <td>3.396346e+05</td>
      <td>3.201342e+06</td>
    </tr>
    <tr>
      <th>-1.65</th>
      <th>-1.71</th>
      <td>1.0</td>
      <td>-0.582291</td>
      <td>0.063118</td>
      <td>0.615352</td>
      <td>6.208448</td>
      <td>57.810596</td>
      <td>0.339063</td>
      <td>-0.036753</td>
      <td>-0.358314</td>
      <td>-3.615125</td>
      <td>...</td>
      <td>1265.501343</td>
      <td>147.256027</td>
      <td>1371.189453</td>
      <td>12767.969727</td>
      <td>1.188902e+05</td>
      <td>1485.703979</td>
      <td>13834.283203</td>
      <td>1.288193e+05</td>
      <td>1.199514e+06</td>
      <td>1.116940e+07</td>
    </tr>
    <tr>
      <th>-1.61</th>
      <th>-1.71</th>
      <td>1.0</td>
      <td>-0.579688</td>
      <td>0.088593</td>
      <td>0.868975</td>
      <td>8.553134</td>
      <td>78.522270</td>
      <td>0.336039</td>
      <td>-0.051356</td>
      <td>-0.503735</td>
      <td>-4.958152</td>
      <td>...</td>
      <td>4655.866211</td>
      <td>543.729858</td>
      <td>4991.726074</td>
      <td>45826.671875</td>
      <td>4.207130e+05</td>
      <td>5351.814941</td>
      <td>49132.476562</td>
      <td>4.510620e+05</td>
      <td>4.140987e+06</td>
      <td>3.801644e+07</td>
    </tr>
    <tr>
      <th>-1.58</th>
      <th>-1.71</th>
      <td>1.0</td>
      <td>-0.576034</td>
      <td>0.124417</td>
      <td>1.217474</td>
      <td>11.722404</td>
      <td>105.592400</td>
      <td>0.331815</td>
      <td>-0.071668</td>
      <td>-0.701306</td>
      <td>-6.752501</td>
      <td>...</td>
      <td>16526.652344</td>
      <td>1961.145264</td>
      <td>17665.492188</td>
      <td>159126.218750</td>
      <td>1.433368e+06</td>
      <td>18882.812500</td>
      <td>170091.531250</td>
      <td>1.532141e+06</td>
      <td>1.380113e+07</td>
      <td>1.243170e+08</td>
    </tr>
    <tr>
      <th>...</th>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1.58</th>
      <th>1.71</th>
      <td>1.0</td>
      <td>-0.593596</td>
      <td>-0.031917</td>
      <td>0.124309</td>
      <td>0.865729</td>
      <td>5.409935</td>
      <td>0.352356</td>
      <td>0.018946</td>
      <td>-0.073790</td>
      <td>-0.513894</td>
      <td>...</td>
      <td>0.452263</td>
      <td>0.080658</td>
      <td>0.504034</td>
      <td>3.149705</td>
      <td>1.968248e+01</td>
      <td>0.561731</td>
      <td>3.510254</td>
      <td>2.193554e+01</td>
      <td>1.370750e+02</td>
      <td>8.565805e+02</td>
    </tr>
    <tr>
      <th>1.61</th>
      <th>1.71</th>
      <td>1.0</td>
      <td>-0.594621</td>
      <td>-0.027048</td>
      <td>0.158013</td>
      <td>1.093115</td>
      <td>7.855449</td>
      <td>0.353574</td>
      <td>0.016083</td>
      <td>-0.093958</td>
      <td>-0.649989</td>
      <td>...</td>
      <td>1.540738</td>
      <td>0.206391</td>
      <td>1.483188</td>
      <td>10.658629</td>
      <td>7.659605e+01</td>
      <td>1.427789</td>
      <td>10.260510</td>
      <td>7.373505e+01</td>
      <td>5.298819e+02</td>
      <td>3.807887e+03</td>
    </tr>
    <tr>
      <th>1.65</th>
      <th>1.71</th>
      <td>1.0</td>
      <td>-0.595455</td>
      <td>-0.020860</td>
      <td>0.201215</td>
      <td>1.422090</td>
      <td>11.353171</td>
      <td>0.354567</td>
      <td>0.012421</td>
      <td>-0.119814</td>
      <td>-0.846791</td>
      <td>...</td>
      <td>5.218610</td>
      <td>0.578684</td>
      <td>4.619892</td>
      <td>36.882626</td>
      <td>2.944502e+02</td>
      <td>4.089862</td>
      <td>32.651169</td>
      <td>2.606686e+02</td>
      <td>2.081032e+03</td>
      <td>1.661379e+04</td>
    </tr>
    <tr>
      <th>1.68</th>
      <th>1.71</th>
      <td>1.0</td>
      <td>-0.596047</td>
      <td>-0.012948</td>
      <td>0.258214</td>
      <td>1.896819</td>
      <td>16.340858</td>
      <td>0.355271</td>
      <td>0.007718</td>
      <td>-0.153907</td>
      <td>-1.130592</td>
      <td>...</td>
      <td>17.803600</td>
      <td>1.762207</td>
      <td>15.181189</td>
      <td>130.784027</td>
      <td>1.126688e+03</td>
      <td>12.945052</td>
      <td>111.519989</td>
      <td>9.607307e+02</td>
      <td>8.276573e+03</td>
      <td>7.130162e+04</td>
    </tr>
    <tr>
      <th>1.71</th>
      <th>1.71</th>
      <td>1.0</td>
      <td>-0.596326</td>
      <td>-0.002742</td>
      <td>0.335006</td>
      <td>2.578177</td>
      <td>23.412802</td>
      <td>0.355605</td>
      <td>0.001635</td>
      <td>-0.199773</td>
      <td>-1.537435</td>
      <td>...</td>
      <td>61.519466</td>
      <td>5.741048</td>
      <td>52.135292</td>
      <td>473.448181</td>
      <td>4.299452e+03</td>
      <td>44.182579</td>
      <td>401.228424</td>
      <td>3.643614e+03</td>
      <td>3.308818e+04</td>
      <td>3.004786e+05</td>
    </tr>
  </tbody>
</table>
<p>10000 rows × 126 columns</p>
</div>
</div>
</div>
<p>We then use another extremely popular machine learning Python package called <code>scikit-learn</code> to easily work with our regression models.</p>
<section id="ordinary-least-squares" class="level4">
<h4 class="anchored" data-anchor-id="ordinary-least-squares">Ordinary least squares</h4>
<p>First, let’s apply ordinary least squares to see if the solution is clear:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics <span class="im">as</span> met</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_ols_results(A,b):</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    ols <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    ols.fit(A, b)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    Rsquare <span class="op">=</span> met.r2_score(ols.predict(A), b)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"R^2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(Rsquare))</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    ols_results <span class="op">=</span> pd.DataFrame(</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>[ols.coef_],</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span>term_matrix.columns,</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        index<span class="op">=</span>[<span class="st">"Coefficients"</span>]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ols_results</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>compute_ols_results(term_matrix, h_t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9987072399768019</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="17">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>1</th>
      <th>h(x, t)</th>
      <th>h_x(x, t)</th>
      <th>h_xx(x, t)</th>
      <th>h_xxx(x, t)</th>
      <th>h_xxxx(x, t)</th>
      <th>h(x, t)**2</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h(x, t)*h_xx(x, t)</th>
      <th>h(x, t)*h_xxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>7.305386e-07</td>
      <td>-0.000263</td>
      <td>0.007865</td>
      <td>0.00078</td>
      <td>0.020306</td>
      <td>0.000558</td>
      <td>-0.000865</td>
      <td>0.001602</td>
      <td>0.003387</td>
      <td>-0.003978</td>
      <td>...</td>
      <td>1.934960e-10</td>
      <td>-9.703247e-10</td>
      <td>-5.762052e-10</td>
      <td>2.650240e-11</td>
      <td>1.596587e-13</td>
      <td>7.131852e-11</td>
      <td>-6.073799e-12</td>
      <td>-4.703859e-13</td>
      <td>2.591677e-14</td>
      <td>-1.654493e-16</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 126 columns</p>
</div>
</div>
</div>
<p>Although the <span class="math inline">\(R^2\)</span> value implies that we have successful explained the variance in <span class="math inline">\(h_t\)</span> by linearly combining our term library, it is unclear which of all the terms most contributes to the time evolution from their coefficients.</p>
</section>
<section id="lasso" class="level4">
<h4 class="anchored" data-anchor-id="lasso">Lasso</h4>
<p>Now, let’s add some regularization to try to remove some terms with the Lasso regression:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_lasso_results(A,b,lamb):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    lasso <span class="op">=</span> lm.Lasso(lamb)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    lasso.fit(A,b)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    lasso_results <span class="op">=</span> pd.DataFrame(</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>[lasso.coef_[lasso.coef_ <span class="op">!=</span> <span class="dv">0</span>]],</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span>term_matrix.columns[lasso.coef_ <span class="op">!=</span> <span class="dv">0</span>],</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        index<span class="op">=</span>[<span class="st">"Coefficients"</span>]</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lasso_results</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>compute_lasso_results(term_matrix,h_t,<span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+03, tolerance: 1.101e+01
  model = cd_fast.enet_coordinate_descent(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="18">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)</th>
      <th>h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)*h_x(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.000019</td>
      <td>0.000042</td>
      <td>0.000006</td>
      <td>-0.000016</td>
      <td>0.000005</td>
      <td>-9.063636e-08</td>
      <td>0.000033</td>
      <td>-0.000011</td>
      <td>-0.000004</td>
      <td>-0.000003</td>
      <td>...</td>
      <td>2.762180e-11</td>
      <td>7.853975e-10</td>
      <td>-2.607652e-10</td>
      <td>4.844794e-12</td>
      <td>-2.047032e-13</td>
      <td>-2.796429e-11</td>
      <td>7.585219e-12</td>
      <td>-3.492170e-13</td>
      <td>1.942534e-14</td>
      <td>-4.624859e-16</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 86 columns</p>
</div>
</div>
</div>
<p>Now this at least removed some of the terms, but it also removed the term we know is correct! It’s somewhat hard to interpret exactly what this means. A convenient analysis using the Lasso method is to perform a “lasso path” in which we steadily decrease the regularization <span class="math inline">\(\lambda\)</span> to add more and more terms and pay attention to the order with which they are added:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_lasso_path_results(A,b):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    lambs, coef_path, _ <span class="op">=</span> lm.lasso_path(A, b, alphas<span class="op">=</span>[<span class="dv">1000</span>,<span class="dv">200</span>,<span class="dv">100</span>,<span class="dv">10</span>,<span class="dv">2</span>])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(coef_path.shape[<span class="dv">1</span>]):</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"lambda = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(lambs[i]))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        temp_results <span class="op">=</span> pd.DataFrame(</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>[coef_path[:,i][coef_path[:,i] <span class="op">!=</span> <span class="dv">0</span>]],</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>            columns<span class="op">=</span>term_matrix.columns[coef_path[:,i] <span class="op">!=</span> <span class="dv">0</span>],</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>            index<span class="op">=</span>[<span class="st">"Coefficients"</span>]</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        display(temp_results)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>compute_lasso_path_results(term_matrix,h_t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>lambda = 1000</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6479.129693739391, tolerance: 11.011605073535403
  model = cd_fast.enet_coordinate_descent_gram(
/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4239.991695254213, tolerance: 11.011605073535403
  model = cd_fast.enet_coordinate_descent_gram(
/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3002.8945381092335, tolerance: 11.011605073535403
  model = cd_fast.enet_coordinate_descent_gram(
/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1179.975834201264, tolerance: 11.011605073535403
  model = cd_fast.enet_coordinate_descent_gram(
/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 654.8432797880382, tolerance: 11.011605073535403
  model = cd_fast.enet_coordinate_descent_gram(</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxxx(x, t)**2</th>
      <th>h_x(x, t)**2*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xxx(x, t)**2</th>
      <th>h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)**2*h_xxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>0.000004</td>
      <td>-1.358431e-07</td>
      <td>-0.000003</td>
      <td>1.734877e-07</td>
      <td>-0.000024</td>
      <td>-0.000018</td>
      <td>0.000018</td>
      <td>1.798373e-07</td>
      <td>2.194965e-08</td>
      <td>0.00002</td>
      <td>...</td>
      <td>1.755277e-12</td>
      <td>5.409176e-09</td>
      <td>-4.538993e-10</td>
      <td>-1.582512e-11</td>
      <td>-5.175585e-13</td>
      <td>-1.599987e-10</td>
      <td>1.446256e-11</td>
      <td>-1.393073e-12</td>
      <td>5.006291e-15</td>
      <td>-1.081716e-15</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 65 columns</p>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>lambda = 200</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxxx(x, t)**2</th>
      <th>h_x(x, t)**2*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)**2</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.000006</td>
      <td>0.000006</td>
      <td>-7.521095e-08</td>
      <td>-0.00002</td>
      <td>-0.000005</td>
      <td>1.823142e-07</td>
      <td>-0.000026</td>
      <td>0.000536</td>
      <td>0.000015</td>
      <td>-0.000011</td>
      <td>...</td>
      <td>-2.414194e-11</td>
      <td>5.320476e-09</td>
      <td>-4.616117e-10</td>
      <td>-1.611757e-11</td>
      <td>-6.934056e-13</td>
      <td>-1.620881e-10</td>
      <td>1.653062e-11</td>
      <td>-8.339717e-13</td>
      <td>-2.720482e-15</td>
      <td>-1.134801e-15</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 73 columns</p>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>lambda = 100</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_xxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxxx(x, t)**2</th>
      <th>h_x(x, t)**2*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)**2</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.000002</td>
      <td>0.000008</td>
      <td>-8.488413e-08</td>
      <td>-0.000036</td>
      <td>-0.000005</td>
      <td>2.101204e-07</td>
      <td>-0.000021</td>
      <td>0.000994</td>
      <td>0.000005</td>
      <td>-0.000006</td>
      <td>...</td>
      <td>-4.268632e-11</td>
      <td>5.269517e-09</td>
      <td>-4.910490e-10</td>
      <td>-1.314523e-11</td>
      <td>-7.742993e-13</td>
      <td>-1.331652e-10</td>
      <td>2.042833e-11</td>
      <td>-5.987478e-13</td>
      <td>2.408342e-15</td>
      <td>-1.135245e-15</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 74 columns</p>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>lambda = 10</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)*h_xx(x, t)*h_xxx(x, t)</th>
      <th>h(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxx(x, t)**2</th>
      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h(x, t)*h_xxxx(x, t)**2</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.000078</td>
      <td>0.000022</td>
      <td>-0.00003</td>
      <td>0.000009</td>
      <td>-3.621464e-08</td>
      <td>-0.000142</td>
      <td>0.000003</td>
      <td>-0.000044</td>
      <td>-0.000006</td>
      <td>2.333756e-07</td>
      <td>...</td>
      <td>-5.158209e-11</td>
      <td>5.223241e-09</td>
      <td>-5.293248e-10</td>
      <td>-6.898464e-12</td>
      <td>-7.832659e-13</td>
      <td>-9.675606e-11</td>
      <td>2.194779e-11</td>
      <td>-3.257965e-13</td>
      <td>9.419454e-15</td>
      <td>-1.060554e-15</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 87 columns</p>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>lambda = 2</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h(x, t)*h_xxxx(x, t)</th>
      <th>h_x(x, t)*h_xxx(x, t)</th>
      <th>h_x(x, t)*h_xxxx(x, t)</th>
      <th>h_xx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)</th>
      <th>h_xx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)</th>
      <th>h_xxxx(x, t)**2</th>
      <th>h(x, t)**2*h_xxxx(x, t)</th>
      <th>...</th>
      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxx(x, t)**3</th>
      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>
      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>
      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxx(x, t)**4</th>
      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>
      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>
      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>
      <th>h_xxxx(x, t)**4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.000016</td>
      <td>0.000541</td>
      <td>-0.00018</td>
      <td>-0.000066</td>
      <td>0.000133</td>
      <td>0.000032</td>
      <td>-0.000033</td>
      <td>0.000008</td>
      <td>-1.995623e-08</td>
      <td>-0.000084</td>
      <td>...</td>
      <td>-6.687787e-11</td>
      <td>4.931579e-09</td>
      <td>-4.735351e-10</td>
      <td>-7.393932e-12</td>
      <td>-6.988554e-13</td>
      <td>-7.077843e-11</td>
      <td>1.972181e-11</td>
      <td>-3.251633e-14</td>
      <td>7.291433e-15</td>
      <td>-8.183021e-16</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 97 columns</p>
</div>
</div>
</div>
<p>Again, although this gives us a sense of sparsity, it also doesn’t seem to capture the solution well.</p>
</section>
<section id="greedy-forward-selection" class="level4">
<h4 class="anchored" data-anchor-id="greedy-forward-selection">Greedy forward selection</h4>
<p>Let’s instead try a greedy method for our system that will inform which terms should be included. To do so, we will use a generic <code>scikit-learn</code> interface called <code>SequentialFeatureSelector</code> as well as the <span class="math inline">\(R^2\)</span> coefficient of determination <code>r2_score</code> to select terms one by one that best “explain the variance” in the time evolution <span class="math inline">\(h_t(x,t)\)</span>. As the terms are selected, we will compute the coefficients of the small libraries via ordinary least squares:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.feature_selection <span class="im">as</span> fs</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_r2_select(A,b,num_terms<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,num_terms<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>        sfs <span class="op">=</span> fs.SequentialFeatureSelector(</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>            lm.LinearRegression(),</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>            n_features_to_select<span class="op">=</span>i,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>            scoring<span class="op">=</span>met.make_scorer(met.r2_score)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        new_A <span class="op">=</span> sfs.fit_transform(A,b)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        new_ols <span class="op">=</span> sfs.estimator</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        new_ols.fit(new_A,b)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        Rsquare <span class="op">=</span> met.r2_score(new_ols.predict(new_A),b)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        feat_names <span class="op">=</span> sfs.get_feature_names_out(A.columns)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"R^2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(Rsquare))</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        temp_results <span class="op">=</span> pd.DataFrame(</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>[new_ols.coef_],</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>            columns<span class="op">=</span>feat_names,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>            index<span class="op">=</span>[<span class="st">"Coefficients"</span>]</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        display(temp_results)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>forward_r2_select(term_matrix, h_t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9999009929293052</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>0.9948</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9999137793593055</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.000245</td>
      <td>-0.004588</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9999166091843339</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)**2*h_xxx(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.000994</td>
      <td>-0.003494</td>
      <td>0.000001</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9999179601805686</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)**2*h_xxx(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.001286</td>
      <td>-0.003129</td>
      <td>0.000001</td>
      <td>1.281114e-08</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>This seems to easily pick up that the only term needed to completely resolve the time evolution is <span class="math inline">\(h_x(x,t)\)</span>!</p>
</section>
</section>
</section>
<section id="sec-noisy" class="level2">
<h2 class="anchored" data-anchor-id="sec-noisy">Application to noisy simulated wave data</h2>
<p>In a real system, we could not expect to immediately have data as smooth as that we used in <a href="#sec-simulated">Section&nbsp;3</a>. However, the procedure is unchanged. The only challenge will be fitting the neural network to our data. Let’s add some noise to the data:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>x,t,noisy_h <span class="op">=</span> load_data(<span class="st">"simple_wave.npz"</span>,<span class="fl">.2</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>animation2 <span class="op">=</span> animate_data(x,t,[noisy_h], [<span class="st">"h noisy"</span>])</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>animation2.save(<span class="st">"noisy_h.gif"</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="noisy_h.gif" class="img-fluid"></p>
<p>Given our data is now noisy, we may want to implement a train-validation-test method for fitting. Simply put, this means that we will hold out a portion of our data from the training procedure. Part of this held-back data (validation set) will be used to validate that our model can generalize to other points during training. The other part of the held-back data (test set) will be used as a final check on how well the model extrapolates out of the training data.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.model_selection <span class="im">as</span> ms</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>X,T <span class="op">=</span> jnp.meshgrid(x,t)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>xt_noisy <span class="op">=</span> np.vstack((X.flatten(),T.flatten())).T</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>h_noisy <span class="op">=</span> noisy_h.flatten()</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>xt_train, xt_test, h_train, h_test <span class="op">=</span> ms.train_test_split(xt_noisy,h_noisy,test_size<span class="op">=</span><span class="fl">.1</span>,train_size<span class="op">=</span><span class="fl">.9</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>xt_train, xt_valid, h_train, h_valid <span class="op">=</span> ms.train_test_split(xt_train,h_train,test_size<span class="op">=</span><span class="fl">.1</span>,train_size<span class="op">=</span><span class="fl">.9</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> batch_data(xt_train[:,<span class="dv">0</span>], xt_train[:,<span class="dv">1</span>], h_train, <span class="dv">1000</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>valid_data <span class="op">=</span> batch_data(xt_valid[:,<span class="dv">0</span>], xt_valid[:,<span class="dv">1</span>], h_valid, <span class="dv">1000</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> batch_data(xt_test[:,<span class="dv">0</span>], xt_test[:,<span class="dv">1</span>], h_test, <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we apply our previous model construction and training:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>rng1,rng2 <span class="op">=</span> jax.random.split(jax.random.PRNGKey(<span class="dv">42</span>))</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>random_data <span class="op">=</span> jax.random.normal(rng1,(<span class="dv">2</span>,))</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> MyNet()</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>params2 <span class="op">=</span> model2.init(rng2,random_data)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(params,<span class="bu">input</span>,targets):</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> squared_error(x,y):</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model2.<span class="bu">apply</span>(params,x)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.mean((y <span class="op">-</span> pred)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.mean(jax.vmap(squared_error)(<span class="bu">input</span>,targets),axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>loss_grad_fn <span class="op">=</span> jax.value_and_grad(mse)</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>tx <span class="op">=</span> optax.adam(learning_rate)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> tx.init(params2)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Training (adjusted to use our validation data</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1200</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>    xt_batch <span class="op">=</span> train_data[i<span class="op">%</span><span class="bu">len</span>(train_data)][<span class="dv">0</span>]</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>    h_batch <span class="op">=</span> train_data[i<span class="op">%</span><span class="bu">len</span>(train_data)][<span class="dv">1</span>]</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    loss_val, grads <span class="op">=</span> loss_grad_fn(params2, xt_batch, h_batch)</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    updates, opt_state <span class="op">=</span> tx.update(grads, opt_state)</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    params2 <span class="op">=</span> optax.apply_updates(params2, updates)</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> mse(params2,xt_train,h_train)</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> mse(params2,xt_valid,h_valid)</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Step </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i))</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training loss: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(train_loss))</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Validation loss: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(valid_loss))</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> mse(params2,xt_test,h_test)</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test loss after training: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(test_loss))</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>hhat2 <span class="op">=</span> model2.<span class="bu">apply</span>(params2,xt_points).reshape(X.shape)</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.sqrt((noisy_h <span class="op">-</span> hhat2)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>diff2 <span class="op">=</span> np.sqrt((hhat1 <span class="op">-</span> hhat2)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>animation3 <span class="op">=</span> animate_data(x,t,[noisy_h,hhat2,diff],[<span class="st">"$h$"</span>,<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$"</span>,<span class="st">"$L^2$ error"</span>])</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>animation3.save(<span class="st">"noisy_h_compare.gif"</span>)</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>animation3 <span class="op">=</span> animate_data(x,t,[hhat1,hhat2,diff2],[<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$ clean"</span>,<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$ noisy"</span>,<span class="st">"$L^2$ error"</span>])</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>animation3.save(<span class="st">"noisy_hhat_compare.gif"</span>)</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 0
Training loss: 1.096426248550415
Validation loss: 1.0614256858825684
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 100
Training loss: 0.24856211245059967
Validation loss: 0.2242167741060257
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 200
Training loss: 0.1528051644563675
Validation loss: 0.13052992522716522
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 300
Training loss: 0.11249998211860657
Validation loss: 0.09524193406105042
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 400
Training loss: 0.06492499262094498
Validation loss: 0.05927733704447746
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 500
Training loss: 0.0453445240855217
Validation loss: 0.043655507266521454
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 600
Training loss: 0.041188061237335205
Validation loss: 0.04045151174068451
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 700
Training loss: 0.04039134457707405
Validation loss: 0.04004029184579849
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 800
Training loss: 0.04142721742391586
Validation loss: 0.041196469217538834
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 900
Training loss: 0.041981782764196396
Validation loss: 0.041676320135593414
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 1000
Training loss: 0.041559334844350815
Validation loss: 0.04100088030099869
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 1100
Training loss: 0.039972953498363495
Validation loss: 0.03931911289691925
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test loss after training: 0.03955989331007004</code></pre>
</div>
</div>
<p>The resulting fit can be seen in the following video:</p>
<p><img src="noisy_h_compare.gif" class="img-fluid"></p>
<p>Looks pretty good all things considered! We can also compare this with the fit on clean data to see how impressive the robustness to noise was:</p>
<p><img src="noisy_hhat_compare.gif" class="img-fluid"></p>
<p>Finally, we construct the terms and check the results after forward selection:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_for_diff(x,t):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    new_x <span class="op">=</span> jnp.array([x,t])</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model2.<span class="bu">apply</span>(params2, new_x)[<span class="dv">0</span>]</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct terms numerically</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>diff_term_values <span class="op">=</span> {}</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_diff_order<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    diff_func <span class="op">=</span> model_for_diff</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteratively apply derivatives</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(i):</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        diff_func <span class="op">=</span> jax.grad(diff_func, <span class="dv">0</span>)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        new_x,new_t <span class="op">=</span> x</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> diff_func(new_x,new_t)</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    diff_term_values[diff_terms[i]] <span class="op">=</span> np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_points))</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>term_values <span class="op">=</span> construct_terms(diff_term_values)</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    new_x,new_t <span class="op">=</span> x</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.grad(model_for_diff,<span class="dv">1</span>)(new_x,new_t)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>h_t_term <span class="op">=</span> sp.Function(<span class="st">"h_t"</span>)(x_sym,t_sym)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>h_t <span class="op">=</span> <span class="op">-</span>np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_points))</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward selection</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>term_matrix <span class="op">=</span> pd.DataFrame(term_values,index<span class="op">=</span>pd.MultiIndex.from_arrays(np.<span class="bu">round</span>(np.array(xt_points),<span class="dv">2</span>).T, names<span class="op">=</span>(<span class="st">"x"</span>,<span class="st">"t"</span>)))</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>forward_r2_select(term_matrix, h_t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9993203642386503</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>0.995716</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9993922570879483</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.008756</td>
      <td>-0.010986</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9993989412117996</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)**2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.008356</td>
      <td>-0.010895</td>
      <td>0.000432</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9994035211848762</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h_xxx(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)**2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>1.008077</td>
      <td>0.000086</td>
      <td>-0.008805</td>
      <td>0.000495</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Boom! Landed right on the money. This is a simple example with a straightforward answer, but example holds to show the overall procedure for handling data with additive noise (multiplicative noise, which is more structural, would be an altogether different challenge).</p>
</section>
<section id="sec-extracted" class="level2">
<h2 class="anchored" data-anchor-id="sec-extracted">Application to extracted wave data</h2>
<p>Now, applying this procedure to real data is as simple as replacing our original dataset with an experimental dataset. However, the extraction process has a strong influence on the quality of the data that we will be using, so it deserves to be treated with some detail.</p>
<section id="image-data-extraction" class="level3">
<h3 class="anchored" data-anchor-id="image-data-extraction">Image data extraction</h3>
<p>The original video we will be using can be found on YouTube <a href="https://www.youtube.com/watch?v=wEbYELtGZwI">here</a>.</p>
<p><video src="youtube_video.mp4" class="img-fluid" controls=""><a href="youtube_video.mp4">Video</a></video></p>
<p>We can load this video into individual image frames via:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> skimage <span class="im">as</span> img</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> imageio.v3 <span class="im">as</span> iio</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>raw_frames <span class="op">=</span> []</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> (<span class="dv">160</span>,<span class="dv">200</span>)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>,<span class="dv">232</span>):</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> iio.imread(<span class="st">"youtube_video.mp4"</span>,plugin<span class="op">=</span><span class="st">"pyav"</span>,index<span class="op">=</span>i)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cut the image to focus only on the wave portion</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    raw_frame <span class="op">=</span> frame[cut[<span class="dv">0</span>]:cut[<span class="dv">1</span>],:,:]</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    raw_frames.append(raw_frame)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>raw_frames <span class="op">=</span> np.array(raw_frames)</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>plt.imshow(raw_frames[<span class="dv">16</span>])</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="workshop3_discovery_files/figure-html/cell-26-output-1.png" width="614" height="72"></p>
</div>
</div>
<p>We then need to remove the background and isolate the wave portion of the image, which is facilitated by the green color of the water in this video:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>frames <span class="op">=</span> []</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(raw_frames)):</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> raw_frames[i]</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find where the image is more green than red or blue and very bright green</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    mean_green <span class="op">=</span> np.mean(frame[:,:,<span class="dv">1</span>])</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    std_green <span class="op">=</span> np.std(frame[:,:,<span class="dv">1</span>])</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> (frame[:,:,<span class="dv">1</span>] <span class="op">&gt;</span> frame[:,:,<span class="dv">0</span>]) <span class="op">&amp;</span> (frame[:,:,<span class="dv">1</span>] <span class="op">&gt;</span> frame[:,:,<span class="dv">2</span>]) <span class="op">&amp;</span> (frame[:,:,<span class="dv">1</span>] <span class="op">&gt;</span> mean_green<span class="op">+</span>std_green)</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    frames.append(frame)</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>frames <span class="op">=</span> np.array(frames)</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(frames[<span class="dv">16</span>],cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="workshop3_discovery_files/figure-html/cell-27-output-1.png" width="614" height="72"></p>
</div>
</div>
<p>By averaging these pixels across all vertical pixels in the image, we can get a rough wave outline:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>heights <span class="op">=</span> []</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(frames)):</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> frames[i]</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Approximate wave height by averaging y-locations of bright green areas</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> np.zeros(frame.shape[<span class="dv">1</span>])</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(frame.shape[<span class="dv">1</span>]):</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        height[j] <span class="op">=</span> np.mean(np.where(frame[:,j] <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>])</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    heights.append(height)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>heights <span class="op">=</span> np.array(heights)</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> heights[<span class="dv">16</span>, <span class="dv">0</span>]</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>plt.imshow(frames[<span class="dv">16</span>],cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>line <span class="op">=</span> plt.plot(heights[<span class="dv">16</span>], color<span class="op">=</span><span class="st">"red"</span>,lw<span class="op">=</span><span class="dv">3</span>)[<span class="dv">0</span>]</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>line2 <span class="op">=</span> plt.plot([<span class="dv">0</span>,heights.shape[<span class="dv">1</span>]], [<span class="dv">31</span>,<span class="dv">31</span>], color<span class="op">=</span><span class="st">"orange"</span>, ls<span class="op">=</span><span class="st">"--"</span>)[<span class="dv">0</span>]</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="workshop3_discovery_files/figure-html/cell-28-output-1.png" width="614" height="70"></p>
</div>
</div>
<p>Finally, we can note that the video is not quite level to the wave surface, so we can use a linear adjustment to align the water boundary heights at the middle of the video:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust images and heights for an un-leveled camera</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>im_width <span class="op">=</span> <span class="bu">len</span>(heights[<span class="dv">16</span>])</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>slope <span class="op">=</span> (heights[<span class="dv">16</span>][<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> heights[<span class="dv">16</span>][<span class="dv">0</span>]) <span class="op">/</span> im_width</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(heights)):</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> frames[i]</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> heights[i]</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adjust</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(height)):</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        shift <span class="op">=</span> <span class="bu">int</span>(slope<span class="op">*</span>(im_width<span class="op">-</span>j))</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move frame pixels per column</span></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        frame[:,j] <span class="op">=</span> np.roll(frame[:,j], shift)</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move height of wave</span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        height[j] <span class="op">+=</span> shift</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    frames[i] <span class="op">=</span> frame</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>    heights[i] <span class="op">=</span> height</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>frames <span class="op">=</span> np.array(frames)</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>raw_frames <span class="op">=</span> np.array(raw_frames)</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>heights <span class="op">=</span> np.array(heights)</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> plt.imshow(frames[<span class="dv">0</span>],cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>line <span class="op">=</span> plt.plot(heights[<span class="dv">0</span>], color<span class="op">=</span><span class="st">"red"</span>,lw<span class="op">=</span><span class="dv">3</span>)[<span class="dv">0</span>]</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a>line2 <span class="op">=</span> plt.plot([<span class="dv">0</span>,heights.shape[<span class="dv">1</span>]], [<span class="dv">31</span>,<span class="dv">31</span>], color<span class="op">=</span><span class="st">"orange"</span>, ls<span class="op">=</span><span class="st">"--"</span>)[<span class="dv">0</span>]</span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> animation_function(i):</span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a>    im.set_array(frames[i])</span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a>    line.set_ydata(heights[i])</span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [im,line,line2]</span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a>wave_animation <span class="op">=</span> anim.FuncAnimation(fig, animation_function, frames<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(frames)), blit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a>wave_animation.save(<span class="st">"extracted_wave.gif"</span>)</span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="extracted_wave.gif" class="img-fluid"></p>
<p>We can now save this data to be used with our previous procedure:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Video portion is about 2 seconds long</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">2</span>,<span class="bu">len</span>(heights))</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="co"># No given space scale</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>x_domain <span class="op">=</span> np.arange(<span class="bu">len</span>(heights[<span class="dv">0</span>]))</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"video_wave_images.npy"</span>,raw_frames)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>np.savez(<span class="st">"video_wave_heights.npz"</span>,h<span class="op">=</span>heights,x<span class="op">=</span>x_domain,t<span class="op">=</span>times)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-our-experimental-dataset" class="level3">
<h3 class="anchored" data-anchor-id="using-our-experimental-dataset">Using our experimental dataset</h3>
<p>Using the same methods as listed in <a href="#sec-noisy">Section&nbsp;4</a>, we can discover an equation for this particular dataset:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>x,t,ext_h <span class="op">=</span> load_data(<span class="st">"video_wave_heights.npz"</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Flip image wave to be more familiar</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>ext_h <span class="op">=</span> <span class="op">-</span>ext_h</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>animation2 <span class="op">=</span> animate_data(x,t,[ext_h], [<span class="st">"extracted h"</span>])</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>animation2.save(<span class="st">"extracted_h.gif"</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="extracted_h.gif" class="img-fluid"></p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting data</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>X,T <span class="op">=</span> jnp.meshgrid(x,t)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>xt_ext <span class="op">=</span> np.vstack((X.flatten(),T.flatten())).T</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>h_ext <span class="op">=</span> ext_h.flatten()</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>xt_train, xt_test, h_train, h_test <span class="op">=</span> ms.train_test_split(xt_ext,h_ext,test_size<span class="op">=</span><span class="fl">.1</span>,train_size<span class="op">=</span><span class="fl">.9</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>xt_train, xt_valid, h_train, h_valid <span class="op">=</span> ms.train_test_split(xt_train,h_train,test_size<span class="op">=</span><span class="fl">.1</span>,train_size<span class="op">=</span><span class="fl">.9</span>)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> batch_data(xt_train[:,<span class="dv">0</span>], xt_train[:,<span class="dv">1</span>], h_train, <span class="dv">1000</span>)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>valid_data <span class="op">=</span> batch_data(xt_valid[:,<span class="dv">0</span>], xt_valid[:,<span class="dv">1</span>], h_valid, <span class="dv">1000</span>)</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> batch_data(xt_test[:,<span class="dv">0</span>], xt_test[:,<span class="dv">1</span>], h_test, <span class="dv">1000</span>)</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>rng1,rng2 <span class="op">=</span> jax.random.split(jax.random.PRNGKey(<span class="dv">42</span>))</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>random_data <span class="op">=</span> jax.random.normal(rng1,(<span class="dv">2</span>,))</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> MyNet()</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>params3 <span class="op">=</span> model3.init(rng2,random_data)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function</span></span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(params,<span class="bu">input</span>,targets):</span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> squared_error(x,y):</span>
<span id="cb75-22"><a href="#cb75-22" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model3.<span class="bu">apply</span>(params,x)</span>
<span id="cb75-23"><a href="#cb75-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.mean((y <span class="op">-</span> pred)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb75-24"><a href="#cb75-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.mean(jax.vmap(squared_error)(<span class="bu">input</span>,targets),axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb75-25"><a href="#cb75-25" aria-hidden="true" tabindex="-1"></a>loss_grad_fn <span class="op">=</span> jax.value_and_grad(mse)</span>
<span id="cb75-26"><a href="#cb75-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-27"><a href="#cb75-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer</span></span>
<span id="cb75-28"><a href="#cb75-28" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb75-29"><a href="#cb75-29" aria-hidden="true" tabindex="-1"></a>tx <span class="op">=</span> optax.adam(learning_rate)</span>
<span id="cb75-30"><a href="#cb75-30" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> tx.init(params3)</span>
<span id="cb75-31"><a href="#cb75-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-32"><a href="#cb75-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Training (adjusted to use our validation data</span></span>
<span id="cb75-33"><a href="#cb75-33" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1200</span></span>
<span id="cb75-34"><a href="#cb75-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb75-35"><a href="#cb75-35" aria-hidden="true" tabindex="-1"></a>    xt_batch <span class="op">=</span> train_data[i<span class="op">%</span><span class="bu">len</span>(train_data)][<span class="dv">0</span>]</span>
<span id="cb75-36"><a href="#cb75-36" aria-hidden="true" tabindex="-1"></a>    h_batch <span class="op">=</span> train_data[i<span class="op">%</span><span class="bu">len</span>(train_data)][<span class="dv">1</span>]</span>
<span id="cb75-37"><a href="#cb75-37" aria-hidden="true" tabindex="-1"></a>    loss_val, grads <span class="op">=</span> loss_grad_fn(params3, xt_batch, h_batch)</span>
<span id="cb75-38"><a href="#cb75-38" aria-hidden="true" tabindex="-1"></a>    updates, opt_state <span class="op">=</span> tx.update(grads, opt_state)</span>
<span id="cb75-39"><a href="#cb75-39" aria-hidden="true" tabindex="-1"></a>    params3 <span class="op">=</span> optax.apply_updates(params3, updates)</span>
<span id="cb75-40"><a href="#cb75-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb75-41"><a href="#cb75-41" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> mse(params3,xt_train,h_train)</span>
<span id="cb75-42"><a href="#cb75-42" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> mse(params3,xt_valid,h_valid)</span>
<span id="cb75-43"><a href="#cb75-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Step </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i))</span>
<span id="cb75-44"><a href="#cb75-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training loss: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(train_loss))</span>
<span id="cb75-45"><a href="#cb75-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Validation loss: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(valid_loss))</span>
<span id="cb75-46"><a href="#cb75-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb75-47"><a href="#cb75-47" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> mse(params3,xt_test,h_test)</span>
<span id="cb75-48"><a href="#cb75-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test loss after training: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(test_loss))</span>
<span id="cb75-49"><a href="#cb75-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-50"><a href="#cb75-50" aria-hidden="true" tabindex="-1"></a>hhat <span class="op">=</span> model3.<span class="bu">apply</span>(params3,xt_ext).reshape(X.shape)</span>
<span id="cb75-51"><a href="#cb75-51" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.sqrt((ext_h <span class="op">-</span> hhat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb75-52"><a href="#cb75-52" aria-hidden="true" tabindex="-1"></a>animation3 <span class="op">=</span> animate_data(x,t,[ext_h,hhat,diff],[<span class="st">"$extracted h$"</span>,<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$"</span>,<span class="st">"$L^2$ error"</span>])</span>
<span id="cb75-53"><a href="#cb75-53" aria-hidden="true" tabindex="-1"></a>animation3.save(<span class="st">"ext_h_compare.gif"</span>)</span>
<span id="cb75-54"><a href="#cb75-54" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 0
Training loss: 1.048029899597168
Validation loss: 1.084288477897644
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 100
Training loss: 0.019095050171017647
Validation loss: 0.018683746457099915
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 200
Training loss: 0.01550883986055851
Validation loss: 0.01508625689893961
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 300
Training loss: 0.015007833018898964
Validation loss: 0.014961878769099712
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 400
Training loss: 0.014233827590942383
Validation loss: 0.013819326646625996
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 500
Training loss: 0.013691544532775879
Validation loss: 0.013394343666732311
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 600
Training loss: 0.013891058973968029
Validation loss: 0.014142909087240696
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 700
Training loss: 0.013472299091517925
Validation loss: 0.01331472396850586
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 800
Training loss: 0.013176359236240387
Validation loss: 0.013051674701273441
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 900
Training loss: 0.01349588856101036
Validation loss: 0.013870802707970142
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 1000
Training loss: 0.01288254652172327
Validation loss: 0.012871635146439075
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 1100
Training loss: 0.01262914389371872
Validation loss: 0.012560416013002396
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test loss after training: 0.01305135153234005</code></pre>
</div>
</div>
<p><img src="ext_h_compare.gif" class="img-fluid"></p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_for_diff(x,t):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    new_x <span class="op">=</span> jnp.array([x,t])</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model3.<span class="bu">apply</span>(params3, new_x)[<span class="dv">0</span>]</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct terms numerically</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>diff_term_values <span class="op">=</span> {}</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_diff_order<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    diff_func <span class="op">=</span> model_for_diff</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteratively apply derivatives</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(i):</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>        diff_func <span class="op">=</span> jax.grad(diff_func, <span class="dv">0</span>)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>        new_x,new_t <span class="op">=</span> x</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> diff_func(new_x,new_t)</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>    diff_term_values[diff_terms[i]] <span class="op">=</span> np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_ext))</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>term_values <span class="op">=</span> construct_terms(diff_term_values)</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unpack_diff_func(x):</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>    new_x,new_t <span class="op">=</span> x</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.grad(model_for_diff,<span class="dv">1</span>)(new_x,new_t)</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>h_t_term <span class="op">=</span> sp.Function(<span class="st">"h_t"</span>)(x_sym,t_sym)</span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>h_t <span class="op">=</span> <span class="op">-</span>np.array(jax.lax.<span class="bu">map</span>(unpack_diff_func, xt_ext))</span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward selection</span></span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>term_matrix <span class="op">=</span> pd.DataFrame(term_values,index<span class="op">=</span>pd.MultiIndex.from_arrays(np.<span class="bu">round</span>(np.array(xt_ext),<span class="dv">2</span>).T, names<span class="op">=</span>(<span class="st">"x"</span>,<span class="st">"t"</span>)))</span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>forward_r2_select(term_matrix, h_t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9904072946412229</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.966266</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9904397777920828</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.96636</td>
      <td>-0.000248</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.990663670137724</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.929208</td>
      <td>-0.028443</td>
      <td>-0.001005</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2: 0.9910591172160474</code></pre>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>h_x(x, t)</th>
      <th>h(x, t)*h_x(x, t)</th>
      <th>h_x(x, t)*h_xx(x, t)</th>
      <th>h(x, t)*h_x(x, t)*h_xx(x, t)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coefficients</th>
      <td>-0.87648</td>
      <td>-0.049882</td>
      <td>-0.005243</td>
      <td>0.00193</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Feel free to play with the parameters of each step to try to change/improve the results we have seen here.</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="appendix" class="level2 appendix"><h2 class="quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">



</div></section><section id="the-benefits-of-jax" class="level3 appendix"><h2 class="quarto-appendix-heading">The benefits of JAX</h2><div class="quarto-appendix-contents">

<p><a href="https://jax.readthedocs.io/en/latest/index.html"><code>jax</code></a> is an automatic differentiation based on the <a href="https://www.tensorflow.org/xla">XLA</a> compiler for Tensorflow. The largest difference between this library and the alternative libraries (like those included in <code>tensorflow</code> main, <code>pytorch</code>, <code>keras</code>, etc.) is that it compiles Python code down to a computational graph structure. Although the majority of excitement around this compiler has surrounded the optimizations that can take place one the graph structure has been identified, it also facilitates taking derivatives of arbitrarry objects. This is because rather than compute gradients along the path (forward mode automatic differentiation) or keeping track of operations as it goes (backward mode automatic differentiation), it has a graph structure to analyze exactly what happens to each value and parameter. At the end of the day, this means it is much easier to compute gradients of exactly what you want.</p>
<p>If <code>jax</code> and <code>flax</code> have appeared too hands-on and complicated after this workshop, consider trying <a href="https://cgarciae.github.io/treex/"><code>treex</code></a> which aims to make using <code>jax</code> for neural networks simple and only need a few lines of code.</p>
</div></section><section id="training-without-normalizing-the-data" class="level3 appendix"><h2 class="quarto-appendix-heading">Training without normalizing the data</h2><div class="quarto-appendix-contents">

<p>In the <a href="#sec-simulated">Section&nbsp;3</a> section, the <code>load_data</code> function performs a normalization of the simulated data from a range of <span class="math inline">\(h(x,t) \in [0,.1]\)</span> where <span class="math inline">\(x \in [0,1]\)</span> and <span class="math inline">\(t \in [0,1]\)</span> to a range of <span class="math inline">\(h(x,t) \in [-1,1]\)</span> with mean <span class="math inline">\(\bar{h}=0\)</span> and standard deviation 1 with <span class="math inline">\(x \in [-1.7,1.7]\)</span> and <span class="math inline">\(t \in [-1.7,1.7]\)</span>. Normalizing data like this is common in machine learning, but it is not always apparent why. Our case can give a strong demonstration as to the benefits of normalizing in this way.</p>
<p>Consider that our neural network uses only the <span class="math inline">\(\tanh\)</span> activation function. For those unfamiliar, this function has the form:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>tanh_dom <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">100</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>tanh_range <span class="op">=</span> np.tanh(tanh_dom)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>plt.plot(tanh_dom, tanh_range, label<span class="op">=</span><span class="st">"$</span><span class="ch">\\</span><span class="st">tanh(x)$"</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)<span class="op">;</span> plt.legend()<span class="op">;</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="workshop3_discovery_files/figure-html/cell-34-output-1.png" width="590" height="429"></p>
</div>
</div>
<p>Each layer of our neural network is connected via linear transformations of the form <span class="math inline">\(\vec{x}^T\mathbf{W} + \vec{b}\)</span>, thus we should be able to shift the data into the appropriate domain and range for the <span class="math inline">\(\tanh\)</span> function. However, in practice, optimizing our parameters to attain this is hard to find. To demonstrate, consider the training of the neural network on clean simulation data without normalization:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>x,t,h <span class="op">=</span> load_data(<span class="st">"simple_wave.npz"</span>,norm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>X,T <span class="op">=</span> jnp.meshgrid(x,t)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> batch_data(X.flatten(),T.flatten(),h.flatten(),<span class="dv">10000</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Random generator seed</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>rng1,rng2 <span class="op">=</span> jax.random.split(jax.random.PRNGKey(<span class="dv">42</span>))</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>random_data <span class="op">=</span> jax.random.normal(rng1,(<span class="dv">2</span>,))</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>model4 <span class="op">=</span> MyNet()</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>params4 <span class="op">=</span> model1.init(rng2,random_data)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(params,<span class="bu">input</span>,targets):</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> squared_error(x,y):</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model4.<span class="bu">apply</span>(params,x)</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.mean((y <span class="op">-</span> pred)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.mean(jax.vmap(squared_error)(<span class="bu">input</span>,targets),axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>loss_grad_fn <span class="op">=</span> jax.value_and_grad(mse)</span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>tx <span class="op">=</span> optax.adam(learning_rate)</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> tx.init(params4)</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a>all_xt <span class="op">=</span> jnp.array([data[i][<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))])</span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>all_h <span class="op">=</span> jnp.array([data[i][<span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))])</span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>    xt_batch <span class="op">=</span> data[i<span class="op">%</span><span class="bu">len</span>(data)][<span class="dv">0</span>]</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>    h_batch <span class="op">=</span> data[i<span class="op">%</span><span class="bu">len</span>(data)][<span class="dv">1</span>]</span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a>    loss_val, grads <span class="op">=</span> loss_grad_fn(params4, xt_batch, h_batch)</span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>    updates, opt_state <span class="op">=</span> tx.update(grads, opt_state)</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>    params4 <span class="op">=</span> optax.apply_updates(params4, updates)</span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> mse(params4,all_xt,all_h)</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training loss step </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i,train_loss))</span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>xt_points <span class="op">=</span> jnp.vstack([X.flatten(),T.flatten()]).T</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>hhat4 <span class="op">=</span> model1.<span class="bu">apply</span>(params4,xt_points).reshape(X.shape)</span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> np.sqrt((h <span class="op">-</span> hhat4)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>animation4 <span class="op">=</span> animate_data(x,t,[h,hhat4,diff],[<span class="st">"$h$"</span>,<span class="st">"$\hat</span><span class="sc">{h}</span><span class="st">$"</span>,<span class="st">"$L^2$ error"</span>])</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>animation4.save(<span class="st">"nonorm_h_compare.gif"</span>)</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 0: 0.3563382923603058</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 100: 0.0009187604882754385</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 200: 0.0008465293794870377</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 300: 0.0007735078688710928</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 400: 0.0006919147563166916</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 500: 0.0006207432597875595</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 600: 0.0005757116014137864</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 700: 0.0005529412883333862</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 800: 0.0005375402979552746</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training loss step 900: 0.0005216024001128972</code></pre>
</div>
</div>
<p><img src="nonorm_h_compare.gif" class="img-fluid"></p>
<p>The fit is terrible! We have apparently fallen into a local minimum far from the global minimum we would like to find. This demonstrates two important ideas relating to neural networks (validated by experience):</p>
<ol type="1">
<li>They are fickle and in some cases small changes to data, architecture, and training, can dramatically change results</li>
<li>Any help that can be given to the neural network via knowledge of the system or data can help. In this case, adjusting for the gap between the range of our data and that of the activation function was sufficient.</li>
</ol>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-brunton2016discovering" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">S. L. Brunton, J. L. Proctor, and J. N. Kutz, <em>Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems</em>, Proceedings of the National Academy of Sciences <strong>113</strong>, 3932 (2016).</div>
</div>
<div id="ref-williams2015data" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">M. O. Williams, I. G. Kevrekidis, and C. W. Rowley, <em>A Data–Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition</em>, Journal of Nonlinear Science <strong>25</strong>, 1307 (2015).</div>
</div>
<div id="ref-cleveland1988locally" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">W. S. Cleveland and S. J. Devlin, <em>Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting</em>, Journal of the American Statistical Association <strong>83</strong>, 596 (1988).</div>
</div>
<div id="ref-press1990savitzky" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">W. H. Press and S. A. Teukolsky, <em>Savitzky-Golay Smoothing Filters</em>, Computers in Physics <strong>4</strong>, 669 (1990).</div>
</div>
<div id="ref-zheng2018unified" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">P. Zheng, T. Askham, S. L. Brunton, J. N. Kutz, and A. Y. Aravkin, <em>A Unified Framework for Sparse Relaxed Regularized Regression: SR3</em>, IEEE Access <strong>7</strong>, 1404 (2018).</div>
</div>
<div id="ref-rudy2017data" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz, <em>Data-Driven Discovery of Partial Differential Equations</em>, Science Advances <strong>3</strong>, e1602614 (2017).</div>
</div>
<div id="ref-xu2019dl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">H. Xu, H. Chang, and D. Zhang, <em>Dl-Pde: Deep-Learning Based Data-Driven Discovery of Partial Differential Equations from Discrete and Noisy Data</em>, arXiv Preprint arXiv:1908.04463 (2019).</div>
</div>
</div></section></div></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"026e1da955724f31b6258ea93ff878d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fd866996e8c4ca497de363cd07bc88a","IPY_MODEL_c26d9692ebd34b7981112cf058a27ca4","IPY_MODEL_6a221cfccbda40fd9ba20a144d7b307e"],"layout":"IPY_MODEL_d40ebda1665b4d50a07ba9cc1ef6c8c0","tabbable":null,"tooltip":null}},"2e85e2dc36f5411e8d5f16191148f914":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b3e0d75b2e445da87ce1438dacc10d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6a221cfccbda40fd9ba20a144d7b307e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b4ff327e237949b9a9d07d0963df7dc4","placeholder":"​","style":"IPY_MODEL_69b3e0d75b2e445da87ce1438dacc10d","tabbable":null,"tooltip":null,"value":" 100/100 [00:01&lt;00:00, 72.60it/s]"}},"7065117b0a5d41dca6aa31c80749cb7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fd866996e8c4ca497de363cd07bc88a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2e85e2dc36f5411e8d5f16191148f914","placeholder":"​","style":"IPY_MODEL_aa5c556126fb4d1c8c08dd9a05dacb78","tabbable":null,"tooltip":null,"value":"100%"}},"9cf843f89f6f4b57979a33e1edfe1856":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa5c556126fb4d1c8c08dd9a05dacb78":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b4ff327e237949b9a9d07d0963df7dc4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c26d9692ebd34b7981112cf058a27ca4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_9cf843f89f6f4b57979a33e1edfe1856","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7065117b0a5d41dca6aa31c80749cb7f","tabbable":null,"tooltip":null,"value":100}},"d40ebda1665b4d50a07ba9cc1ef6c8c0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/connor-robertson-773ba8b1/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cnrrobertson">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-right">Connor Robertson, Copyright 2022</div>
  </div>
</footer>



</body></html>