{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation\"\n",
        "csl: ../../american-physics-society.csl\n",
        "bibliography: ../../workshops.bib\n",
        "author: Connor Robertson\n",
        "execute:\n",
        "    daemon: 500\n",
        "    keep-ipynb: true\n",
        "---"
      ],
      "id": "350fd093"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "One common hallmark of popular machine learning methods is their \"black-box\" nature.\n",
        "Since many of these methods are meant solely for prediction, this has not been too much of an issue.\n",
        "After all, a black box method can be as complex as needed since it does not need to be analyzed after the fact.\n",
        "This mentality has given birth to increasingly complex but effective models (just take a look at [the model](https://nikcheerla.github.io/deeplearningschool//media/alphago_arch.png) that defeated the worlds best Go player).\n",
        "\n",
        "However, there has been some recent interest in models that can be understood and analyzed.\n",
        "This is particularly true in the scientific realm, where practicioners looking to use machine learning would like to get an idea of the mechanisms underlying their system of study.\n",
        "In order to do so, new tools have been created and old, interpretable tools, such as linear regression, have been adapted to meet this challenge.\n",
        "\n",
        "Many of these new, interpretable, models have been named \"data-driven model discovery.\"\n",
        "Their goals is to model collected data from a system with machine learning tools to determine a human-readable model.\n",
        "\n",
        "## Sparse Identification of Nonlinear Dynamics\n",
        "One method for model discovery as described above is called Sparse Identification of Nonlinear Dynamics (SINDy)[@brunton2016discovering].\n",
        "The goal of this method is to extract the most probable differential equation directly from data of the important state variables of a continuum system.\n",
        "\n",
        "### Setting up linear problem\n",
        "As its name suggests, this method works discover models for linear or nonlinear systems.\n",
        "It is based on a simple idea that nonlinear differential equations can be expressed as a linear combination of nonlinear terms[@williams2015data].\n",
        "Assuming we are looking at the nonlinear time evolution of some quantity, this could then be written as the sum of $K$ nonlinear terms:\n",
        "$$\n",
        "u_t(x,t) = \\xi_1\\mathcal{N}_1(u,x,t) + \\ldots + \\xi_K\\mathcal{N}_K(u,x,t)\n",
        "$$\n",
        "If we can then determine what nonlinear terms are possible $\\mathcal{N}_i(u,x,t)$, we can sift through these terms to determine which best contribute to the time evolution of the system.\n",
        "\n",
        "Ultimately, this boils down to a regression problem.\n",
        "Given some space and time samples of our state variable: $u(x_i,t_j)$ for $i \\leq N$ and $j \\leq M$, we can consider the linear system:\n",
        "$$\n",
        "u_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)\n",
        "$$\n",
        "Expanded for all the data samples (flattened across space and time), this can be written as the system:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "u_t(x_1, t_1) \\\\\n",
        "\\vdots \\\\\n",
        "u_t(x_N, t_1) \\\\\n",
        "\\vdots \\\\\n",
        "u_t(x_N, t_M) \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\mathcal{N}_1(x_1, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n",
        "\\vdots &  & \\vdots \\\\\n",
        "\\mathcal{N}_1(x_N, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n",
        "\\vdots &  & \\vdots \\\\\n",
        "\\mathcal{N}_1(x_N, t_M) & \\ldots & \\mathcal{N}_K(x_1, t_1)\n",
        "\\end{bmatrix}\n",
        "\\vec{\\xi}\n",
        "$$ {#eq-linear-system}\n",
        "\n",
        "Solving this system is then a straightforward linear regression.\n",
        "\n",
        "### Determining nonlinear \"library\" of terms\n",
        "Determining what $\\mathcal{N}_i(u,x,t)$ are reasonable for the system is somewhat of a traditional modeling problem.\n",
        "Are there any symmetries in the system that need to be satisfied?\n",
        "Is there periodic behavior that might warrant inclusion of trignometric terms?\n",
        "What order of polynomial interactions are possible for the system?\n",
        "\n",
        "The most common library of terms for a 1D function is to put together polynomial interactions with spatial derivatives.\n",
        "Such a library up to 3rd order polynomials and derivatives could be written:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{N}_1(u,x,t) &= u\\\\\n",
        "\\mathcal{N}_2(u,x,t) &= u^2\\\\\n",
        "&\\vdots \\\\\n",
        "\\mathcal{N}_i(u,x,t) &= u_x\\\\\n",
        "\\mathcal{N}_{i+1}(u,x,t) &= u_x^2\\\\\n",
        "&\\vdots \\\\\n",
        "\\mathcal{N}_K(u,x,t) &= u^3u_{xxx}\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Numerical differentiation of the terms\n",
        "In order to actually compute the values in the linear system written in @eq-linear-system, we must compute numerical derivatives in both $t$ and in $x$.\n",
        "This isn't an issue if we have smooth, reliable data and can be quickly computed with finite differences.\n",
        "\n",
        "However, the intent of this method is to use data samples $u(x_i,t_j)$ that are collected from the real world, implying that they will each be polluted with some level of noise.\n",
        "There have been several classical methods presented for dealing with numerical differentiation of noisy data that could be used, but generally the methods revolve around an approximate fitting of a differentiable function basis to the data.\n",
        "Notable among these are:\n",
        "\n",
        "- Local polynomial regression (LOESS[@cleveland1988locally], Savitsky-Golay filter[@press1990savitzky], etc.)\n",
        "- Radial basis functions (Gaussian kernel) \n",
        "- Smoothing splines\n",
        "- Least squares spectral analysis (LSSA)\n",
        "\n",
        "These can be written along the lines of:\n",
        "$$\n",
        "\\underset{\\vec{c}}{\\text{argmin}} \\; \\sum_{i,j}^{N,M}\\|u(x_i,t_j) - F(x_i,t_j,\\vec{c})\\|_2\n",
        "$$\n",
        "where\n",
        "$$\n",
        "F(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\phi_l(x_i,t_j)\n",
        "$$\n",
        "and $\\phi$ represents our chosen basis function.\n",
        "Once computed, we can easily approximate derivatives of $u$ via:\n",
        "$$\n",
        "u_x(x_i,t_j) \\approx F_x(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\frac{d}{dx}\\phi_l(x_i,t_j)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Each of these has the goal of smoothing the given data while simultaneously providing an exact derivative of the approximation.\n",
        "This is a similar idea as we have discussed with automatic differentiation of neural networks.\n",
        "In fact, you could consider fitting a neural network to be the same as fitting a randomly initialized nested basis of nonlinear functions (since they are dense according to the universal approximation theorem).\n",
        "We will explore this idea in the example problem in @sec-simulated.\n",
        "\n",
        "### Sparse regression\n",
        "Once the matrix in @eq-linear-system has been created using numerical differentiation, it remains to sift through the nonlinear terms to determine which, if any, contribute to the time evolution of our state variable of interest.\n",
        "It is usually reasonable to consider that not all the nonlinear terms should be included in the equation, so we would like to determine the most parsimonious (smallest) combination of them that will capture our desired qualitative and quantitative behavior in the system.\n",
        "\n",
        "There are two main families of sparse regression methods:\n",
        "\n",
        "**Greedy methods**: Iterative add/remove terms that best match the time derivative in some metric ($R^2$ coefficient of determination, Akaike Information Criteria (AIC), etc.).\n",
        "\n",
        "- Forward selection: Start with no terms, add one by one according to which maximizes $R^2$ or AIC at each step\n",
        "- Backward selection: Start with all terms, remove one by one according to which least reduces $R^2$ or AIC\n",
        "- (Orthogonal) Matching pursuit: Start with no terms, add one by one according to which maximizes correlation (orthogonalizing after each step)\n",
        "\n",
        "**Regularization methods**: Add a penalty to the regression for having too many terms or large coefficients $\\xi_i$.\n",
        "These can be written roughly as:\n",
        "$$\n",
        "\\underset{\\vec{\\xi}}{\\text{argmin}}\\; \\|u_t(x_i,t_j) - \\mathbf{\\mathcal{N}}(u_{ij},x_i,t_j) \\cdot \\vec{\\xi}\\|_2^2 + \\lambda \\|\\xi\\|_C\n",
        "$$\n",
        "\n",
        "- Ridge regression: Let $C=2$ forcing coefficients $\\vec{\\xi}$ to be smaller. We hope that important coefficients will remain larger while unimportant ones shrink.\n",
        "- Lasso regression: Let $C=1$ forcing coefficients $\\vec{\\xi}$ to be smaller and various to be set to 0 (due to the geometry of the 1-norm).\n",
        "- 0-norm regression: Let $C=0$ which is a measure that counts the number of nonzero coefficients in $\\vec{\\xi}$. Computing this usually requires a combination of regularization and relaxation best captured by the SR3 method[@zheng2018unified].\n",
        "\n",
        "Combinations of these two methods which iterative perform regularization methods removing terms with small coefficients according to a given threshold have also been proposed (Sequential Threshold Ridge Regression[@rudy2017data] or the original SINDy algorithm[@brunton2016discovering]).\n",
        "\n",
        "### Summary of the method\n",
        "In summary, the procedure to use SINDy is as follows:\n",
        "\n",
        "1. Collect sample points of a continuum state variable of interest $u(x_i,t_j)$\n",
        "2. Form a \"library\" of possible terms for the differential model of the system $\\mathcal{N}_k(u,x,t)$\n",
        "3. Compute the libary at sample points using noise robust numerical differentiation to compute both $u_t(x_i,t_j)$ and $\\mathcal{N}_k(u_{ij},x_i,t_j)$\n",
        "4. Use sparse regression to determine a sparse vector $\\vec{\\xi}$ which closely approximates $u_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)$\n",
        "\n",
        "To really explore this method, we will walk through this process using simulated traveling wave data in @sec-simulated and using real extracted data in @sec-extracted.\n",
        "\n",
        "## Application to simulated wave data {#sec-simulated}\n",
        "\n",
        ":::{.callout-note}\n",
        "For this workshop you will need to install the following packages:\n",
        "\n",
        "```bash\n",
        "mamba install numpy matplotlib py-pde sympy jax optax flax scikit-learn scikit-image av\n",
        "```\n",
        ":::\n",
        "\n",
        "Given some data generated via finite differences of the simple advection equation:\n",
        "$$\n",
        "h_t(x,t) = h_x(x,t)\n",
        "$$\n",
        "with periodic boundaries and a Gaussian initial condition, we have the following measurement of state variable $h$ (height of the wave):\n"
      ],
      "id": "f3c8d80d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Generate simple wave data\n",
        "import numpy as np\n",
        "import pde\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Domain\n",
        "xmax = 1.0\n",
        "nx = 100\n",
        "dt = 1e-6\n",
        "tmax = 1.0-2*dt\n",
        "save_dt = 0.01\n",
        "init_cond = \".1*exp(-(1/.01)*(x-0.3)**2)\"\n",
        "\n",
        "grid = pde.CartesianGrid([(0.0,xmax)],nx,periodic=True)\n",
        "h = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\n",
        "eq = pde.PDE({\"h\": \"-d_dx(h)\"})\n",
        "storage = pde.MemoryStorage()\n",
        "\n",
        "result = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt),ret_info=False)\n",
        "\n",
        "# pde.plot_kymograph(storage)\n",
        "movie = pde.visualization.movie(storage,\"simple_wave.gif\")\n",
        "\n",
        "h=np.array(storage.data)\n",
        "x=storage.grid.coordinate_arrays[0]\n",
        "t=np.array(storage.times)\n",
        "np.savez(\"simple_wave.npz\",h=h,x=x,t=t)\n",
        "plt.close()"
      ],
      "id": "58333656",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](simple_wave.gif)\n",
        "\n",
        "### Generating nonlinear library\n",
        "Generating a library can be most easily accomplished using the `sympy` symbolic math Python library.\n",
        "To be overly thorough, we will generate up to 4th order polynomial combinations of up to 4th order spatial derivatives.\n",
        "\n",
        "We can first initialize our spatial and state variables:"
      ],
      "id": "d6f118e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sympy as sp\n",
        "\n",
        "x_sym,t_sym = sp.symbols(\"x t\")\n",
        "h_sym = sp.Function(\"h\")"
      ],
      "id": "d7b43351",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a specified order, we can now create symbolic derivative terms (constructed to be most legible):"
      ],
      "id": "1bcabf56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Library parameters\n",
        "max_poly_order = 4\n",
        "max_diff_order = 4\n",
        "\n",
        "diff_terms = [h_sym(x_sym,t_sym)]\n",
        "diff_terms += [sp.Function(str(h_sym)+\"_\"+(i*str(x_sym)))(x_sym,t_sym) for i in range(1,max_diff_order+1)]\n",
        "print(diff_terms)"
      ],
      "id": "0386c492",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, combining these into polynomials up to 4th order (again, this is overkill, but for a system you don't fully understand, you may want to have a very complete library):"
      ],
      "id": "47835d70"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from itertools import combinations_with_replacement\n",
        "\n",
        "terms = []\n",
        "for po in range(max_poly_order+1):\n",
        "    if po == 0:\n",
        "        term = sp.core.numbers.One()\n",
        "    else:\n",
        "        combos = combinations_with_replacement(diff_terms,po)\n",
        "        for combo in combos:\n",
        "            term = 1\n",
        "            for combo_term in combo:\n",
        "                term *= combo_term\n",
        "            terms.append(term)\n",
        "print(terms)"
      ],
      "id": "122c049d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Approximating data\n",
        "In order to provide numerical derivatives of our data, we will use a neural network approximation.\n",
        "\n",
        ":::{.callout-note}\n",
        "This is far beyond what is necessary for this particular setting, but is a method that can generalize to data not on a uniform grid and in high dimension, which can be useful.\n",
        "The lack of requirement for a grid can also help with robustly fitting to noisy data by using a train-test methodology in @sec-noisy which classical basis functions do not handle well.\n",
        "Using neural networks in this way as a combination with SINDy is explored more in[@xu2019dl].\n",
        ":::\n",
        "\n",
        "To begin, we will be using the Google developed [`flax`](https://flax.readthedocs.io/en/latest/) neural network framework which is built on their [`jax`](https://jax.readthedocs.io/en/latest/index.html) automatic differentiation library and the [`optax`](https://optax.readthedocs.io/en/latest/optax-101.html) optimization library.\n",
        "The reason for this will become clearer when we consider taking a fourth order derivative in $x$ of the network, a task which many other popular frameworks (`pytorch`, `keras`, `tensorflow`, etc.) cannot do (at least not nearly as concisely).\n",
        "However, the `jax` library is state-of-the-art for automatic differentiation and is used heavily for differentiable programming and neural network research today (see Appendix for more information). \n",
        "\n",
        "#### Creating the neural network model\n",
        "First, we will create a simple dense neural network model using the $\\tanh$ activation (to ensure a smooth approximation):"
      ],
      "id": "702e6b74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(60)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(12)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(1)(x)\n",
        "        return x"
      ],
      "id": "eb749337",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model will take an input of $(x_i,t_j)$ (a dimension 2 array), linearly map it to a dimension 60 space, apply a tanh activation, linearly map to a dimension 12 space, apply a tanh activation, then linearly map to a dimension 1 output (this particular width and depth was chosen arbitrarily).\n",
        "\n",
        "We next initialize the parameters of the network (each of the linear transformation matrices) and print out the dimensions of the corresponding arrays:"
      ],
      "id": "c9ec0e9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax\n",
        "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
        "\n",
        "# Random generator seed\n",
        "rng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\n",
        "random_data = jax.random.normal(rng1,(2,))\n",
        "model1 = MyNet()\n",
        "params1 = model1.init(rng2,random_data)\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, params1))"
      ],
      "id": "0aeee04d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "The confusing `tree_util.tree_map` command is a convenience function for mapping a function (in this case `lambda x: x.shape`) across a set of different objects.\n",
        "This is useful because these objects can be arrays, dictionaries, lists, classes (i.e. other neural networks), etc.\n",
        ":::\n",
        "\n",
        "#### Loading and processing data\n",
        "In order to fit this model to the data, we must load the data into batches of $(x_i,t_j,u(x_i,t_j))$ points.\n",
        "Since our data is known to be quite smooth and we want to maximize the fit, we will use batches of size 10000:"
      ],
      "id": "e496d94c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def load_data(data_path,noise_scale=0,norm=True):\n",
        "    raw_data = np.load(data_path)\n",
        "    h = raw_data[\"h\"].astype(jnp.float32)\n",
        "    x = raw_data[\"x\"].astype(jnp.float32)\n",
        "    t = raw_data[\"t\"].astype(jnp.float32)\n",
        "\n",
        "    # Add noise if needed\n",
        "    h += noise_scale*jnp.std(h)*np.random.normal(size=h.shape)\n",
        "\n",
        "    # Mean center, std center data\n",
        "    if norm:\n",
        "        h = (h - jnp.mean(h)) / jnp.std(h)\n",
        "        x = (x - jnp.mean(x)) / jnp.std(x)\n",
        "        t = (t - jnp.mean(t)) / jnp.std(t)\n",
        "    return x,t,h\n",
        "\n",
        "def batch_data(x,t,h,batch_size):\n",
        "    # Split data into batches\n",
        "    data = []\n",
        "    for i in range(0,len(x),batch_size):\n",
        "        temp_xt = jnp.vstack((x[i:i+batch_size], t[i:i+batch_size])).T\n",
        "        temp_h = h[i:i+batch_size].reshape((-1,1))\n",
        "        data.append((temp_xt,temp_h))\n",
        "    return data\n",
        "\n",
        "x,t,h = load_data(\"simple_wave.npz\")\n",
        "X,T = jnp.meshgrid(x,t)\n",
        "data = batch_data(X.flatten(),T.flatten(),h.flatten(),10000)"
      ],
      "id": "1eb280a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the data needed to be centered and scaled to have a mean of $\\bar{h}=0$ and standard deviation of $\\overline{(h - \\bar{h})}=1$ in order to best use the $\\tanh$ activation (which extends from -1 to 1). \n",
        "\n",
        "#### Training the model\n",
        "We will use the mean squared error fit of the data to our neural network output (just in time compiled with `@jax.jit` for maximum speed):"
      ],
      "id": "108c6940"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@jax.jit\n",
        "def mse(params,input,targets):\n",
        "    def squared_error(x,y):\n",
        "        pred = model1.apply(params,x)\n",
        "        return jnp.mean((y - pred)**2)\n",
        "    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\n",
        "loss_grad_fn = jax.value_and_grad(mse)"
      ],
      "id": "ffee8282",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this loss defined, we initialize an ADAM optimizer and optimizer state and wrap the loss function to return both the output and gradient:"
      ],
      "id": "a5f97395"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import optax\n",
        "\n",
        "learning_rate = 1e-2\n",
        "tx = optax.adam(learning_rate)\n",
        "opt_state = tx.init(params1)"
      ],
      "id": "8c1d920b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train the model to take in $(x_i,t_j)$ and output $u(x_i,t_j)$.\n",
        "Performing 1000 iterations over the data, we will print the mean squared error on the data as we proceed with the training:"
      ],
      "id": "064044cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "epochs = 1000\n",
        "all_xt = jnp.array([data[i][0] for i in range(len(data))])\n",
        "all_h = jnp.array([data[i][1] for i in range(len(data))])\n",
        "for i in range(epochs):\n",
        "    xt_batch = data[i%len(data)][0]\n",
        "    h_batch = data[i%len(data)][1]\n",
        "    loss_val, grads = loss_grad_fn(params1, xt_batch, h_batch)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params1 = optax.apply_updates(params1, updates)\n",
        "    if i % 100 == 0:\n",
        "        train_loss = mse(params1,all_xt,all_h)\n",
        "        print(\"Training loss step {}: {}\".format(i,train_loss))"
      ],
      "id": "88fd6f71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can tell, this procedure is somewhat more manual than other libraries such as `keras` but keep you closer to the details, allowing for more flexibility in implementation.\n",
        "\n",
        "#### Validating fit\n",
        "The fit to the model can be visualized as follows:"
      ],
      "id": "2356bfdf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.animation as anim\n",
        "\n",
        "X,T = jnp.meshgrid(x,t)\n",
        "xt_points = jnp.vstack([X.flatten(),T.flatten()]).T\n",
        "hhat1 = model1.apply(params1,xt_points).reshape(X.shape)\n",
        "diff = np.sqrt((h - hhat1)**2)\n",
        "\n",
        "def animate_data(x,t,data_list,labels):\n",
        "    fig = plt.figure()\n",
        "    plt.xlabel(\"$x$\")\n",
        "    plots = []\n",
        "\n",
        "    for i in range(len(data_list)):\n",
        "        plot = plt.plot(x,data_list[i][0,:],label=labels[i])[0]\n",
        "        plots.append(plot)\n",
        "\n",
        "    def anim_func(j):\n",
        "        for i in range(len(plots)):\n",
        "            plots[i].set_ydata(data_list[i][j,:])\n",
        "        return plots\n",
        "\n",
        "    plt.legend()\n",
        "    approx_anim = anim.FuncAnimation(fig, anim_func, range(len(t)))\n",
        "    return approx_anim\n",
        "\n",
        "animation1 = animate_data(x,t,[h,hhat1,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\n",
        "animation1.save(\"clean_h_compare.gif\")\n",
        "plt.close()"
      ],
      "id": "1c3819f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](clean_h_compare.gif)\n",
        "\n",
        "### Numerically differentiating the neural network model\n",
        "The original reason to fit this model to the data was to be able to construct each of the terms in our nonlinear libary for the system.\n",
        "In order to differentiate the model, we must wrap it in a function that takes our inputs and returns the output. "
      ],
      "id": "0e1c3f60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def model_for_diff(x,t):\n",
        "    new_x = jnp.array([x,t])\n",
        "    return model1.apply(params1, new_x)[0]\n",
        "\n",
        "# Take a derivative with respect to the first input (x) at point (x_i,t_j)\n",
        "x_i = 0.3; t_j = 0.3\n",
        "jax.grad(model_for_diff,0)(x_i,t_j)"
      ],
      "id": "d09ae472",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "If we were to differentiate the model directly, we would compute derivatives for all the parameters!\n",
        "This is the main challenge with using other neural network frameworks for this kind of function approximation.\n",
        ":::\n",
        "Applying this iteratively, we can construct derivatives $h_x(x,t), \\ldots, h_{xxxx}(x,t)$ as is required by our library:"
      ],
      "id": "40b7ceae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "diff_term_values = {}\n",
        "for i in range(max_diff_order+1):\n",
        "    diff_func = model_for_diff\n",
        "    # Iteratively apply derivatives\n",
        "    for _ in range(i):\n",
        "        diff_func = jax.grad(diff_func, 0)\n",
        "    def unpack_diff_func(x):\n",
        "        new_x,new_t = x\n",
        "        return diff_func(new_x,new_t)\n",
        "    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))"
      ],
      "id": "99972353",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then reconstruct our terms attaching them to their corresponding values on our $(x,t)$ grid:"
      ],
      "id": "2f163bab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def construct_terms(diff_term_values):\n",
        "    term_values = {}\n",
        "    term_shape = np.shape(diff_term_values[list(diff_term_values.keys())[0]])\n",
        "    for order in range(max_poly_order+1):\n",
        "        if order == 0:\n",
        "            term = sp.core.numbers.One()\n",
        "            term_values[term] = np.ones(term_shape)\n",
        "        else:\n",
        "            combos = combinations_with_replacement(diff_terms,order)\n",
        "            for combo in combos:\n",
        "                term = 1\n",
        "                temp_term_value = 1\n",
        "                for combo_term in combo:\n",
        "                    term *= combo_term\n",
        "                    temp_term_value *= diff_term_values[combo_term]\n",
        "                term_values[term] = temp_term_value\n",
        "    return term_values\n",
        "term_values = construct_terms(diff_term_values)"
      ],
      "id": "f8ce7817",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we compute the derivative of the network with respect to time:"
      ],
      "id": "7bde39fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def unpack_diff_func(x):\n",
        "    new_x,new_t = x\n",
        "    return jax.grad(model_for_diff,1)(new_x,new_t)\n",
        "\n",
        "h_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\n",
        "h_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))"
      ],
      "id": "48061d30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solving the sparse regression problem\n",
        "In order to cleanly work with our term library, we will use a very popular Python data science package called `pandas`.\n",
        "Simply put, this library allows you to easily load, manipulate, and save tabular data.\n",
        "Here is our library as a `pandas` `DataFrame`:"
      ],
      "id": "fa129d31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "term_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\n",
        "term_matrix"
      ],
      "id": "7adfcd80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then use another extremely popular machine learning Python package called `scikit-learn` to easily work with our regression models.\n",
        "\n",
        "#### Ordinary least squares\n",
        "First, let's apply ordinary least squares to see if the solution is clear:\n"
      ],
      "id": "a678867e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.linear_model as lm\n",
        "import sklearn.metrics as met\n",
        "\n",
        "def compute_ols_results(A,b):\n",
        "    ols = lm.LinearRegression()\n",
        "    ols.fit(A, b)\n",
        "    Rsquare = met.r2_score(ols.predict(A), b)\n",
        "    print(\"R^2: {}\".format(Rsquare))\n",
        "    ols_results = pd.DataFrame(\n",
        "        data=[ols.coef_],\n",
        "        columns=term_matrix.columns,\n",
        "        index=[\"Coefficients\"]\n",
        "    )\n",
        "    return ols_results\n",
        "compute_ols_results(term_matrix, h_t)"
      ],
      "id": "90de5f74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although the $R^2$ value implies that we have successful explained the variance in $h_t$ by linearly combining our term library, it is unclear which of all the terms most contributes to the time evolution from their coefficients.\n",
        "\n",
        "#### Lasso\n",
        "Now, let's add some regularization to try to remove some terms with the Lasso regression:\n"
      ],
      "id": "a2638491"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_lasso_results(A,b,lamb):\n",
        "    lasso = lm.Lasso(lamb)\n",
        "    lasso.fit(A,b)\n",
        "    lasso_results = pd.DataFrame(\n",
        "        data=[lasso.coef_[lasso.coef_ != 0]],\n",
        "        columns=term_matrix.columns[lasso.coef_ != 0],\n",
        "        index=[\"Coefficients\"]\n",
        "    )\n",
        "    return lasso_results\n",
        "compute_lasso_results(term_matrix,h_t,30)"
      ],
      "id": "d37171bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now this at least removed some of the terms, but it also removed the term we know is correct!\n",
        "It's somewhat hard to interpret exactly what this means.\n",
        "A convenient analysis using the Lasso method is to perform a \"lasso path\" in which we steadily decrease the regularization $\\lambda$ to add more and more terms and pay attention to the order with which they are added:\n"
      ],
      "id": "78a5236e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_lasso_path_results(A,b):\n",
        "    lambs, coef_path, _ = lm.lasso_path(A, b, alphas=[1000,200,100,10,2])\n",
        "    for i in range(coef_path.shape[1]):\n",
        "        print(\"lambda = {}\".format(lambs[i]))\n",
        "        temp_results = pd.DataFrame(\n",
        "            data=[coef_path[:,i][coef_path[:,i] != 0]],\n",
        "            columns=term_matrix.columns[coef_path[:,i] != 0],\n",
        "            index=[\"Coefficients\"]\n",
        "        )\n",
        "        display(temp_results)\n",
        "compute_lasso_path_results(term_matrix,h_t)"
      ],
      "id": "7db92cfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, although this gives us a sense of sparsity, it also doesn't seem to capture the solution well.\n",
        "\n",
        "#### Greedy forward selection\n",
        "Let's instead try a greedy method for our system that will inform which terms should be included.\n",
        "To do so, we will use a generic `scikit-learn` interface called `SequentialFeatureSelector` as well as the $R^2$ coefficient of determination `r2_score` to select terms one by one that best \"explain the variance\" in the time evolution $h_t(x,t)$.\n",
        "As the terms are selected, we will compute the coefficients of the small libraries via ordinary least squares:\n"
      ],
      "id": "a139d76b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.feature_selection as fs\n",
        "\n",
        "def forward_r2_select(A,b,num_terms=4):\n",
        "    for i in range(1,num_terms+1):\n",
        "        sfs = fs.SequentialFeatureSelector(\n",
        "            lm.LinearRegression(),\n",
        "            n_features_to_select=i,\n",
        "            scoring=met.make_scorer(met.r2_score)\n",
        "        )\n",
        "        new_A = sfs.fit_transform(A,b)\n",
        "        new_ols = sfs.estimator\n",
        "        new_ols.fit(new_A,b)\n",
        "        Rsquare = met.r2_score(new_ols.predict(new_A),b)\n",
        "        feat_names = sfs.get_feature_names_out(A.columns)\n",
        "        print(\"R^2: {}\".format(Rsquare))\n",
        "        temp_results = pd.DataFrame(\n",
        "            data=[new_ols.coef_],\n",
        "            columns=feat_names,\n",
        "            index=[\"Coefficients\"]\n",
        "        )\n",
        "        display(temp_results)\n",
        "\n",
        "forward_r2_select(term_matrix, h_t)"
      ],
      "id": "bbe8a43a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This seems to easily pick up that the only term needed to completely resolve the time evolution is $h_x(x,t)$!\n",
        "\n",
        "## Application to noisy simulated wave data {#sec-noisy}\n",
        "In a real system, we could not expect to immediately have data as smooth as that we used in @sec-simulated.\n",
        "However, the procedure is unchanged.\n",
        "The only challenge will be fitting the neural network to our data.\n",
        "Let's add some noise to the data:\n"
      ],
      "id": "dbd6f9fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x,t,noisy_h = load_data(\"simple_wave.npz\",.2)\n",
        "animation2 = animate_data(x,t,[noisy_h], [\"h noisy\"])\n",
        "animation2.save(\"noisy_h.gif\")\n",
        "plt.close()"
      ],
      "id": "ede0696f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](noisy_h.gif)\n",
        "\n",
        "Given our data is now noisy, we may want to implement a train-validation-test method for fitting.\n",
        "Simply put, this means that we will hold out a portion of our data from the training procedure.\n",
        "Part of this held-back data (validation set) will be used to validate that our model can generalize to other points during training.\n",
        "The other part of the held-back data (test set) will be used as a final check on how well the model extrapolates out of the training data.\n"
      ],
      "id": "cde5fc64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.model_selection as ms\n",
        "\n",
        "X,T = jnp.meshgrid(x,t)\n",
        "xt_noisy = np.vstack((X.flatten(),T.flatten())).T\n",
        "h_noisy = noisy_h.flatten()\n",
        "xt_train, xt_test, h_train, h_test = ms.train_test_split(xt_noisy,h_noisy,test_size=.1,train_size=.9)\n",
        "xt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n",
        "\n",
        "train_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\n",
        "valid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\n",
        "test_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)"
      ],
      "id": "8d6bda35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we apply our previous model construction and training:\n"
      ],
      "id": "3b4af39f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize model\n",
        "rng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\n",
        "random_data = jax.random.normal(rng1,(2,))\n",
        "model2 = MyNet()\n",
        "params2 = model2.init(rng2,random_data)\n",
        "\n",
        "# Loss function\n",
        "@jax.jit\n",
        "def mse(params,input,targets):\n",
        "    def squared_error(x,y):\n",
        "        pred = model2.apply(params,x)\n",
        "        return jnp.mean((y - pred)**2)\n",
        "    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\n",
        "loss_grad_fn = jax.value_and_grad(mse)\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 1e-2\n",
        "tx = optax.adam(learning_rate)\n",
        "opt_state = tx.init(params2)\n",
        "\n",
        "# Training (adjusted to use our validation data\n",
        "epochs = 1200\n",
        "for i in range(epochs):\n",
        "    xt_batch = train_data[i%len(train_data)][0]\n",
        "    h_batch = train_data[i%len(train_data)][1]\n",
        "    loss_val, grads = loss_grad_fn(params2, xt_batch, h_batch)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params2 = optax.apply_updates(params2, updates)\n",
        "    if i % 100 == 0:\n",
        "        train_loss = mse(params2,xt_train,h_train)\n",
        "        valid_loss = mse(params2,xt_valid,h_valid)\n",
        "        print(\"Step {}\".format(i))\n",
        "        print(\"Training loss: {}\".format(train_loss))\n",
        "        print(\"Validation loss: {}\".format(valid_loss))\n",
        "        print()\n",
        "test_loss = mse(params2,xt_test,h_test)\n",
        "print(\"Test loss after training: {}\".format(test_loss))\n",
        "\n",
        "hhat2 = model2.apply(params2,xt_points).reshape(X.shape)\n",
        "diff = np.sqrt((noisy_h - hhat2)**2)\n",
        "diff2 = np.sqrt((hhat1 - hhat2)**2)\n",
        "animation3 = animate_data(x,t,[noisy_h,hhat2,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\n",
        "animation3.save(\"noisy_h_compare.gif\")\n",
        "plt.close()\n",
        "animation3 = animate_data(x,t,[hhat1,hhat2,diff2],[\"$\\hat{h}$ clean\",\"$\\hat{h}$ noisy\",\"$L^2$ error\"])\n",
        "animation3.save(\"noisy_hhat_compare.gif\")\n",
        "plt.close()"
      ],
      "id": "c799ff69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting fit can be seen in the following video:\n",
        "\n",
        "![](noisy_h_compare.gif)\n",
        "\n",
        "Looks pretty good all things considered!\n",
        "We can also compare this with the fit on clean data to see how impressive the robustness to noise was:\n",
        "\n",
        "![](noisy_hhat_compare.gif)\n",
        "\n",
        "Finally, we construct the terms and check the results after forward selection:\n"
      ],
      "id": "43d2930e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def model_for_diff(x,t):\n",
        "    new_x = jnp.array([x,t])\n",
        "    return model2.apply(params2, new_x)[0]\n",
        "\n",
        "# Construct terms numerically\n",
        "diff_term_values = {}\n",
        "for i in range(max_diff_order+1):\n",
        "    diff_func = model_for_diff\n",
        "    # Iteratively apply derivatives\n",
        "    for _ in range(i):\n",
        "        diff_func = jax.grad(diff_func, 0)\n",
        "    def unpack_diff_func(x):\n",
        "        new_x,new_t = x\n",
        "        return diff_func(new_x,new_t)\n",
        "    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))\n",
        "term_values = construct_terms(diff_term_values)\n",
        "\n",
        "def unpack_diff_func(x):\n",
        "    new_x,new_t = x\n",
        "    return jax.grad(model_for_diff,1)(new_x,new_t)\n",
        "\n",
        "h_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\n",
        "h_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))\n",
        "\n",
        "# Forward selection\n",
        "term_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\n",
        "forward_r2_select(term_matrix, h_t)"
      ],
      "id": "6f0549c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boom!\n",
        "Landed right on the money.\n",
        "This is a simple example with a straightforward answer, but example holds to show the overall procedure for handling data with additive noise (multiplicative noise, which is more structural, would be an altogether different challenge).\n",
        "\n",
        "## Application to extracted wave data {#sec-extracted}\n",
        "Now, applying this procedure to real data is as simple as replacing our original dataset with an experimental dataset.\n",
        "However, the extraction process has a strong influence on the quality of the data that we will be using, so it deserves to be treated with some detail.\n",
        "\n",
        "### Image data extraction\n",
        "The original video we will be using can be found on YouTube [here](https://www.youtube.com/watch?v=wEbYELtGZwI).\n",
        "\n",
        "![](youtube_video.mp4)\n",
        "\n",
        "We can load this video into individual image frames via:\n"
      ],
      "id": "7001109e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import skimage as img\n",
        "import imageio.v3 as iio\n",
        "\n",
        "raw_frames = []\n",
        "cut = (160,200)\n",
        "for i in range(200,232):\n",
        "    frame = iio.imread(\"youtube_video.mp4\",plugin=\"pyav\",index=i)\n",
        "\n",
        "    # Cut the image to focus only on the wave portion\n",
        "    raw_frame = frame[cut[0]:cut[1],:,:]\n",
        "    raw_frames.append(raw_frame)\n",
        "raw_frames = np.array(raw_frames)\n",
        "plt.figure(figsize=(8,1))\n",
        "plt.imshow(raw_frames[16])\n",
        "plt.axis(False); plt.show()"
      ],
      "id": "0bce6dc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then need to remove the background and isolate the wave portion of the image, which is facilitated by the green color of the water in this video:\n"
      ],
      "id": "f3f2b411"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frames = []\n",
        "for i in range(len(raw_frames)):\n",
        "    frame = raw_frames[i]\n",
        "\n",
        "    # Find where the image is more green than red or blue and very bright green\n",
        "    mean_green = np.mean(frame[:,:,1])\n",
        "    std_green = np.std(frame[:,:,1])\n",
        "    frame = (frame[:,:,1] > frame[:,:,0]) & (frame[:,:,1] > frame[:,:,2]) & (frame[:,:,1] > mean_green+std_green)\n",
        "    frames.append(frame)\n",
        "frames = np.array(frames)\n",
        "plt.figure(figsize=(8,1))\n",
        "plt.imshow(frames[16],cmap=\"gray\")\n",
        "plt.axis(False); plt.show()"
      ],
      "id": "3f881944",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By averaging these pixels across all vertical pixels in the image, we can get a rough wave outline:\n"
      ],
      "id": "6d0e17ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "heights = []\n",
        "for i in range(len(frames)):\n",
        "    frame = frames[i]\n",
        "    \n",
        "    # Approximate wave height by averaging y-locations of bright green areas\n",
        "    height = np.zeros(frame.shape[1])\n",
        "    for j in range(frame.shape[1]):\n",
        "        height[j] = np.mean(np.where(frame[:,j] == 1)[0])\n",
        "    heights.append(height)\n",
        "heights = np.array(heights)\n",
        "base = heights[16, 0]\n",
        "\n",
        "plt.figure(figsize=(8,1))\n",
        "plt.imshow(frames[16],cmap=\"gray\")\n",
        "line = plt.plot(heights[16], color=\"red\",lw=3)[0]\n",
        "line2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\n",
        "plt.axis(False); plt.show()"
      ],
      "id": "7c784403",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can note that the video is not quite level to the wave surface, so we can use a linear adjustment to align the water boundary heights at the middle of the video:\n"
      ],
      "id": "936a994f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adjust images and heights for an un-leveled camera\n",
        "im_width = len(heights[16])\n",
        "slope = (heights[16][-1] - heights[16][0]) / im_width\n",
        "for i in range(len(heights)):\n",
        "    frame = frames[i]\n",
        "    height = heights[i]\n",
        "\n",
        "    # Adjust\n",
        "    for j in range(len(height)):\n",
        "        shift = int(slope*(im_width-j))\n",
        "        # Move frame pixels per column\n",
        "        frame[:,j] = np.roll(frame[:,j], shift)\n",
        "        # Move height of wave\n",
        "        height[j] += shift\n",
        "    frames[i] = frame\n",
        "    heights[i] = height\n",
        "\n",
        "frames = np.array(frames)\n",
        "raw_frames = np.array(raw_frames)\n",
        "heights = np.array(heights)\n",
        "\n",
        "fig = plt.figure(figsize=(8,1))\n",
        "im = plt.imshow(frames[0],cmap=\"gray\")\n",
        "line = plt.plot(heights[0], color=\"red\",lw=3)[0]\n",
        "line2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\n",
        "plt.axis(False);\n",
        "\n",
        "def animation_function(i):\n",
        "    im.set_array(frames[i])\n",
        "    line.set_ydata(heights[i])\n",
        "    return [im,line,line2]\n",
        "\n",
        "wave_animation = anim.FuncAnimation(fig, animation_function, frames=range(len(frames)), blit=True)\n",
        "wave_animation.save(\"extracted_wave.gif\")\n",
        "plt.close()"
      ],
      "id": "92c525b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](extracted_wave.gif)\n",
        "\n",
        "We can now save this data to be used with our previous procedure:"
      ],
      "id": "93e30d1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Video portion is about 2 seconds long\n",
        "times = np.linspace(0,2,len(heights))\n",
        "# No given space scale\n",
        "x_domain = np.arange(len(heights[0]))\n",
        "np.save(\"video_wave_images.npy\",raw_frames)\n",
        "np.savez(\"video_wave_heights.npz\",h=heights,x=x_domain,t=times)"
      ],
      "id": "ecc1ded4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using our experimental dataset\n",
        "Using the same methods as listed in @sec-noisy, we can discover an equation for this particular dataset:\n"
      ],
      "id": "ac6154a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x,t,ext_h = load_data(\"video_wave_heights.npz\")\n",
        "# Flip image wave to be more familiar\n",
        "ext_h = -ext_h\n",
        "animation2 = animate_data(x,t,[ext_h], [\"extracted h\"])\n",
        "animation2.save(\"extracted_h.gif\")\n",
        "plt.close()"
      ],
      "id": "9322205c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](extracted_h.gif)\n"
      ],
      "id": "589cac01"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Splitting data\n",
        "X,T = jnp.meshgrid(x,t)\n",
        "xt_ext = np.vstack((X.flatten(),T.flatten())).T\n",
        "h_ext = ext_h.flatten()\n",
        "xt_train, xt_test, h_train, h_test = ms.train_test_split(xt_ext,h_ext,test_size=.1,train_size=.9)\n",
        "xt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n",
        "\n",
        "train_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\n",
        "valid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\n",
        "test_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)\n",
        "\n",
        "# Initialize model\n",
        "rng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\n",
        "random_data = jax.random.normal(rng1,(2,))\n",
        "model3 = MyNet()\n",
        "params3 = model3.init(rng2,random_data)\n",
        "\n",
        "# Loss function\n",
        "@jax.jit\n",
        "def mse(params,input,targets):\n",
        "    def squared_error(x,y):\n",
        "        pred = model3.apply(params,x)\n",
        "        return jnp.mean((y - pred)**2)\n",
        "    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\n",
        "loss_grad_fn = jax.value_and_grad(mse)\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 1e-2\n",
        "tx = optax.adam(learning_rate)\n",
        "opt_state = tx.init(params3)\n",
        "\n",
        "# Training (adjusted to use our validation data\n",
        "epochs = 1200\n",
        "for i in range(epochs):\n",
        "    xt_batch = train_data[i%len(train_data)][0]\n",
        "    h_batch = train_data[i%len(train_data)][1]\n",
        "    loss_val, grads = loss_grad_fn(params3, xt_batch, h_batch)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params3 = optax.apply_updates(params3, updates)\n",
        "    if i % 100 == 0:\n",
        "        train_loss = mse(params3,xt_train,h_train)\n",
        "        valid_loss = mse(params3,xt_valid,h_valid)\n",
        "        print(\"Step {}\".format(i))\n",
        "        print(\"Training loss: {}\".format(train_loss))\n",
        "        print(\"Validation loss: {}\".format(valid_loss))\n",
        "        print()\n",
        "test_loss = mse(params3,xt_test,h_test)\n",
        "print(\"Test loss after training: {}\".format(test_loss))\n",
        "\n",
        "hhat = model3.apply(params3,xt_ext).reshape(X.shape)\n",
        "diff = np.sqrt((ext_h - hhat)**2)\n",
        "animation3 = animate_data(x,t,[ext_h,hhat,diff],[\"$extracted h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\n",
        "animation3.save(\"ext_h_compare.gif\")\n",
        "plt.close()"
      ],
      "id": "1671209c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](ext_h_compare.gif)\n"
      ],
      "id": "ce6418d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def model_for_diff(x,t):\n",
        "    new_x = jnp.array([x,t])\n",
        "    return model3.apply(params3, new_x)[0]\n",
        "\n",
        "# Construct terms numerically\n",
        "diff_term_values = {}\n",
        "for i in range(max_diff_order+1):\n",
        "    diff_func = model_for_diff\n",
        "    # Iteratively apply derivatives\n",
        "    for _ in range(i):\n",
        "        diff_func = jax.grad(diff_func, 0)\n",
        "    def unpack_diff_func(x):\n",
        "        new_x,new_t = x\n",
        "        return diff_func(new_x,new_t)\n",
        "    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_ext))\n",
        "term_values = construct_terms(diff_term_values)\n",
        "\n",
        "def unpack_diff_func(x):\n",
        "    new_x,new_t = x\n",
        "    return jax.grad(model_for_diff,1)(new_x,new_t)\n",
        "\n",
        "h_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\n",
        "h_t = -np.array(jax.lax.map(unpack_diff_func, xt_ext))\n",
        "\n",
        "# Forward selection\n",
        "term_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_ext),2).T, names=(\"x\",\"t\")))\n",
        "forward_r2_select(term_matrix, h_t)"
      ],
      "id": "58f97ea7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feel free to play with the parameters of each step to try to change/improve the results we have seen here.\n",
        "\n",
        "## Appendix {.appendix}\n",
        "\n",
        "### The benefits of JAX {.appendix}\n",
        "[`jax`](https://jax.readthedocs.io/en/latest/index.html) is an automatic differentiation based on the [XLA](https://www.tensorflow.org/xla) compiler for Tensorflow.\n",
        "The largest difference between this library and the alternative libraries (like those included in `tensorflow` main, `pytorch`, `keras`, etc.) is that it compiles Python code down to a computational graph structure.\n",
        "Although the majority of excitement around this compiler has surrounded the optimizations that can take place one the graph structure has been identified, it also facilitates taking derivatives of arbitrarry objects.\n",
        "This is because rather than compute gradients along the path (forward mode automatic differentiation) or keeping track of operations as it goes (backward mode automatic differentiation), it has a graph structure to analyze exactly what happens to each value and parameter.\n",
        "At the end of the day, this means it is much easier to compute gradients of exactly what you want.\n",
        "\n",
        "If `jax` and `flax` have appeared too hands-on and complicated after this workshop, consider trying [`treex`](https://cgarciae.github.io/treex/) which aims to make using `jax` for neural networks simple and only need a few lines of code.\n",
        "\n",
        "### Training without normalizing the data {.appendix}\n",
        "\n",
        "In the @sec-simulated section, the `load_data` function performs a normalization of the simulated data from a range of $h(x,t) \\in [0,.1]$ where $x \\in [0,1]$ and $t \\in [0,1]$ to a range of $h(x,t) \\in [-1,1]$ with mean $\\bar{h}=0$ and standard deviation 1 with $x \\in [-1.7,1.7]$ and $t \\in [-1.7,1.7]$.\n",
        "Normalizing data like this is common in machine learning, but it is not always apparent why.\n",
        "Our case can give a strong demonstration as to the benefits of normalizing in this way.\n",
        "\n",
        "Consider that our neural network uses only the $\\tanh$ activation function.\n",
        "For those unfamiliar, this function has the form:\n"
      ],
      "id": "f5a00a85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tanh_dom = np.linspace(-5,5,100)\n",
        "tanh_range = np.tanh(tanh_dom)\n",
        "plt.plot(tanh_dom, tanh_range, label=\"$\\\\tanh(x)$\")\n",
        "plt.xlabel(\"$x$\"); plt.legend(); plt.show()"
      ],
      "id": "8a3d4f11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each layer of our neural network is connected via linear transformations of the form $\\vec{x}^T\\mathbf{W} + \\vec{b}$, thus we should be able to shift the data into the appropriate domain and range for the $\\tanh$ function.\n",
        "However, in practice, optimizing our parameters to attain this is hard to find.\n",
        "To demonstrate, consider the training of the neural network on clean simulation data without normalization:\n"
      ],
      "id": "d8a759df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x,t,h = load_data(\"simple_wave.npz\",norm=False)\n",
        "X,T = jnp.meshgrid(x,t)\n",
        "data = batch_data(X.flatten(),T.flatten(),h.flatten(),10000)\n",
        "# Random generator seed\n",
        "rng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\n",
        "random_data = jax.random.normal(rng1,(2,))\n",
        "model4 = MyNet()\n",
        "params4 = model1.init(rng2,random_data)\n",
        "\n",
        "@jax.jit\n",
        "def mse(params,input,targets):\n",
        "    def squared_error(x,y):\n",
        "        pred = model4.apply(params,x)\n",
        "        return jnp.mean((y - pred)**2)\n",
        "    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\n",
        "loss_grad_fn = jax.value_and_grad(mse)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "tx = optax.adam(learning_rate)\n",
        "opt_state = tx.init(params4)\n",
        "\n",
        "epochs = 1000\n",
        "all_xt = jnp.array([data[i][0] for i in range(len(data))])\n",
        "all_h = jnp.array([data[i][1] for i in range(len(data))])\n",
        "for i in range(epochs):\n",
        "    xt_batch = data[i%len(data)][0]\n",
        "    h_batch = data[i%len(data)][1]\n",
        "    loss_val, grads = loss_grad_fn(params4, xt_batch, h_batch)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params4 = optax.apply_updates(params4, updates)\n",
        "    if i % 100 == 0:\n",
        "        train_loss = mse(params4,all_xt,all_h)\n",
        "        print(\"Training loss step {}: {}\".format(i,train_loss))\n",
        "\n",
        "xt_points = jnp.vstack([X.flatten(),T.flatten()]).T\n",
        "hhat4 = model1.apply(params4,xt_points).reshape(X.shape)\n",
        "diff = np.sqrt((h - hhat4)**2)\n",
        "animation4 = animate_data(x,t,[h,hhat4,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\n",
        "animation4.save(\"nonorm_h_compare.gif\")\n",
        "plt.close()"
      ],
      "id": "2fd889b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](nonorm_h_compare.gif)\n",
        "\n",
        "The fit is terrible!\n",
        "We have apparently fallen into a local minimum far from the global minimum we would like to find.\n",
        "This demonstrates two important ideas relating to neural networks (validated by experience):\n",
        "\n",
        "1. They are fickle and in some cases small changes to data, architecture, and training, can dramatically change results\n",
        "2. Any help that can be given to the neural network via knowledge of the system or data can help. In this case, adjusting for the gap between the range of our data and that of the activation function was sufficient."
      ],
      "id": "8f9fe32a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}