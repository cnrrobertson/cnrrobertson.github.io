{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  daemon: 500\n",
        "  keep-ipynb: true\n",
        "---"
      ],
      "id": "c1aef8f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ùÑÔ∏è Frozen Lake\n",
        "\n",
        " **Frozen Lake** is a simple environment composed of tiles, where the AI has to **move from an initial tile to a goal**.\n",
        "\n",
        "Tiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever. \n",
        "\n",
        "The AI, or agent, has 4 possible actions: go **‚óÄÔ∏è LEFT**, **üîΩ DOWN**, **‚ñ∂Ô∏è RIGHT**, or **üîº UP**. \n",
        "\n",
        "The agent must learn to avoid holes in order to reach the goal in a **minimal number of actions**.\n",
        "\n",
        "# Required Libraries \n"
      ],
      "id": "c5a82536"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2491a9af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize the Environment\n"
      ],
      "id": "48fd44cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery = False) #in non-slippery version actions cannot be ignored\n",
        "env.reset()\n",
        "env.render()"
      ],
      "id": "499b85c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* S: starting point, safe\n",
        "* F: frozen surface, safe\n",
        "* H: hole, stuck forever\n",
        "* G: goal, safe\n"
      ],
      "id": "6349fc68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Image(filename = \"FrozenLake.gif\", width=400)"
      ],
      "id": "2de0a743",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Image(filename = \"Final.gif\", width=400)"
      ],
      "id": "8737f70e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reward\n",
        "\n",
        "Reward schedule:\n",
        "\n",
        "* Reach goal(G): +1\n",
        "\n",
        "* Reach hole(H): 0\n",
        "\n",
        "* Reach frozen surface(F): 0\n",
        "\n",
        "# Size of Action and State Space\n"
      ],
      "id": "9b6fdbdb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"State space: \", env.observation_space.n)\n",
        "print(\"Action space: \", env.action_space.n)"
      ],
      "id": "620cce30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Frozen Lake, there are **16 tiles**, which means our agent can be found in 16 different positions, called states. \n",
        "\n",
        "For each state, there are **4 possible actions**: \n",
        "\n",
        "* ‚óÄÔ∏è LEFT: **0**\n",
        "* üîΩ DOWN: **1**\n",
        "* ‚ñ∂Ô∏è RIGHT: **2**\n",
        "* üîº UP: **3**\n",
        "\n",
        "# Initialize Q Table\n"
      ],
      "id": "6719c250"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Image(filename = \"QTable.gif\", width=400)"
      ],
      "id": "3fce9e6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Our table has the following dimensions:\n",
        "# (rows x columns) = (states x actions) = (16 x 4)\n",
        "\n",
        "nb_states = env.observation_space.n  # = 16\n",
        "nb_actions = env.action_space.n      # = 4\n",
        "qtable = np.zeros((nb_states, nb_actions))\n",
        "\n",
        "# Let's see how it looks\n",
        "print('Q-table =')\n",
        "print(qtable)"
      ],
      "id": "d1258007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Update Formula\n",
        "\n",
        "### $ Q_{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha \\times (r_t + \\gamma \\times max_a Q(s_{t+1}, a) - Q(s_t, a_t)) $\n",
        "\n",
        "# Epsilon-Greedy Algorithm\n",
        "\n",
        "In this method, we want to allow our agent to either:\n",
        "\n",
        "* Take the action with the highest value **(exploitation)**;\n",
        "* Choose a random action to try to find even better ones **(exploration)**.\n"
      ],
      "id": "6283dcf6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Image(filename = \"tradeoff.gif\", width=700)"
      ],
      "id": "81d277f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters\n"
      ],
      "id": "cfc64bef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "episodes = 1000        # Total number of episodes\n",
        "alpha = 0.5            # Learning rate\n",
        "gamma = 0.9            # Discount factor\n",
        "epsilon = 1.0          # Amount of randomness in the action selection\n",
        "epsilon_decay = 0.001  # Fixed amount to decrease"
      ],
      "id": "106a68bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traning\n"
      ],
      "id": "2f5aa892"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of outcomes to plot\n",
        "outcomes = []\n",
        "\n",
        "for _ in range(episodes):\n",
        "    \n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # By default, we consider our outcome to be a failure\n",
        "    outcomes.append(\"Failure\")\n",
        "    \n",
        "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
        "    while not done:\n",
        "        # Generate a random number between 0 and 1\n",
        "        rnd = np.random.random()\n",
        "\n",
        "        # If random number < epsilon, take a random action\n",
        "        if rnd < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        # Else, take the action with the highest value in the current state\n",
        "        else:\n",
        "            action = np.argmax(qtable[state])\n",
        "             \n",
        "        # Implement this action and move the agent in the desired direction\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a)\n",
        "        qtable[state, action] = qtable[state, action] + \\\n",
        "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
        "        \n",
        "        # Update our current state\n",
        "        state = new_state\n",
        "\n",
        "        # If we have a reward, it means that our outcome is a success\n",
        "        if reward:\n",
        "            outcomes[-1] = \"Success\"\n",
        "\n",
        "    # Update epsilon\n",
        "    epsilon = max(epsilon - epsilon_decay, 0)"
      ],
      "id": "ad8a2f9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Updated Q Table\n"
      ],
      "id": "51d3752a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('===========================================')\n",
        "print('Q-table after training:')\n",
        "print(qtable)"
      ],
      "id": "e650c16f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Outcomes\n"
      ],
      "id": "74e464ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.xlabel(\"Run number\")\n",
        "plt.ylabel(\"Outcome\")\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor('gainsboro')\n",
        "plt.bar(range(len(outcomes)), outcomes, color=\"navy\", width=1.0)\n",
        "plt.show()"
      ],
      "id": "8b6357ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation \n"
      ],
      "id": "b7448586"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "episodes = 1\n",
        "nb_success = 0\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "\n",
        "# Until the agent gets stuck or reaches the goal, keep training it\n",
        "while not done:\n",
        "    \n",
        "    # Choose the action with the highest value in the current state\n",
        "    action = np.argmax(qtable[state])\n",
        "\n",
        "    # Implement this action and move the agent in the desired direction\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Render the environment \n",
        "    print()\n",
        "    env.render()\n",
        "\n",
        "    # Update our current state\n",
        "    state = new_state\n",
        "\n",
        "    # When we get a reward, it means we solved the game\n",
        "    nb_success += reward\n",
        "\n",
        "# Let's check our success rate!\n",
        "print()\n",
        "print (f\"Success rate = {nb_success/episodes*100}%\")"
      ],
      "id": "718cbe0c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}