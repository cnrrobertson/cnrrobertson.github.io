{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation'\n",
        "csl: american-physics-society.csl\n",
        "bibliography: ../../workshops.bib\n",
        "author: Connor Robertson\n",
        "execute:\n",
        "  keep-ipynb: true\n",
        "---"
      ],
      "id": "c4b6e176"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-tip collapse=\"true\"}\n",
        "## Other formats\n",
        "Download the workshop as a Jupyter notebook <a href=\"workshop1_intro.ipynb\" download>here</a>.\n",
        "\n",
        "After downloading the workshop Jupyter notebook, you can upload it to [Google Colab](https://colab.research.google.com/) to get a quick start, but you will not be able to see the animations.\n",
        ":::\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "### Machine Learning and Optimization Seminar\n",
        "The goal of the Machine Learning and Optimization seminar this year is to expose the participants to topics in machine learning and optimization by:\n",
        "\n",
        "1. Facilitating hands-on workshops and group discussions to explore and gain experience\n",
        "2. Inviting speakers to introduce machine learning and optimization concepts\n",
        "\n",
        "We are excited to get started on this but recognize that since neither machine learning nor optimization are standardized in the department, participants will have a varied level of exposure to different topics.\n",
        "Our hope is that we can use this disparity of experience to increase collaboration during the workshops in a way that can't be achieved during the talks.\n",
        "All are encouraged to share their knowledge and experience with one another openly during the workshops and to give feedback to the organizers after.\n",
        "\n",
        "All workshop material will be available [here](../../machinelearning_optimization_seminar.qmd) for later reference.\n",
        "\n",
        "This first workshop is focused on tooling and the basic concepts of machine learning and optimization with the goal that everyone can be on the same footing for later workshops.\n",
        "\n",
        "## Your Python environment \n",
        "It is safe to say that Python is the language of choice for machine learning.\n",
        "This interpreted language has a very clear and high-level syntax, is extremely convenient for interactive programming and debugging, and has an enormous user base of enterprises, researchers, and hobbyists who have built an almost infinite collection of open-source packages for every topic.\n",
        "For these three reasons the workshops for this seminar will use Python.\n",
        "\n",
        "To get started, we will give the basics of the Python programming language. \n",
        "Just kidding!\n",
        "That would take too long.\n",
        "We will instead guide you on how to install Python most conveniently, teach you how to get started learning about Python, and then point you to a curated library of much more high-quality instruction for using Python.\n",
        "A list of some such references as well as documentation for setup and important Python packages can be found in the Appendix [here](#python-resources-and-packages).\n",
        "\n",
        "### Setting up\n",
        ":::{.callout-tip}\n",
        "If you are looking for the easiest and most immediate way to get going with a Python Jupyter Notebook, check out the [Google Colab](#google-colab) section.\n",
        ":::\n",
        ":::{.callout-note}\n",
        "This section will require use of a terminal emulator for installing and running Python.\n",
        "If you are not familiar with the terminal, check out [this quick tutorial](https://mrkaluzny.com/blog/terminal-101-getting-started-with-terminal/) to get started.\n",
        "\n",
        "If you are using a computer with Windows, the terminal instructions may not apply.\n",
        ":::\n",
        "\n",
        "Python is installed by default on MacOS and most Linux distributions.\n",
        "However, it can be challenging to navigate between the versions and packages that your operating system uses and those needed for other projects.\n",
        "Thus, there are a variety of version, package, and environment management tools:\n",
        "\n",
        "- **Version management**: Which version of Python are you using? Can you change versions to run a specific Python program if it requires?\n",
        "    - `pyenv`\n",
        "    - `conda`/`mamba`\n",
        "- **Package management**: How can you install the many amazing Python packages people have created?\n",
        "    - `pip`\n",
        "    - `conda`/`mamba`\n",
        "- **Environment management**: If you have two projects that require different packages (or different versions of the same package), can you switch which packages are available depending on which project you are working on?\n",
        "    - `venv`\n",
        "    - `virtualenv`\n",
        "    - `poetry`\n",
        "    - `conda`/`mamba`\n",
        "    - many more\n",
        "    \n",
        "The `conda` package manager is the only one that fills all three roles.\n",
        "It is formally a part of the Anaconda Python distribution which is a favorite in the fields of data science and machine learning.\n",
        "`mamba` is a newer and faster rewrite used in exactly the same way and which is highly recommended.\n",
        "\n",
        "The best way to get started with `mamba` is to install `mambaforge`.\n",
        "You can find installer downloads for Windows, MacOS, or Linux [here](https://github.com/conda-forge/miniforge#mambaforge).\n",
        "\n",
        "For Windows, run the `.exe` file once it is downloaded.\n",
        "\n",
        "For MacOS and Linux, open a terminal and navigate to the download location:\n",
        "```bash\n",
        "cd ~/Downloads\n",
        "```\n",
        "Then run the installer as follows:\n",
        "```bash\n",
        "./Mambaforge-Linux-x86_64.sh\n",
        "```\n",
        "The installer will walk you through a few steps and end by asking if you'd like to \"initialize Mambaforge by running conda init?\"\n",
        "Answer yes and restart your terminal.\n",
        "This final command will have added `conda` and `mamba` to your system `$PATH` variable, which means it is available to your terminal.\n",
        "Once restarted, run `mamba -V` to print the version and to verify that the installation worked.\n",
        "\n",
        "### Environments\n",
        "The idea of a `conda`/`mamba` environment is that once an environment is created and activated, all new packages installed will be added to that environment and will be accessible to any Python program run while the environment is active.\n",
        "As an example, let's create an environment called `workshop` with a specific version of Python installed.\n",
        "The following will create the environment and install a specific version of `python`:\n",
        "```bash\n",
        "mamba create -n workshop python=3.9\n",
        "```\n",
        "Once created, we can list our environments via the command\n",
        "```bash\n",
        "mamba env list\n",
        "```\n",
        "```\n",
        "# conda environments:\n",
        "#\n",
        "base                     /home/user/mambaforge\n",
        "workshop                 /home/user/mambaforge/envs/workshop\n",
        "```\n",
        "Note that there is a \"base\" environment which is where `conda` and `mamba` themselves are installed as well as their dependencies.\n",
        "The best practice is to create an environment for each of your projects to minimize dependency issues (when packages require separate versions of the same package).\n",
        "\n",
        "To activate our new environment:\n",
        "```bash\n",
        "mamba activate workshop\n",
        "```\n",
        "Running `mamba env list` will now show our active environment via an asterisk:\n",
        "```\n",
        "base                     /home/user/mambaforge\n",
        "workshop              *  /home/user/mambaforge/envs/workshop\n",
        "```\n",
        "\n",
        "### Installing packages\n",
        "Now that we have activated the `workshop` `conda` environment, let's install some common machine learning packages in Python.\n",
        "It is as easy as writing:\n",
        "```bash\n",
        "mamba install numpy matplotlib pandas jupyter scipy scikit-learn scikit-image\n",
        "```\n",
        "This command will search the [conda-forge](https://conda-forge.org/#page-top) repository of packages and install the most up-to-date versions (the `forge` in `mambaforge`).\n",
        "\n",
        ":::{.callout-tip}\n",
        "Either `conda` or `mamba` could be used for all the commands discussed in this section.\n",
        "However, `mamba` is significantly faster when installing packages.\n",
        ":::\n",
        "\n",
        "Now that these packages have been installed, we can easily use them in an interactive `ipython` prompt (installed with the `jupyter` package):\n",
        "```bash\n",
        "ipython\n",
        "```\n",
        "```default\n",
        "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)\n",
        "Type 'copyright', 'credits' or 'license' for more information\n",
        "IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n",
        "\n",
        "In [1]: import numpy as np\n",
        "\n",
        "In [2]: import matplotlib.pyplot as plt\n",
        "\n",
        "In [3]: x = np.linspace(0,10,100)\n",
        "\n",
        "In [4]: y = np.sin(x)\n",
        "\n",
        "In [5]: plt.plot(x,y); plt.show()\n",
        "```\n",
        "This should plot a simple $\\sin$ curve.\n",
        "\n",
        "### Cleaning up\n",
        "After we are done using the environment that has our desired version of Python and the needed packages, we can go back to our regular terminal by deactivating the environment:\n",
        "```bash\n",
        "mamba deactivate workshop\n",
        "```\n",
        "If we have somehow broken our environment and need to remove it:\n",
        "```bash\n",
        "mamba env remove -n workshop\n",
        "```\n",
        "There are many more commands and functionalities that `conda` and `mamba` provide that can be found in the [python resources and packages](#python-resources-and-packages) section of the Appendix.\n",
        "\n",
        "### Google Colab\n",
        "As an alternative to the entire procedure above, you can use an online [Jupyter Notebook](https://jupyter.org) service hosted by Google called [Colab](https://colab.research.google.com).\n",
        "This service will get you up and running immediately but cannot save your environment between notebooks and has limited functionality to run scripts, save data, view animations, change package versions, etc.\n",
        "Thus, if your notebook requires a package that is not installed by default, you will need to add the installation command in one of the first notebook cells.\n",
        "For example, to install the [reservoirpy]() package, we would write in a notebook cell:\n",
        "```default\n",
        "!pip install reservoirpy\n",
        "```\n",
        "In a notebook, the `!` denotes a terminal command.\n",
        "The package will now be ready for import and use within the current notebook session:\n",
        "```default\n",
        "from reservoirpy.nodes import Input,Reservoir,Ridge\n",
        "```\n",
        "\n",
        "## Basic concepts of machine learning \n",
        "\n",
        "Machine learning is, at its most basic, automated data analysis, usually with the goal of finding patterns or making predictions.\n",
        "The \"machines\" in this analysis are equations or algorithms and the \"learning\" is usually some form of parameter selection and/or fitting.\n",
        "Due to the uncertain nature of most data, the majority of these models are probabilistic in nature.\n",
        "In fact, it can be hard to distinguish the methodological lines between what is termed \"machine learning\" and the field of statistics.\n",
        "However, there are a few important distinctions between the tools, goals, and terminology of the two areas.\n",
        "Today, machine learning has emerged as a broad description of almost any data-driven computing which may or may not include classical descriptive and inferential statistics[@murphy2012machine].\n",
        "\n",
        "At first glance, machine learning can be separated into three main classes: \n",
        "\n",
        "- **Supervised learning**: Given dependent and independent variable data, train a model which effectively maps the independent variable data to produce the dependent variable data.\n",
        "    - Generalized linear models (linear, logistic, etc. regression)\n",
        "    - Naive Bayes\n",
        "    - Neural networks (most)\n",
        "    - Support vector machine (SVM)\n",
        "    - Random forests\n",
        "    - etc.\n",
        "- **Unsupervised learning**: Given data, find patterns (no specified output, though there is still a measure of success)\n",
        "    - Clustering\n",
        "    - Mixture models\n",
        "    - Dimensionality reduction\n",
        "    - Association rules\n",
        "    - etc.\n",
        "- **Reinforcement learning**: Given input data and desired outcomes, simulate and use the results to update a model to improve the simulation's ability to achieve those outcomes\n",
        "    - Q-learning\n",
        "    - SARSA\n",
        "    - etc.\n",
        "\n",
        "There is an enormous amount of current interest in machine learning methods and there is a corresponding amount of high-quality material discussing it.\n",
        "We will end the introduction here and direct you to established textbooks[@james2013introduction;@hastie2009elements;@murphy2012machine], NJIT classes (Math 478, Math 678, Math 680, CS 675, CS 677), and online resources (too many to even start listing).\n",
        "\n",
        "### General procedure\n",
        "In practice, machine learning algorithms often boil down to an optimization problem.\n",
        "To characterize this in a few steps, consider a problem with data $x$:\n",
        "\n",
        "1. Select a model representation $f$ with parameters $p$ for the problem:\n",
        "$$\n",
        "y = f(x;p)\n",
        "$$\n",
        "2. Determine an appropriate objective function $\\mathcal{L}$:\n",
        "$$\n",
        "\\mathcal{L}(f(x;p),x)\n",
        "$$\n",
        "3. Use an optimization method $\\mathcal{O}$ with parameters $d$ to find parameters $p$ that minimize or maximize the objective for the model:\n",
        "$$\n",
        "p^* = \\mathcal{O}(\\mathcal{L},f,x;d)\n",
        "$$\n",
        "\n",
        "In some sense, this is the same procedure used for _inverse problems_ in traditional applied mathematics but with a broader set of models $f$ that may or may not be based on first-principles understanding of the problem.\n",
        "\n",
        "### Incorporating data\n",
        "Depending on the class of problem considered (supervised, unsupervised, or reinforcement), there are a variety of choices for models $f$ and objectives $\\mathcal{L}$.\n",
        "For supervised learning (the most common), the objective is often to predict or generate the output or dependent variable data of some process.\n",
        "For this, data can be separated into three sets:\n",
        "\n",
        "1. **Training data** ($x$): used to tune the parameters $p$\n",
        "2. **Validation data** ($x^v$): used to evaluate the generalization of the model $f$ to data not in the training set during training\n",
        "3. **Testing data** ($x^t$): used to benchmark the predictive or generative ability of the model after training is completed\n",
        "\n",
        "## A first machine learning problem\n",
        "\n",
        ":::{.callout-note}\n",
        "The code for this problem will require the following packages: `numpy`, `matplotlib`, `autograd`\n",
        ":::\n",
        "\n",
        "These workshops are about learning by doing, so let's build understanding by fitting a simple \"machine\" to some data as a supervised problem.\n",
        "Consider some data $(x,y)$:\n",
        "<!-- Consider some data $(x,y)$ generated with the function: -->\n",
        "<!-- $$ -->\n",
        "<!-- y = e^{-x}(\\sin(x^3) + \\sin(x^2) - x) + \\frac{1}{1 + e^{-(x-1)}} -->\n",
        "<!-- $$ -->"
      ],
      "id": "671c7294"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Data generation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# To see animations in a Jupyter notebook, uncomment the following line:\n",
        "# %matplotlib notebook\n",
        "def f_known(x):\n",
        "    part1 = np.exp(-x)*(np.sin((x)**3) + np.sin((x)**2) - x)\n",
        "    part2 = 1/(1 + np.exp(-1*(x-1)))\n",
        "    return part1 + part2\n",
        "xsamples = np.random.uniform(-1/2,5,100)\n",
        "ysamples = f_known(xsamples)\n",
        "plt.scatter(xsamples,ysamples)\n",
        "plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Data\")\n",
        "plt.show()"
      ],
      "id": "d13a8f38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We would like to fit a model of the following form to this data:\n",
        "$$\n",
        "f(x;p_0,p_1) = e^{-p_0x}(\\sin((p_0x)^3) + \\sin((p_0x)^2) - p_0x) + \\frac{1}{1 + e^{-p_1(x-1)}}\n",
        "$$\n",
        "<!-- which looks like the following for $p_0=1,p_1=1$: -->\n",
        "\n",
        "To formulate this as a machine learning/optimization problem, we can consider an objective/loss to minimize the $L^2$ norm distance between the model output $f(x)$ and the true data $y$:\n",
        "$$\n",
        "\\mathcal{L(f(x;\\vec{p}),y)} = ||f(x) - y||_2^2\n",
        "$$\n",
        "The problem can then be written as the unconstrained optimization problem:\n",
        "$$\n",
        "p^* = \\underset{\\vec{p}}{\\text{minimize }} \\mathcal{L}(f(x;\\vec{p}),y)\n",
        "$$\n",
        "We then expect our model $f(x;p^*)$ to represent a \"machine\" that has accurately \"learned\" the relationship between $x$ and $y$.\n",
        "\n",
        "There are several ways to approach this problem, but a simple and popular approach for a continuous and unconstrained problem is to use an iterative gradient method.\n",
        "\n",
        "### Gradient descent\n",
        "_Gradient descent_ is a straightforward method taught early in an undergraduate numerical methods class.\n",
        "Its simplicity and relatively low computational cost has made it popular for machine learning methods (which can contain enough parameters that second-order methods, like Newton's method, are infeasibly expensive because of the Hessian computation).\n",
        "Beginning with an initial parameter guess $\\vec{p}_0$, its update procedure can be written as:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n",
        "v^i &= -\\alpha \\nabla_p \\mathcal{L}\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\alpha$ controls the step size in the direction of the gradient (usually called a \"learning rate\" in machine learning).\n",
        "This method will follow the gradient of the objective/loss $\\mathcal{L}$ until the objective is sufficiently small, or until it reaches a steady state.\n",
        "\n",
        "Simply implemented in Python, this method can be written as:"
      ],
      "id": "7a1763fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    for s in range(steps):\n",
        "        v_i = -alpha*f_p(x)\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "4ad376ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, this method contains a troublesome parameter $\\alpha$ which, if chosen too large, could prevent convergence of the solution or, if chosen too small, could require an unreasonable number of steps to converge.\n",
        "The method itself is also prone to terminate in local minima rather than in a global minimum unless the correct initial guess and learning rate are chosen.\n",
        "For this reason, \"vanilla\" (or normal) gradient descent is almost always replaced with a modified method in learning problems[@sebastian_ruder_2020;@john_chen_2020].\n",
        "\n",
        "The following demonstrates an animation of the above gradient descent method applied to our data with two different learning rates, one successful, one not.\n",
        "It uses the following animation code and the `autograd` automatic differentiation library that will be further discussed later:"
      ],
      "id": "a78569d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Animation code\n",
        "from matplotlib import animation as anim\n",
        "from matplotlib import gridspec\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# To display the animations in a jupyer notebook uncomment the following line:\n",
        "# %matplotlib notebook\n",
        "\n",
        "def animate_steps_2d(xs,loss,xmin=-.1,xmax=2.5,ymin=-1,ymax=3,interval=50):\n",
        "    fig = plt.figure(figsize=(10,6),constrained_layout=True)\n",
        "    gs = gridspec.GridSpec(ncols=6,nrows=2,figure=fig)\n",
        "    ax = fig.add_subplot(gs[:,0:4],projection=\"3d\")\n",
        "    ax1 = fig.add_subplot(gs[0,4:])\n",
        "    ax2 = fig.add_subplot(gs[1,4:])\n",
        "    ax.view_init(47,47)\n",
        "    ax.set_xlabel(\"$p_0$\"); ax.set_ylabel(\"$p_2$\"); ax.set_zlabel(\"loss\",rotation=90)\n",
        "    ax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"loss\")\n",
        "    ax2.set_xlabel(\"$p_1$\"); ax2.set_ylabel(\"loss\")\n",
        "\n",
        "    xs_arr = np.array(xs)\n",
        "    fxs = np.linspace(xmin,xmax,100)\n",
        "    fys = np.linspace(ymin,ymax,100)\n",
        "    loss_fx = [loss([fxs[j],xs[0][1]]) for j in range(len(fxs))]\n",
        "    loss_fy = [loss([xs[0][0],fys[j]]) for j in range(len(fys))]\n",
        "    X,Y = np.meshgrid(fxs,fys)\n",
        "    Z = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Z[i,j] = loss([X[i,j],Y[i,j]])\n",
        "    # Add surface plot\n",
        "    surf = ax.plot_surface(X,Y,Z,cmap=\"gist_earth\")\n",
        "    ax1.set_xlim(np.min(X),np.max(X)); ax1.set_ylim(np.min(Z),np.max(Z))\n",
        "    ax2.set_xlim(np.min(Y),np.max(Y)); ax2.set_ylim(np.min(Z),np.max(Z))\n",
        "    plot1 = ax.plot(xs[0][0],xs[0][1],loss(xs[0]),zorder=100,color=\"red\",linestyle=\"\",marker=\"o\")[0]\n",
        "    plot2 = ax.plot([],[],[],color=\"orange\")[0]\n",
        "    # Add flat plots for perspective\n",
        "    plot3 = ax1.plot(fxs,loss_fx)[0]\n",
        "    plot4 = ax1.scatter(xs[0][0],loss(xs[0]),color=\"red\",s=100,zorder=100)\n",
        "    plot5 = ax2.plot(fys,loss_fy)[0]\n",
        "    plot6 = ax2.scatter(xs[0][1],loss(xs[0]),color=\"red\",s=100,zorder=100)\n",
        "    def anim_func(i):\n",
        "        x_loss = loss(xs[i])\n",
        "        plot1.set_data_3d(xs[i][0],xs[i][1],x_loss)\n",
        "        temp_x1 = [xs[j][0] for j in range(i)]\n",
        "        temp_x2 = [xs[j][1] for j in range(i)]\n",
        "        temp_losses = [loss(xs[j]) for j in range(i)]\n",
        "        plot2.set_data_3d(temp_x1,temp_x2,temp_losses)\n",
        "        loss_fx = [loss([fxs[j],xs[i][1]]) for j in range(len(fxs))]\n",
        "        loss_fy = [loss([xs[i][0],fys[j]]) for j in range(len(fys))]\n",
        "        plot3.set_data(fxs,loss_fx)\n",
        "        plot4.set_offsets([xs[i][0],x_loss])\n",
        "        plot5.set_data(fys,loss_fy)\n",
        "        plot6.set_offsets([xs[i][1],x_loss])\n",
        "        plots = [plot1,plot2,plot3,plot4,plot5,plot6]\n",
        "        return plots\n",
        "\n",
        "    tanim = anim.FuncAnimation(fig,anim_func,interval=50,frames=len(xs),blit=True)\n",
        "    return tanim"
      ],
      "id": "5efdcc34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autograd import grad\n",
        "import autograd.numpy as anp\n",
        "def f_model(p):\n",
        "    part1 = anp.exp(-p[0]*xsamples)*(anp.sin((p[0]*xsamples)**3) + anp.sin((p[0]*xsamples)**2) - p[0]*xsamples) \n",
        "    part2 = 1/(1 + anp.exp(-p[1]*(xsamples-1)))\n",
        "    return part1 + part2\n",
        "loss = lambda p: anp.sum((f_model(p) - ysamples)**2)\n",
        "grad_loss = grad(loss) # automatically differentiated"
      ],
      "id": "a0fcd0f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = gradient_descent(grad_loss,p0,.005,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "f770343d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/gradient_descent1.gif)\n",
        "\n",
        "Although this behavior is somewhat typical of vanilla gradient descent, this model was pathologically chosen to be challenging.\n",
        "The curvature of the loss function for each parameter at the correct parameter values are as follows:"
      ],
      "id": "845c3524"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Plotting parameter loss curves\n",
        "ps = np.linspace(-.1,2.5,1000)\n",
        "fig,axs = plt.subplots(1,2,sharey=True,figsize=(9,4))\n",
        "ax1,ax2 = axs\n",
        "ax1.plot(ps,[loss(np.array([p_i,1])) for p_i in ps])\n",
        "ax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"Loss\")\n",
        "ax2.plot(ps,[loss(np.array([1,p_i])) for p_i in ps])\n",
        "ax2.set_xlabel(\"$p_1$\")\n",
        "# plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Model\")\n",
        "plt.show()"
      ],
      "id": "91b9b597",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this loss, $p_0$ demonstrates a plethora of local minima which could trap the descent algorithm while $p_1$ has a small gradient which will slow down the convergence.\n",
        "The following methods are meant to simplify the choice of a learning rate while overcoming these specific convergence issues.\n",
        "\n",
        "### Adaptive steps\n",
        "Due to the challenges of determining a good learning rate (especially in models with many parameters and large variance in loss gradients), many methods have been developed to automatically adjust the $\\alpha$ parameter with each step and for each parameter.\n",
        "One of the most common adaptive algorithms is called `adagrad` (for adaptive gradient. very creative).\n",
        "Originally developed to provide parameter specific learning rates in sparse problems (in the case that some parameters are only occasionally important) it scales the learning rate by a squared sum of previous gradients:\n",
        "\n",
        "**adagrad:**\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n",
        "v^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\n",
        "G^i &= \\sum_{j=0}^i (\\nabla_p \\mathcal{L}(x;\\vec{p}^j))^2\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "c7eebded"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: adagrad code\n",
        "def adagrad(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    # --------- NEW -----------\n",
        "    sum_sq_grad = 0\n",
        "    for s in range(steps):\n",
        "        sum_sq_grad = f_p(x)**2 + sum_sq_grad\n",
        "        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n",
        "    # -------------------------\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "c00a3366",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = adagrad(grad_loss, p0,.2,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "eeb497ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/adagrad.gif)\n",
        "However, depending on the problem, this scaled learning rate may slow down convergence considerably.\n",
        "As an adjustment to remove this monotone decreasing learning rate, `RMSprop` attempts to balance the current gradient with a dampened version of the sum of squares of previous gradients from `adagrad`:\n",
        "\n",
        "**RMSprop:**\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n",
        "v^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\n",
        "G^i &= \\gamma G^{i-1} + (1-\\gamma)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "7aeeeee3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: RMSprop code\n",
        "def rmsprop(f_p,x0,gamma=0.9,alpha=0.001,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    # --------- NEW -----------\n",
        "    sum_sq_grad = 0\n",
        "    for s in range(steps):\n",
        "        sum_sq_grad = (1-gamma)*(f_p(x)**2) + gamma*sum_sq_grad\n",
        "        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n",
        "    # -------------------------\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "801860e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = rmsprop(grad_loss, p0,0.2,.05,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "29008312",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/rmsprop.gif)\n",
        "This helps with the challenge of a monotone decreasing learning rate, but it introduces a dampening parameter $\\gamma$ that must be chosen (recommended values are $\\alpha = 0.001$ and $\\gamma = 0.9$).\n",
        "\n",
        "\n",
        "### With momentum\n",
        "Though the previous adaptive methods address the challenge of determining a learning rate, these methods are still likely to terminate in local minima.\n",
        "To address this issue, there are several methods which utilize a concept of \"momentum\" to propel iterations out of local minima with the hope of landing in the global minimum.\n",
        "This momentum is most simply added by incorporating previous gradients into the current update:\n",
        "\n",
        "**Gradient descent with momentum:**\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n",
        "    v^i &= -\\alpha G^i \\\\\n",
        "    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i) + \\gamma G^{i-1}\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "88021e02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Gradient descent with momentum code\n",
        "def gradient_descent_momentum(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    # --------- NEW -----------\n",
        "    sum_grad = 0\n",
        "    for s in range(steps):\n",
        "        sum_grad = f_p(x) + gamma*sum_grad\n",
        "        v_i = -alpha*sum_grad\n",
        "    # -------------------------\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "a639a56e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = gradient_descent_momentum(grad_loss, p0,.9,.005,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "283274e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/momentum.gif)\n",
        "where $\\gamma$ is a momentum parameter that determines how much of previous updates are kept for the current step ($0 <= \\gamma <=1$ where $\\gamma = 0$ includes no momentum).\n",
        "\n",
        "However, iterations that include this momentum may jump right out of global minima and/or delay convergence.\n",
        "To incorporate a counterbalance to the momentum based on current success (to slow down in the right places), `Nesterov acceleration` adjusts the gradient according to an approximated step (to see how successful it may be in the future).\n",
        "By so doing, it can effectively reduce or increase the momentum according to the next future iteration:\n",
        "\n",
        "**Nesterov accelerated gradient descent:**\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n",
        "    v^i &= -\\alpha G^i \\\\\n",
        "    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i - \\gamma v^{i-1}) + \\gamma G^{i-1}\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "5fc08b52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Gradient descent with Nesterov acceleration code\n",
        "def gradient_descent_nesterov(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    # --------- NEW -----------\n",
        "    sum_grad = 0\n",
        "    for s in range(steps):\n",
        "        sum_grad = f_p(x-gamma*v_i) + gamma*sum_grad\n",
        "        v_i = -alpha*sum_grad\n",
        "    # -------------------------\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "97ac8218",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = gradient_descent_nesterov(grad_loss, p0,.8,.002,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "299c58e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/nesterov.gif)\n",
        "where again $\\gamma$ is a momentum parameter.\n",
        "Notice that the only adjustment compared to vanilla gradient descent with momentum is calculating the gradient at an approximation of the next parameter values rather than at the current parameters.\n",
        "\n",
        "### Combining ideas\n",
        "Of course, the ideas of adaptive gradients and momentum both address important issues and are not mutually exclusive, so they can both be used simultaneously (with the downside of adding more parameters to tune).\n",
        "Note that the adaptive step size methods (`adagrad`,`RMSprop`) use a sum of squares of the previous gradient while the momentum methods (gradient descent with momentum or Nesterov acceleration) use a sum of the previous gradient.\n",
        "These can be seen as the second moment and first moment of the previous gradients respectively.\n",
        "The `Adam` (adaptive moment estimation) method uses both of these gradient moments to incorporate both momentum and adaptivity:\n",
        "\n",
        "**adam:**\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\ \n",
        "    v^i &= -\\frac{\\alpha}{\\sqrt{b^i}}m^i \\\\\n",
        "    m^i &= \\beta_1m^{i-1} + (1-\\beta_1)\\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\n",
        "    b^i &= \\beta_2b^{i-1} + (1-\\beta_2)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "b1b2dc72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Adam code\n",
        "def adam(f_p,x0,beta1,beta2,alpha=0.01,tol=1e-12,steps=1000):\n",
        "    x = x0\n",
        "    xs = [x]\n",
        "    # --------- NEW -----------\n",
        "    sum_grad = 0\n",
        "    sum_sq_grad = 0\n",
        "    for s in range(1,steps):\n",
        "        sum_grad = beta1*sum_grad + (1-beta1)*f_p(x)\n",
        "        sum_sq_grad = beta2*sum_sq_grad + (1-beta2)*(f_p(x)**2)\n",
        "        v_i = -alpha*sum_grad/np.sqrt(sum_sq_grad)\n",
        "    # -------------------------\n",
        "        xnew = x + v_i\n",
        "        if np.linalg.norm(f_p(xnew)) < tol:\n",
        "            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        elif np.linalg.norm(xnew - x) < tol:\n",
        "            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n",
        "            return x,xs\n",
        "        x = xnew\n",
        "        xs.append(x)\n",
        "    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n",
        "    return x,xs"
      ],
      "id": "e8fd1e4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "p0 = np.array([2.1,.2])\n",
        "xs = adam(grad_loss, p0,0.9,0.99,.05,tol=1e-8,steps=100)[1]\n",
        "animate_steps_2d(xs,loss)"
      ],
      "id": "43eb4a0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](videos/adam.gif)\n",
        "Parameters $\\beta_1,\\beta_2$ represent decay rates in incorporating previous moments into the update step.\n",
        "\n",
        "Although this list is not comprehensive, it demonstrates the reasoning behind the common solutions for the challenges of using gradient descent in learning methods.\n",
        "For a more comprehensive list of recently proposed methods, see[@john_chen_2020].\n",
        "\n",
        "### Automatic differentiation\n",
        "Gradient descent methods rely on computing the gradient of the loss $\\nabla_p \\mathcal{L}$ for a given parameter set $\\vec{p}^i$.\n",
        "For simple models, the gradients can be calculated by hand.\n",
        "However, for models with many nested functions and parameters (neural networks in particular) or methods whose form depends on hyperparameters, we will need an automated method.\n",
        "The \"automatic differentiation\" method is the solution to these challenges.\n",
        "Though it was developed for neural networks and mainly used there, its breadth of applications is only just becoming apparent.\n",
        "(Consider the Julia language which has worked hard to make automatic differentiation possible everywhere).\n",
        "\n",
        "There are both \"forward\" and \"backward\" automatic differentiation methods, but we will consider only the forward which best demonstrates the \"automatic\" moniker.\n",
        "To do so, consider the first order expansion of two functions at a given point $a$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "f(a + \\epsilon) &= f(a) + \\epsilon f'(a) \\\\\n",
        "g(a + \\epsilon) &= g(a) + \\epsilon g'(a)\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\epsilon$ is a small perturbation.\n",
        "Basic operations with these functions at this point can then be written as:\n",
        "$$\n",
        "\\begin{align*}\n",
        "f + g &= [f(a) + g(a)] + \\epsilon[f'(a) + g'(a)] \\\\\n",
        "f - g &= [f(a) - g(a)] + \\epsilon[f'(a) - g'(a)] \\\\\n",
        "f \\cdot g &= [f(a) \\cdot g(a)] + \\epsilon[f(a)\\cdot g'(a) + g(a)\\cdot f'(a)] \\\\\n",
        "f \\div g &= [f(a) \\div g(a)] + \\epsilon\\left[\\frac{f'(a)\\cdot g(a) + f(a)\\cdot g'(a)}{g(a)^2}\\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "The derivatives are then represented by the $\\epsilon$ terms in the above equalities.\n",
        "\n",
        "To implement this on a computer, you can create a new \"Dual\" number that performs additions, subtractions, multiplications, and divisions with the above operating rules (and can be extended for other functions: $\\sin,\\cos,\\exp$,etc).\n",
        "\n",
        ":::{.callout-note}\n",
        "This is cumbersome and slow to implement in Python which is why large and complicated libraries have been written in C (`tensorflow`,`pytorch`,`jax`, and others).\n",
        "\n",
        "It can be cleanly and easily implemented in Julia.\n",
        "To see a demonstration of this, see[@chris_rackauckas_2020].\n",
        ":::\n",
        "\n",
        "A quick demonstration of a simple implementation of this is as follows[@mostafa_samir_2020].\n",
        "Define a dual number class:"
      ],
      "id": "caa4e21e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DualNumber:\n",
        "    def __init__(self, val, der):\n",
        "        self.val = val\n",
        "        self.der = der\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, DualNumber):\n",
        "            # DualNumber + DualNumber\n",
        "            return DualNumber(self.val + other.val, self.der + other.der)\n",
        "        else:\n",
        "            # DualNumber + a number\n",
        "            return DualNumber(self.val + other, self.der)\n",
        "    def __radd__(self, other):\n",
        "        # a number + DualNumber\n",
        "        return self.__add__(other)\n",
        "    def __mul__(self,other):\n",
        "        if isinstance(other, DualNumber):\n",
        "            # DualNumber * DualNumber\n",
        "            return DualNumber(self.val * other.val, self.val*other.der + self.der*other.val)\n",
        "        else:\n",
        "            # DualNumber * a number\n",
        "            return DualNumber(self.val * other, self.der * other)\n",
        "    def __rmul__(self,other):\n",
        "        # a number * DualNumber\n",
        "        return self.__mul__(other)\n",
        "    def __pow__(self,power):\n",
        "        # DualNumber ^ a number\n",
        "            return DualNumber(self.val ** power, self.der * power * (self.val ** (power - 1)))\n",
        "    def __repr__(self):\n",
        "        # Printing a DualNumber\n",
        "        return \"DualNumber({},{})\".format(self.val,self.der)\n",
        "dual1 = DualNumber(1,2); dual2 = DualNumber(3,4); other = 5\n",
        "print(dual1,\"+\",dual2,\"=\",dual1+dual2)\n",
        "print(dual1,\"+\",other,\"=\",dual1+other)\n",
        "print(dual1,\"*\",dual2,\"=\",dual1*dual2)\n",
        "print(dual1,\"*\",other,\"=\",dual1*other)\n",
        "print(dual1,\"^\",2,\"=\",dual1**2)"
      ],
      "id": "5a6ff167",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This seems simple, but by using a few rules like this, we can compute simple polynomial derivatives at a point automatically.\n",
        "For example, computing the derivative of $f(x) = 3x^3 + 2x + 4$ at $x=2$.\n",
        "We first initialize the function and the dual number $(2,1)$ ($1$ because the derivative of $x$ is $1$)."
      ],
      "id": "e8d0cb04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f = lambda x: 3*x**3 + 2*x + 4\n",
        "df = lambda x: 9*x**2 + 2\n",
        "x = 2; dualx = DualNumber(x,1)"
      ],
      "id": "e63e9c2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then pass the dual number through the polynomial function, unpack the results, and compare the result with the true derivative $f'(2)$:"
      ],
      "id": "f3436956"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "poly_dual = f(dualx)\n",
        "dual_val = poly_dual.val; dual_der = poly_dual.der\n",
        "true_der = df(x)\n",
        "print(\"True derivative at x=2: \", true_der)\n",
        "print(\"Dual number derivative: \", dual_der)"
      ],
      "id": "1b5a4c74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the rules for dual numbers we provided above, the derivative \"automatically\" popped out of the polynomial evaluation.\n",
        "\n",
        "In contrast with the forward method, the backward method keeps a log of the operations performed and then uses defined chain rules (similar to how we defined rules for the forward pass) to backtrack from the final output back to the start. \n",
        "This approach is more efficient when the dimension of the gradient is large and that of the output small.\n",
        "Forward evaluation is more efficient when the dimension of the gradient is small and that of the output large.\n",
        "\n",
        "All in all, automatic differentiation is programming with hard-coded differential rules for primitive operations.\n",
        "It can also require keeping track of which operations happen and in what order.\n",
        "Implementing such a method relies heavily on computer science techniques since it boils down to parsing operation calls.\n",
        "This represents one of the clearest differences in the approaches of machine learning to those of statistics or optimization due to its computer language-heavy principles.\n",
        "Through this lens, machine learning could be viewed as \"inferential statistics using new tools from computer science for non-traditional problems.\"\n",
        "\n",
        "## Further Exploration\n",
        "\n",
        "1. Each of the parameters for the gradient descent methods was chosen by hand for the above animations. Try adjusting the starting point and parameters for each to get a feel for how they behave.\n",
        "2. Try splitting the sample data into a training and a testing set. Use the training set and one of the gradient descent methods to fit some parameters, then see how well the model generalizes to the testing set with those parameters.\n",
        "3. Try implementing another gradient descent method from[@sebastian_ruder_2020] using the given methods as a template.\n",
        "4. Add the missing subtraction (`__sub__`, `__rsub__`) and division (`__truediv__`, `__rtruediv__`) methods in the `DualNumber` class and play around with automatically differentiation functions using those operators (you can use [this](https://github.com/Mostafa-Samir/Hands-on-Intro-to-Auto-Diff/blob/master/dualnumbers/dualnumbers.py) as a reference). You can also make functions such as `sin`,`cos`, or `exp` that are meant to compute with `DualNumber`s.\n",
        "\n",
        "## Appendix {.appendix}\n",
        "\n",
        "### Python resources and packages {.appendix}\n",
        "**Python help**\n",
        "\n",
        "- [Python documentation (especially sections 3-9)](https://docs.python.org/3.9/tutorial/introduction.html)\n",
        "- [Quick cheatsheet of general Python knowledge](https://www.pythoncheatsheet.org)\n",
        "- [Quicker introduction](https://learnxinyminutes.com/docs/python/)\n",
        "\n",
        "**Conda/mamba help**\n",
        "\n",
        "- [Conda user guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html)\n",
        "- [Mamba website](https://mamba.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "**Python essential packages**\n",
        "\n",
        "- [`numpy`](https://numpy.org) - Creating, manipulating, and operating (linear algebra, fft, etc.) on multi-dimensional arrays. A list of packages built on `numpy` for a variety of domains can be found on the homepage under the _ECOSYSTEM_ heading.\n",
        "- [`scipy`](https://scipy.org) - Fundamentals in optimization, integration, interpolation, differential equations, statistics, signal processing, etc.\n",
        "- [`matplotlib`](https://matplotlib.org) - Static or interactive plots and animations\n",
        "- [`scikit-learn`](https://scikit-learn.org/stable/index.html) - Standard machine learning tools and algorithms built on `numpy`, `scipy`, and `matplotlib`\n",
        "- [`pandas`](https://pandas.pydata.org) - Easily represent, manipulate, and visualize structured datasets (matrices with names for columns and rows)\n",
        "- [`keras`](https://keras.io) - High level neural network framework built on `tensorflow`\n",
        "- [`tensorflow`](https://www.tensorflow.org) - In depth neural network framework focused on ease and production\n",
        "- [`pytorch`](https://pytorch.org) - In depth neural network framework focused on facilitating the path from research to production\n",
        "- [`scikit-image`](https://scikit-image.org) - Image processing algorithms and tools\n",
        "- [`jupyter`](https://jupyter.org) - Interactive \"notebook\" style programming\n",
        "\n",
        "### Julia as an alternative to Python {.appendix}\n",
        "Julia is a fairly new language that has been mainly proposed as an alternative to Python and Matlab, though it is general use.\n",
        "Its strength and its weakness is that it is \"just-in-time\" compiled (meaning your code is automatically analyzed and compiled just before it is run).\n",
        "A clever language design combined with just-in-time compilation makes Julia as clear to read and write as Python while being much faster.\n",
        "It can even approach the speed of C when written carefully.\n",
        "However, the just-in-time compilation and type system remove a chunk of the interactive convenience of Python and its young age also means that it does not have the volume of packages that Python does.\n",
        "\n",
        "Nonetheless, it is an elegant and high-performance language to use and has shown rapid growth recently.\n",
        "Concise, simple, and easy to read and contribute to packages have been quickly emerging and it already provides many useful tools.\n",
        "As a result, it is worth describing it's installation process, environment management, and noteable packages.\n",
        "\n",
        "#### Installation {.appendix}\n",
        "The officially supported method of installation for Julia is now using the `juliaup` version manager.\n",
        "The [installer](https://github.com/JuliaLang/juliaup#windows) can be downloaded from the Windows store on Windows or run on MacOS or Linux with:\n",
        "\n",
        "```bash\n",
        "curl -fsSL https://install.julialang.org | sh\n",
        "```\n",
        "#### Environments {.appendix}\n",
        "Julia comes with a standard environment and package manager named [`Pkg`](https://pkgdocs.julialang.org/v1/).\n",
        "Interestingly, the easiest way to use it is to run the Julia REPL (read-eval-print-loop), i.e. to run `julia` interactively.\n",
        "You can do so by typing `julia` into the terminal.\n",
        "You will then be presented with a terminal interface such as:\n",
        "```default\n",
        "   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n",
        "  (_)     | (_) (_)    |\n",
        "   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n",
        "  | | | | | | |/ _` |  |\n",
        "  | | |_| | | | (_| |  |  Version 1.8.0 (2022-08-17)\n",
        " _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n",
        "|__/                   |\n",
        "\n",
        "julia>\n",
        "```\n",
        "Typing `]` will put you into \"`Pkg` mode\":\n",
        "```default\n",
        "(@v1.8) pkg>\n",
        "```\n",
        "Type `?` and hit enter to get options in this mode.\n",
        "We can create and activate a new environment called workshop with the command:\n",
        "```default\n",
        "(@v1.8) pkg> activate --shared workshop\n",
        "```\n",
        "Note that the `--shared` flag will make a \"global\" environment that can be accessed from any directory.\n",
        "If we were to leave out this flag, `Pkg` would put a `Project.toml` and `Manifest.toml` file in the current directory that contain the name of the environment, its installed packages, and their dependencies.\n",
        "This can be useful to easily isolate and share environments.\n",
        "After running this command, our `Pkg` mode will have changed to represent the active environment:\n",
        "```default\n",
        "(@workshop) pkg>\n",
        "```\n",
        "\n",
        "#### Installing packages {.appendix}\n",
        "To install some packages in the active environment, write:\n",
        "```default\n",
        "(@workshop) pkg> add Plots MLJ DataFrames Flux Pluto\n",
        "```\n",
        "These packages will install and precompile.\n",
        "To test one of them, press backspace to leave `Pkg` mode and input:\n",
        "```default\n",
        "julia> using Plots\n",
        "[ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
        "julia> x = range(0,10,100);\n",
        "julia> y = sin.(x);\n",
        "julia> plot(x,y)\n",
        "```\n",
        "This should show a plot of a simple $\\sin$ curve.\n",
        "Note that the precompilation of `Plots` took some time.\n",
        "However, this will not need to occur again until the package is updated.\n",
        "Also note that the call to `plot(x,y)` took some time.\n",
        "This is due to the just-in-time compilation.\n",
        "Now that the compilation has been done for inputs of the types of `x` and `y`, if you run `plot(x,y)` again, it should be almost instantaneous.\n",
        "\n",
        "#### Cleaning up {.appendix}\n",
        "To deactivate the environment, enter the `Pkg` mode again by pressing `]` on an empty line, then enter:\n",
        "\n",
        "```default\n",
        "(@workshop) pkg> activate\n",
        "```\n",
        "To delete the environment we created you can delete the environment folder at the listed location at creation.\n",
        "This is usually `/home/user/.julia/environments/workshop` on MacOS or Linux.\n",
        "\n",
        "**Julia help**\n",
        "\n",
        "- [Julia documentation](https://docs.julialang.org/en/v1/)\n",
        "- [Quick cheatsheet of Julia](https://juliadocs.github.io/Julia-Cheat-Sheet/)\n",
        "- [Comparison of the syntax of Julia, Python, and Matlab](https://cheatsheets.quantecon.org)\n",
        "\n",
        "**Julia packages**\n",
        "\n",
        "As compared to Python, Julia has many scientific computing tools built into its standard library.\n",
        "Thus, a lot of the functionality found in `numpy` are loaded by default.\n",
        "On the other hand, because of the interoperability of the language and the reduced need for a polyglot codebase (i.e. needing C and Fortran code for a Python package to be fast), packages are usually much smaller modules in Julia.\n",
        "For example, the functionality of the `scipy` package in Python can be found spread across possibly a dozen different packages in Julia.\n",
        "This is convenient to only load and use what you need, but inconvenient in that it may require more searching to find and the interfaces may not be standardized.\n",
        "The following are some packages that roughly recreate the essential Python packages [here](#python-resources-and-packages). \n",
        "\n",
        "- `numpy` - [Standard library](https://docs.julialang.org/en/v1/manual/arrays/),[`FFTW.jl`](https://juliamath.github.io/FFTW.jl/latest/)\n",
        "- `scipy` - [`Statistics.jl`]()\n",
        "- `matplotlib` - [`Plots.jl`](https://docs.juliaplots.org/latest/)\n",
        "- `scikit-learn` - [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
        "- `pandas` - [`DataFrames.jl`](https://dataframes.juliadata.org/stable/)\n",
        "- `keras`,`tensorflow`,`pytorch` - [`Flux.jl`](https://fluxml.ai/Flux.jl/stable/)\n",
        "- `scikit-image` - [`Images.jl`](https://juliaimages.org/latest/install/)\n",
        "- `jupyter` - [`Pluto.jl`](https://github.com/fonsp/Pluto.jl) although you can use Julia with Jupyter via [`IJulia.jl`](https://julialang.github.io/IJulia.jl/stable/)"
      ],
      "id": "2732e465"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}