{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Workshop 2: Neural networks - structure, building, and training'\n",
        "author: Jake Brusca\n",
        "execute:\n",
        "  keep-ipynb: true\n",
        "---"
      ],
      "id": "2044ed46"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup \n",
        "Information on setting up a Python environment and package management can be found in the [first workshop.](https://cnrrobertson.github.io/other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html)\n",
        "\n",
        "\n",
        "In Terminal:\n",
        "```\n",
        "mamba activate workshop \n",
        "mamba install numpy py-pde tensorflow matplotlib\n",
        "```\n",
        "\n",
        "You can use [Google colab](https://colab.research.google.com) if unable to run local Jupyter Notebooks.\n",
        "\n",
        "In cell:\n",
        "```\n",
        "!pip install numpy py-pde tensorflow matplotlib\n",
        "```\n"
      ],
      "id": "f4630174"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "#Import packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "abd8d14e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning a simple model\n",
        "As a quick example, we'll try to fit a wide, shallow neural network to $sin(x)$ on $[0, 4\\pi]$. We pick a network with a single layer, using ReLU as the activation function. We create a set of sample data $\\overline{X},\\overline{Y}$, then try to fit the network $N(x_i,\\beta) \\approx y_i$\n",
        "$$\n",
        "\\beta^* = \\text{argmin} \\sum (N(x_i;\\beta)-y_i)^2\n",
        "$$\n",
        "From the universal approximation theorem, we know that some network exists which can approximate this curve to any precision, however it's unclear that the network we discover from our optimization problem will be that network.\n"
      ],
      "id": "e9f8346e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "# Generate sin data\n",
        "xmin_s = 0\n",
        "xmax_s= 4*np.pi\n",
        "nx_s = 100\n",
        "x_s= np.linspace(xmin_s,xmax_s,nx_s)\n",
        "y_s= np.sin(x_s)\n",
        "\n",
        "# Build a model\n",
        "model_s = Sequential()\n",
        "model_s.add(Dense(700, input_shape=(1,), activation='relu'))\n",
        "model_s.add(Dense(1))\n",
        "model_s.compile(loss = 'mae',optimizer = 'adam')\n",
        "model_s.fit(x_s,y_s,epochs=10000,batch_size = 25, verbose = 0)\n",
        "\n",
        "# Make Prediction\n",
        "y_pred_s = model_s.predict(x_s) # Prediction with training data\n",
        "x_test_s = np.linspace(xmin_s,2*xmax_s,4*nx_s)\n",
        "y_test_s = model_s.predict(x_test_s)\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize = (12,4))\n",
        "plt.plot(x_test_s,y_test_s)\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(x_s,y_s)\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(x_s,y_pred_s)\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(x_test_s,y_test_s)"
      ],
      "id": "ac2ee7e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning a PDE\n",
        "We consider a simple 1D transport equation with periodic boundary conditions\n",
        "\\begin{cases}\n",
        "u_t = u_x, \\quad (x,t) \\in [0,L]\\times(0,\\infty)\n",
        "\\\\\n",
        "u(x,0) = u_0(x)\n",
        "\\\\\n",
        "u(x,t) = u(x+L,t)\n",
        "\\end{cases}\n",
        "The true solution to this PDE is given by $u(x,t) = u_0(\\text{mod}(x-t,1))$, which can be found using the method of characteristics on the free domain and then truncating to a periodic domain. Though we have access to the true solution, we will generate our training data using a prepackaged numerical PDE solver `py-pde`. We will use a Gaussian as our initial data $u_0(x) = e^{-100(x-.3)^2}$ and solve on the domain $[0,1]$.\n",
        "\n",
        "Note that this isn't an attempt to solve the PDE using NNs, rather we are just using the PDE data as a specific set of training data.\n"
      ],
      "id": "35f2e29a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pde\n",
        "\n",
        "# Domain\n",
        "xmin = 0.0\n",
        "xmax = 1.0\n",
        "nx = 101\n",
        "tmin = 0.0\n",
        "tmax = 1.0\n",
        "dt = 1e-6\n",
        "save_dt = 0.01\n",
        "init_cond = \"1*exp(-(1/.01)*(x-0.3)**2)\"\n",
        "\n",
        "# Initialize objects\n",
        "grid = pde.CartesianGrid([(xmin,xmax)],nx,periodic=True)\n",
        "h = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\n",
        "eq = pde.PDE({\"h\": \"-d_dx(h)\"})\n",
        "storage = pde.MemoryStorage()\n",
        "\n",
        "# Run\n",
        "result = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt))\n",
        "\n",
        "# Save data\n",
        "data = np.array(storage.data)\n",
        "np.save(\"simple_wave.npy\", data)"
      ],
      "id": "fc43ffe5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Processing\n",
        "Arguably the most important part of training a machine learning algorithm is the data. Most prepackaged algorithms expect data to be formatted in a specific way, usually as an array where each column represents different features and each row represents the samples. As it stands, we have our target $h(x,t)$ data represented as a matrix, and have our $x,t$ each represented as single arrays. We need create an array of each pair of $x,t$ data points, and map the $h(x,t)$ to array of the corresponding values.\n"
      ],
      "id": "ee24026b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create training data\n",
        "x = np.linspace(xmin,xmax,nx)\n",
        "nt = int((tmax-tmin)/save_dt+1)\n",
        "t = np.linspace(tmin,tmax,nt)\n",
        "T = np.repeat(t,nx)\n",
        "X = np.tile(x,int(nt))\n",
        "XT = np.transpose(np.array([X,T]))\n",
        "y = np.transpose(data.reshape(nt*nx))"
      ],
      "id": "e0830ff2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Network\n",
        "We'll use the Keras package to build our Neural Networks. Keras is an API for building Neural Networks built on `tensorflow`. We can initialize the network using the Sequential() class, then add layers .add() method for our model. We will use Dense layers, which means that each node takes inputs from all of the other nodes in the previous layer. We can specify the initial input shape in the first layer, and each size and activation function for each layer. \n"
      ],
      "id": "a64ba4f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_shape=(2,), activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train model\n",
        "model.fit(XT,y,epochs=1000, batch_size=200, verbose = 0)\n",
        "yPred = np.array(model.predict(XT)).reshape(nt,nx)\n",
        "plt.figure(figsize = (12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "p = plt.imshow(data,aspect = 'auto')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "p = plt.imshow(yPred,aspect = 'auto')\n",
        "plt.colorbar()"
      ],
      "id": "55863afa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create subset of training data\n",
        "t_stop= .5/tmax\n",
        "t_split = nx*int(t_stop*nt)\n",
        "XT_Train = XT[0:t_split,:]\n",
        "y_Train = y[0:t_split]\n",
        "\n",
        "# Fit new model\n",
        "model.fit(XT_Train,y_Train,epochs=1000, batch_size=200, verbose = 0)\n",
        "\n",
        "plt.figure(figsize = (12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "p = plt.imshow(y_Train.reshape(int(t_stop*nt),nx))\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "yPred = np.array(model.predict(XT)).reshape(nx,nt)\n",
        "p = plt.imshow(yPred)\n",
        "plt.colorbar()"
      ],
      "id": "152149dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyper Parameters \n",
        "One of the many challenges you face when working with Neural Networks is the wide range of hyper-parameters you need to choose in order to build the network. Some of the more obvious ones are the number of layers, the depth of each layer, and the activation function you use. It's clear that these can have dramatic effects on the resulting neural network, but even smaller changes can too. In this example, we double the number of epochs that the network is trained on. This tends to result in a lower quality prediction, likely from overfitting to specific training data. There are ways to better chose hyper-parameters and mitigate things like over fitting, but that is beyond the scope of this workshop. \n"
      ],
      "id": "ba65dee7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.fit(XT_Train,y_Train,epochs=2000, batch_size=200, verbose = 0)\n",
        "yPred = np.array(model.predict(XT)).reshape(nx,nt)\n",
        "p = plt.imshow(yPred)\n",
        "plt.colorbar()"
      ],
      "id": "53d88701",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}