{
  "hash": "0c760be4e9a4da449f389c5a255a6034",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Lab 3 - Fixed point methods\nauthor: Connor Robertson\n---\n\n## Introduction\n\nOur goal is to find the fixed point of a function $g(x)$, that is, the value $x^*$ such that $x^*= g(x^*)$ (if it exists).\nIf $|g'(x^{*})|<1$, we can find the fixed point by iteration if we start sufficiently close to $x^*$.\n\nFor the example problem $g(x) = \\cos{(x)}$, we can begin with an initial guess $x_1=0.5$ and find the next point, $x_2$, by applying $g$: $x_2=g(x_1)$.\nWe repeat this process $n$ times to find $x_{n+1}=g(x_n)$.\nThis can be summarized in the following table:\n\n| n  | $ x_n$ | $x_{n+1}=\\cos(x_n)  $ | $ E_n=|x_n-x_{n+1}|$ |\n| :--:|:-----:| :-----: |:------:|\n| 1  | 0.5000 | 0.8775  | 0.3775 |\n| 2  | 0.8775 | 0.6390  | 0.2385 |\n| 3  | 0.6390 | 0.8026  | 0.1636 |\n| 4  | 0.8026 | 0.6947  | 0.1079 |\n| 5  | 0.6947 | 0.7681  | 0.0734 |\n| 6  | 0.7681 | 0.7191  | 0.0490 |\n| 7  | 0.7191 | 0.7523  | 0.0331 |\n| 8  | 0.7523 | 0.7300  | 0.0222 |\n| 9  | 0.7300 | 0.7451  | 0.0150 |\n| 10 | 0.7451 | 0.7350  | 0.0101 |\n| 11 | 0.7350 | 0.7418  | 0.0068 |\n\nThis table shows an iteration $n$, the $x$ value $x_n$, the result of the iteration $x_{n+1} = \\cos(x_n)$, and the difference between the current step and the next $E_n = |x_n - x_{n+1}|$.\nNote that the difference in $x_i$ between each step gets smaller and smaller as we iterate.\nThis shows that we are indeed approaching a fixed point.\nIn this case, if we continue we will eventually converge to the fixed point $x^*\\approx0.739085133$.\n\nThis process is only guaranteed to converge if $|g'(x^*)|<1$.\nIn this case, $g(x)$ is \"contracting\" around $x^*$.\nTo see this contraction visually, we can consider the following image:\n\n![[Code for the image](images/gen_img1.py)](images/img1)\n\nIn this image, the dashed line represents the line $y=x$, so the arrows go from $(x_1, g(x_1))\\to (x_2, x_2) \\to (x_2, g(x_2)) \\to \\ldots (x_4, g(x_4)$.\n\nIn practice, we do not know when we have arrived at the fixed point $x^*$, so we instead stop by looking at the difference between iterations: $E_n = |x_{n+1} - x_n|$.\nWhen this difference becomes sufficiently small, we can assume that we have the best answer that the fixed point method can give us.\n\nBelow is a program that performs fixed point iteration for a given function $g(x)$.\nThe convergence criterion for the program will be given by the error condition which can be written as:\n$$\n\\begin{align*}\n    E_n={|x_{n+1} - x_n|} <\\text{ tol}\n\\end{align*}\n$$\nwhere tol is a given tolerance.\n\n::: {.callout-note}\nAlthough $|g'(x^*)|<1$ guarantees that the method will converge \\textit{in some interval around $x^*$}, in practice we usually do not know the interval.\nThus, if your initial guess for $x^*$ is not sufficiently close to the actual value of $x^*$ the method may not converge.\nInformed initial guesses and trial and error are common techniques for overcoming this shortcoming.\n:::\n\n## Newton's Method\n\nNewton's method is a fixed point iteration that converges to the root of a function $f(x)$.\nIt uses the iteration function:\n$$\n\\begin{align*}\n\tg(x) = x - \\frac{f(x)}{f'(x)}\n.\\end{align*}\n$$\nAnalyzing this function, we can see that the fixed point $x^*$ for $g(x)$ is also the root of $f(x)$:\n$$\n\\begin{align*}\n    g(x^*) &= x^{*} \\\\\n    x^* - \\frac{f(x^*)}{f'(x^*)}&= x^* \\\\\n    - \\frac{f(x^*)}{f'(x^*)} &= 0 \\\\\n\tf(x^*) &= 0\n\\end{align*}\n$$\nas long as $f'(x^*) \\neq 0$.\n\n### Convergence\nThis method was published in 1685 but is still one of the state-of-the-art methods for root finding because it \"converges\" so quickly.\nThe idea of convergence is key to numerical methods and refers to the rate at which the error decreases as you perform more iterations.\nThe easiest way to illustrate convergence is to consider the Taylor series of our fixed point iteration centered at $x^*$:\n$$\n\\begin{align*}\n    g(x_{n}) &= g(x^*) + g'(c)(x_{n}-x^*)\n\\end{align*}\n$$\nfor some $c \\in (x^*, x_{n})$.\nWe can then substitute $g(x_{n}) = x_{n+1}$ and rearrange to get:\n$$\n\\begin{align*}\n    g(x_{n}) &= g(x^*) + g'(c)(x_{n}-x^*) \\\\\n\tx_{n+1} &= x^* + g'(c)(x_{n} - x^*) \\\\\n\t\\frac{x_{n+1} - x^*}{x_{n}-x^*} &= g'(c)\\\\\n\t\\frac{|x_{n+1} - x^*|}{|x_{n}-x^*|} &= |g'(c)| < 1\\\\\n\\end{align*}\n$$\nas long as $c$ is close enough to $x^*$.\nThus, we know that $|x_n - x^*|$ is larger than $|x_{n+1} - x^*|$ which means we are getting closer and closer to the fixed point with each iteration.\nWe also know that the error at step $n+1$ over the error at step $n$ is some constant $g'(c)$.\nIf $g'(c) \\neq 0$ and $g'(c) < 1$, then this is called \"first-order\" convergence.\nThis means, for example, if our error at step $n$ is $\\frac{1}{2}$ and our constant is $1$, our error will be reduced by $\\frac{1}{2}$ after 1 step.\n\nHowever, if our fixed point iteration has the property that $g'(x^*) = 0$, then we have to take a higher-order Taylor series:\n$$\n\\begin{align*}\n    g(x_{n}) &= g(x^*) + g'(x^*)(x_{n}-x^*) +\\frac{1}{2} g\"(c)(x_{n}-x^*)^2 \\\\\n    x_{n+1} &= x^* + \\frac{1}{2} g\"(c)(x_{n}-x^*)^2 \\\\\n    \\frac{x_{n+1} - x^*}{(x_n - x^*)^2} &= \\frac{1}{2} g\"(c) \\\\\n    \\frac{|x_{n+1} - x^*|}{|x_n - x^*|^2} &= \\frac{1}{2} |g\"(c)| = C\n.\\end{align*}\n$$\nNow, the error at step $n+1$ over the SQUARE of the error at the previous step is equal to a constant.\nThis is called \"second-order\" convergence.\nThis means, for example, if our error at step $n$ is $\\frac{1}{2}$ and the constant is $1$, then our error will be reduced by $\\frac{1}{2}^{2} = \\frac{1}{4}$ after 1 step.\nThus, it will take far fewer iterations to converge on the true answer.\nNewton's method is a \"second-order\" method.\n\nOne way to test this convergence is by assuming we are near the fixed point and considering:\n$$\n\\begin{align*}\n    E_n = |x_{n} - x_{n-1}|\n\\end{align*}\n$$\nthen we can write the convergence definition approximately as:\n$$\n\\begin{align*}\n    C = \\frac{|x_{n+1} - x^*|}{|x_n - x^*|^d} &\\approx \\frac{|x_{n+1} - x_n|}{|x_n - x_{n-1}|^d} \\\\\n    &= \\frac{E_n}{(E_{n-1})^d}\n\\end{align*}\n$$\nwhere $C$ is some constant and $d$ is our order of convergence.\nThis approximation really only makes sense when $x_n$ and $x_{n-1}$ are close to $x^*$.\nHowever, we can check this version numerically by computing the error $E_n$ of our method and then trying $\\frac{E_n}{E_{n-1}^d}$ with different values of $d$ to see which one is roughly constant.\n\n## Examples\n\nThe following is a Matlab function that takes in a contraction function $g$, a starting value $x_1$, and an error tolerance \\texttt{tol}.\nIt outputs an array that contains all your iteration values: $x = [x_1, x_2, x_3, \\ldots, x_N]$.\nThe convergence criteria for the function is the error: $|x_{n+1} - x_n| < \\text{tol}$.\n\n\n```{matlab}\n%| output: false\n%%file MyFixedPoint.m\nfunction [xs] = MyFixedPoint(g, x1, tol)\n    xs = [x1];\n    err = Inf;\n    while err > tol\n        xs = [xs g(xs(end))];\n        err = abs(xs(end) - xs(end-1));\n    end\nend\n```\n\n\nTesting this function on $g(x) = \\sin(x-5)$ with starting point $x_1 = 1$ and a tolerance of $10^{-7}$ is close to $0.847$:\n\n\n```{matlab}\ng = @(x) sin(x - 5);\np1 = MyFixedPoint(g, 1, 1e-7);\np1(end)\n```\n\n\n### Newton's method\nConsider finding the root of the function $f(x) = x - \\cos(x) - 1$.\nTo find the root of this function with Newton's method, we need to reformulate it as a fixed point problem in which the fixed point is the root.\nThis can be easily done by considering:\n$$\n\\begin{align*}\n    f'(x) &= 1 + sin(x) \\\\\n    g(x) &= x - \\frac{f(x)}{f'(x)} \\\\\n    &= x - \\frac{x - \\cos(x) -1 }{1 + \\sin(x)}\n\\end{align*}\n$$\nTo approximate the root of the function with an initial guess of $x_1 = 2.5$ and a tolerance of $10^{-10}$:\n\n\n```{matlab}\ng = @(x) x - ((x - 1 - cos(x))/ (1 + sin(x)));\n[xVec] = MyFixedPoint(@(x)g(x), 2.5, 1e-10);\nerrorVec = [0 abs(xVec(2:end) - xVec(1:end-1))];\nnVec=1:1:length(xVec);\np2_table = table(transpose(nVec), transpose(xVec), transpose(errorVec));\np2_table.Properties.VariableNames=[\"n\", \"x_n\", \"E_n\"]; disp(p2_table);\n```\n\n\nIf we make a table with the ratios $\\frac{E_n}{E_{n-1}}$, $\\frac{E_n}{(E_{n-1})^2}$, and $\\frac{E_n}{(E_{n-1})^3}$ the preivous results, we can examine an approximate convergence rate for the method:\n\n\n```{matlab}\nerrorVec = [0 abs(xVec(2:end) - xVec(1:end-1))];\nratio1 = errorVec(2:end) ./ errorVec(1:end-1);\nratio2 = errorVec(2:end) ./ (errorVec(1:end-1) .^2);\nratio3 = errorVec(2:end) ./ (errorVec(1:end-1) .^3);\np2_table = table(ratio1', ratio2', ratio3');\np2_table.Properties.VariableNames=[\"E_n / E_n-1\", \"E_n / E_n-1^2\", \"E_n / E_n-1^3\"];\ndisp(p2_table);\n```\n\n\nIn this, we can see that the second column (which represents a roughly quadratic convergence) is roughly constant, demonstrating that this ratio is holding true.\nOn the other hand, the first column decays to 0, showing that the errors\n\n### Stability of fixed points\nConsider the function $g(x) = 1.8x - x^2$ which has fixed points $x=0$ and $x=0.8$.\nIf we use an initial guess $x_1 = 0.1$ and tolerance $10^{-5}$:\n\n\n```{matlab}\ng = @(x) 1.8*x - x^2;\np3 = MyFixedPoint(g, .1, 1e-5);\ndisp(p3(end))\n```\n\n\nWe approach the fixed point $x=0.8$ because it is a stable (attracting) fixed point.\nOn the other hand, if we start at $x_1 = -0.1$:\n\n\n```{matlab}\ng = @(x) 1.8*x - x^2;\np3 = MyFixedPoint(g, -0.1, 1e-5);\ndisp(['Final answer: ', num2str(p3(end))])\n```\n\n\nWe are no longer in the basin of attraction of $x=0.8$ and so our iteration is expelled from the unstable fixed point $x=0$.\n\n",
    "supporting": [
      "Lab3_fixed_points_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}