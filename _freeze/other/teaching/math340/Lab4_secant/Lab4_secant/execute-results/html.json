{
  "hash": "c359dab76111d35cfbb129decd563590",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Lab 4 - Secant method\nauthor: Connor Robertson\n---\n\n## Secant method\n\nThere is an alternative to Newton's method that does not require a closed form for $f'(x)$ called \"Secant method.\"\nAlthough the method is presented from the context of Newton's method, it actually predates it by thousands of years.\nIn order to understand it as it relates to Newton's method, we can consider the definition of the derivative $f'(x)$:\n$$\n\\begin{align*}\n\tf'(x) = \\lim_{\\delta \\to 0} \\frac{f(x) - f(x-\\delta)}{\\delta}\n.\\end{align*}\n$$\nWe can rewrite this as:\n$$\n\\begin{align*}\n\tf'(x) = \\lim_{\\delta \\to 0} \\frac{f(x) - f(x-\\delta)}{x - (x - \\delta)}\n.\\end{align*}\n$$\nTaking a fixed small number $\\delta$ and plugging in some value $x_n$ where $x_n - \\delta \\approx x_{n-1}$ to get:\n$$\n\\begin{align*}\n\tf'(x_n) \\approx \\frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}\n.\\end{align*}\n$$\nThis is called the \"secant line\" at $x_n$.\nIf we now plug in this derivative approximation to Newton's method, we get:\n$$\n\\begin{align*}\n\tg(x_n, x_{n-1}) = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}\n\\end{align*}\n$$\nwhich is the secant method without an explicit form for $f'(x)$!\nAlthough this has simplified our procedure, there are also some drawbacks in convergence.\n\n### Pseudocode\nGiven two initial points $x1$ and $x2$, the fixed point function $g$ (that takes in variables for $x_n$ and $x_{n-1}$), an ending tolerance $tol$, and a max number of steps $Nmax$, we compute:\n\n```matlab\ni = 2;            % Counter variable\nxs = [x1, x2];    % Array for storing results\nwhile abs(xs(i)-xs(i-1)) > tol && i < Nmax\n    % Append  g(xs(i),xs(i-1)) to xs\n    % Add 1 to i\nend\n```\n\n### Convergence\nFirstly, because the secant method can be viewed as an approximation of Newton's method, it is sensitive to the starting value $x_1$ like Newton's method.\nSpecifically, if the slope of the function $f(x)$ is near $0$, the approximate derivative can give strange results.\nAdditionally, if you start with an $x_1$ that is too far away from your fixed point/root $x^{*}$, the method will not converge.\n\nUsing a similar analysis as we would use with Newton's method (namely Taylor's series and mean value theorem), we ultimately get a convergence expression of\n$$\n\\begin{align*}\n\tC = \\frac{|x_{n + 1} - x^{*}|}{|x_n - x^{*}|^{\\frac{1 + \\sqrt{5} }{2}}}\n\\end{align*}\n$$\nwhich represents convergence of order $\\frac{1 + \\sqrt{5}}{2}$.\nAlthough this is slower than Newton's method, it is still faster than linear (order 1) and is thus faster than a method like bisection method.\n\n## Regula Falsi\nThe secant method addresses the requirement for a closed form derivative $f'(x)$ to be used in Newton's method, it does not solve the sensitivity to the initial starting point.\n\nThe \"regula falsi\" method can be viewed as a further extension of the secant method which is guaranteed to converge to the true root $x^{*}$ by \"bracketing\" the root like the bisection method (i.e. always considering an interval $[a,b]$ for which $f(a)f(b) < 0$).\nTo do so, it adjusts the bisection method midpoint selection using a secant line:\n$$\n\\begin{align*}\n\tf'(b) \\approx \\frac{f(b) - f(a)}{b - a}\n.\\end{align*}\n$$\nThis line is then applied to finding a \"pseudo-midpoint\" $c$ that is between $a$ and $b$:\n$$\n\\begin{align*}\n\tc &= b - \\frac{f(b)}{f'(b)} \\\\\n\t&\\approx b - f(b) \\frac{b - a}{f(b) - f(a)}\n.\\end{align*}\n$$\nAfter computing the \"midpoint\" $c$, we can check $f(a)f(c) < 0$ and adjust the interval $[a,b]$ accordingly.\n\n### Pseudocode\nGiven the initial interval $[a,b]$, the function $f$, and an ending tolerance $tol$ (the size of the final interval), we compute:\n```matlab\n% Create a function g that computes the midpoint using regula falsi formula\ng = @(a,b) ...\n% While half the interval is bigger than the given accuracy tolerance\nc = g(a,b)\nwhile |f(c)| > tol\n    % c = g(a,b)\n    % If f(c) = 0, stop                  -- this would be lucky (c=root)!\n    % If f(a)f(c) < 0, then b=c          -- the root is between a and c\n    % Else, a=c                          -- the root is between c and b\nend\n% The final c is an approximate root\n```\nNote that this is almost identical to the bisection method with an adjusted calculation of $c$!\n\n### Convergence\nThe general concept of the regula falsi method is to use the secant line to identify a point much closer to the root than the bisection method could, while still bracketing the root and guaranteeing convergence.\nIn that sense, it should share a similar convergence rate as the bisection method along with guaranteed convergence.\n\nHowever, unlike the bisection method, the regula falsi method does not have a guaranteed amount that it will shrink the interval $[a,b]$ at each step due to this new \"pseudo-midpoint\" selection.\nThus, it can on occasion converge slower than the bisection method!\nThis demonstrates the challenging trade-offs associated with numerical methods: improved convergence, computational cost, or speed may result in instability, lack of convergence, or more assumptions and vice versa.\n\n## Comparison of root finding methods\nAs we have explored in the previous sections, there are various pros and cons associated with each of the root finding methods we have discussed.\nThese can be roughly summarized in the following table:\n\n| Method | Pros | Cons |\n| :-----: | :-----: | :-----: |\n| Bisection | Guaranteed to converge | Slow linear convergence |\n| Newton's | Fast quadratic convergence | Need to know $f'$, may not converge |\n| Secant | Good superlinear convergence | May not converge |\n| Regula falsi | Usually superlinear convergence, guaranteed to converge | Can have very slow convergence |\n\nTo further illustrate these differences, consider the following plots and table showing the results of these methods on the function $f(x) = (x-3)^3\\sin(x) + 3.5$ on the interval $[1,3]$ using a tolerance of $10^{-5}$:\n\n| $n$ | $x_n$ | $f(x_n)$ | $x_n$  | $f(x_n)$  | $x_n$  | $f(x_n)$  | $x_n$  | $f(x_n)$  |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| 0 | 2.00| 2.59| 2.00| 2.59| 1.00| -3.23| 1.96| 2.4 |\n| 1 | 1.50| 0.13| 1.18| -2.10| 2.00| 2.59| 1.54| 0.4 |\n| 2 | 1.25| -1.58| 1.48| 0.01| 1.55| 4.83| 1.48| 0.0 |\n| 3 | 1.37| -0.71| 1.47| 0.00| 1.45| -0.17| 1.48| 0.0 |\n| 4 | 1.43| -0.28| | | 1.48| 0.004| | |\n| 5 | 1.46| -0.07| | | 1.47| 0.00| | |\n| 6 | 1.48| 0.03| | | | | | |\n| 7 | 1.47| -0.02| | | | | | |\n\n: Iteration results for: Bisection ($a=1,b=3$), Newton's ($x_1=2$), Secant ($x_1=1,x_2=2$), Regula Falsi ($a=1,b=3$)\n\nIt can be immediately recognized that Newton's method converged to a very accurate solution most rapidly followed closely by Regula falsi then Secant and finally Bisection.\nIt is impressive that Regula falsi was so quick but this is not always the case.\nIn addition, we needed more information for Newton's method and for Regula falsi.\n\nTo get a sense for how these results can change, consider $f(x) = 2x^{3} - 4x^{2} + 3x$ on the interval $[-1,2]$ using a tolerance of $1\\times 10^{-2}$:\n\n| $n$  | $x_n$  | $f(x_n)$  | $x_n$   | $f(x_n)$   | $x_n$   | $f(x_n)$   | $x_n$   | $f(x_n)$ |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| 0 | 0.50 | 0.75 | -1.00 | -9.00 | -1.00 | -9.00 | 0.80 | 0.86 |\n| 1 | -0.25 | -1.03 | -0.47 | -2.50 | -0.50 | -2.75 | 0.64 | 0.80 |\n| 2 | 0.13 | 0.32 | -0.16 | -0.59 | -0.28 | -1.19 | 0.50 | 0.75 |\n| 3 | -0.06 | -0.20 | -0.02 | -0.08 | -0.11 | -0.38 | 0.39 | 0.68  |\n| 4 | 0.03 | 0.09 | -0.00 | -0.00 | -0.03 | -0.09 | 0.29 | 0.58 |\n| 5 | -0.02 | -0.05 |  |  | -0.00 | -0.01 | 0.21 | 0.47 |\n| 6 | 0.01 | 0.02 |  |  | -0.00 | -0.00 | 0.15 | 0.37 |\n| 7 | -0.00 | -0.01 |  |  |  |  | 0.10 | 0.27 |\n| 8 | 0.00 | 0.01 |  |  |  |  | 0.07 | 0.20 |\n\n: Iteration results for: Bisection ($a=-1,b=2$), Newton's ($x_1=-1$), Secant ($x_1=-1,x_2=0.5$), Regula Falsi ($a=-1,b=2$)\n\nIn this example, the regula falsi method suffers from a small secant line and shrinks the interval by only about $\\frac{1}{3}$ at each step making it much slower than the bisection method.\nAlthough this example was selected specifically to demonstrate this behavior (and it's not a very common behavior), it is something to be aware of when selecting your root finding method.\n\n## Examples\n\nThe following is a Matlab function that takes in a function handle `f`, initial points `x1` and `x2`, a tolerance `tol`, and a max number of possible iterations `Nmax`.\nThis function performs the secant method iteration until $|f(x_n)| < \\text{tol}$ and return all iterations $x_n$ as an array `xs`:\n\n\n```{matlab}\n%| output: false\n%%file MySecantMethod.m\nfunction [xs] = MySecantMethod(f, x1, x2, tol, Nmax)\n    g = @(x1,x2) x2 - f(x2) * ((x2 - x1) / (f(x2) - f(x1)));\n    xs = [x1,x2];\n    err = Inf;\n    while err > tol\n        xs = [xs g(xs(end-1), xs(end))];\n        err = abs(f(xs(end)));\n        if length(xs) > Nmax\n            return;\n        end\n    end\nend\n```\n\n\nConsidering a function $f(x) = (x-3)^3\\sin(x) + 3.5$:\n\n\n```{matlab}\nf = @(x) (x - 3).^3 .* sin(x) + 3.5;\nxdom = linspace(0, 4, 100);\ny = f(xdom);\nfigure()\nplot(xdom, y);\nhold on; grid on\nxlabel(\"x\", 'Interpreter','latex'); ylabel(\"f(x)\", 'Interpreter','latex');\nlegend(\"$(x-3)^3 \\sin(x) + 3.5$\", 'interpreter','latex')\nhold off\n```\n\n\nApplying this function to find an approximate root for the above function with initial points $x_1 = 1$ and $x_2=2$, a tolerance $10^{-5}$, and max number of iterations `Nmax=20`:\n\n\n```{matlab}\nxs = MySecantMethod(f, 1, 2, 1e-5, 20);\ndisp(xs)\n```\n\n\nRepeating this procedure using starting points $x_1 = 1$ and $x_2 = 3$:\n\n\n```{matlab}\nxs = MySecantMethod(f, 1, 3, 1e-5, 20);\ndisp(xs)\n```\n\nNote that the results change very significantly between the two different initial conditions.\nIn fact, they seem to no longer converge in the second case.\nThis demonstrates the sensitivity of the secant method to the two initial guess points.\n\n### Regula falsi\nThe following is a Matlab function that takes in a function handle `f`, interval lower bound `a`, interval upper bound `b`, and a tolerance `tol`.\nThis function performs the regula falsi iteration until $|f(x_n)| < \\text{tol}$ and return all iterations $x_n$ as an array `xs`:\n\n\n```{matlab}\n%| output: false\n%%file MyRegulaFalsi.m\nfunction [xs] = MyRegulaFalsi(f, a, b, tol)\n    N = 0;\n    xs = [];\n    c = b - f(b) * ((b - a) / (f(b) - f(a)));\n    while abs(f(c)) > tol\n        c = b - f(b) * ((b - a) / (f(b) - f(a)));\n        xs(N+1) = c;\n        if f(c) == 0\n            return\n        elseif f(a)*f(c) < 0\n            b = c;\n        else\n            a = c;\n        end\n        N = N + 1;\n    end\nend\n```\n\n\nApplying the regula falsi function to find an approximate root for $f(x) = (x-3)^3\\sin(x) + 3.5$ with interval $[1,3]$ and a tolerance $10^{-5}$:\n\n\n```{matlab}\nxs = MyRegulaFalsi(f, 1, 3, 1e-5);\ndisp(xs)\n```\n\n\n### Finding a quantile of a probability distributions\nA very common computation in statistics is to find a \"quantile\" of a probability distribution.\nFor example, say we have a normal distribution of the heights of people in the class given by the cumulative density function (CDF):\n$$\n\\begin{align*}\n    C(x) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma \\sqrt{2}}\\right)\\right)\n\\end{align*}\n$$\nwhere $C(x)$ is the probability that someone in the class is shorter than height $x$ (ft) given the class has average height $\\mu$ (ft) and standard deviation $\\sigma$ (erf is the error function which is also called \\mcode{erf} in Matlab).\n\nThe 95% \"quantile\" of this CDF is the height $x$ for which 95% of the class is shorter (i.e. the $x^*$ such that $C(x^*) = 0.95$ given $\\mu$ and $\\sigma$).\nThis computation is actually a root finding problem!\nAll we need to do is find the root of the function:\n$$\n\\begin{align*}\n    f(x) = 0.95 - C(x)\n.\\end{align*}\n$$\nThe derivative of this function can be written as:\n$$\n\\begin{align*}\n    f'(x) = - P(x)\n\\end{align*}\n$$\nwhere $P$ is the probability density function (PDF) of the normal distribution:\n$$\n\\begin{align*}\n    P(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} }e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^{2}}\n.\\end{align*}\n$$\n\nThe following is a plot of the CDF for $x$ on the interval $[3, 8]$ given $\\mu = 5.5$ and $\\sigma = 0.35$.\n\n\n```{matlab}\nm = 5.5;\ns = 0.35;\nC = @(x) (1/2) .* (1 + erf((x - m) ./ (sqrt(2)*s)));\nP = @(x) (1 ./ (s*sqrt(2*pi))) * exp( (-1/2) .* ((x - m) ./ s) .^ 2);\nnquant = @(x) 0.95 - C(x);\n\nxdom = linspace(3, 8);\n% plot(xdom, P(xdom))\nfigure()\nplot(xdom, C(xdom))\nhold on\nxlabel(\"x\", 'Interpreter','latex');\nylabel(\"C(X)\", 'Interpreter','latex');\ngrid on\nhold off\n```\n\n\nTo Find the $95\\%$ quantile of this CDF:\n\n- Using Secant method with starting points `x1=4.5` and `x2=6`, tolerance $10^{-5}$, and `Nmax=10`:\n\n```{matlab}\nxs3 = MySecantMethod(nquant, 4.5, 6, 1e-5, 10);\ndisp(xs3);\n```\n\n- Using regula falsi with interval $[4, 7]$ and tolerance $10^{-5}$:\n\n```{matlab}\nxs4 = MyRegulaFalsi(nquant, 4, 7, 1e-5);\ndisp(xs4);\n```\n\n",
    "supporting": [
      "Lab4_secant_files"
    ],
    "filters": [],
    "includes": {}
  }
}