{
  "hash": "b419c2e00239dd0c4b0017ab4ea1a2f3",
  "result": {
    "markdown": "---\ntitle: 'Workshop 1: Python set up, machine learning basics, gradient descent, and automatic differentiation'\ncsl: american-physics-society.csl\nbibliography: ../../workshops.bib\nauthor: Connor Robertson\nexecute:\n  keep-ipynb: true\n---\n\n:::{.callout-tip collapse=\"true\"}\n## Other formats\nDownload the workshop as a Jupyter notebook <a href=\"workshop1_intro.ipynb\" download>here</a>.\n\nOpen the workshop as a Jupyter notebook in Google Colab [here](https://colab.research.google.com/drive/1eyAxZgdgxWSIsH6oAx9KyRoflI_plqhX?usp=sharing) (with your NJIT Google account).\n:::\n\n\n## Overview\n\n### Machine Learning and Optimization Seminar\nThe goal of the Machine Learning and Optimization seminar this year is to expose the participants to topics in machine learning and optimization by:\n\n1. Facilitating hands-on workshops and group discussions to explore and gain experience\n2. Inviting speakers to introduce machine learning and optimization concepts\n\nWe are excited to get started on this but recognize that since neither machine learning nor optimization are standardized in the department, participants will have a varied level of exposure to different topics.\nOur hope is that we can use this disparity of experience to increase collaboration during the workshops in a way that can't be achieved during the talks.\nAll are encouraged to share their knowledge and experience with one another openly during the workshops and to give feedback to the organizers after.\n\nAll workshop material will be available [here](machinelearning_optimization_seminar.qmd) for later reference.\n\nThis first workshop is focused on tooling and the basic concepts of machine learning and optimization with the goal that everyone can be on the same footing for later workshops.\n\n## Your Python environment \nIt is safe to say that Python is the language of choice for machine learning.\nThis interpreted language has a very clear and high-level syntax, is extremely convenient for interactive programming and debugging, and has an enormous user base of enterprises, researchers, and hobbyists who have built an almost infinite collection of open-source packages for every topic.\nFor these three reasons the workshops for this seminar will use Python.\n\nTo get started, we will give the basics of the Python programming language. \nJust kidding!\nThat would take too long.\nWe will instead guide you on how to install Python most conveniently, teach you how to get started learning about Python, and then point you to a curated library of much more high-quality instruction for using Python.\nA list of some such references as well as documentation for setup and important Python packages can be found in the Appendix [here](#python-resources-and-packages).\n\n### Setting up\n:::{.callout-tip}\nIf you are looking for the easiest and most immediate way to get going, check out the [Google Colab](#google-colab) section.\n:::\n:::{.callout-note}\nThis section will require use of a terminal emulator for installing and running Python.\nIf you are not familiar with the terminal, check out [this quick tutorial](https://mrkaluzny.com/blog/terminal-101-getting-started-with-terminal/) to get started.\n\nIf you are using a computer with Windows, the terminal instructions may not apply.\n:::\n\nPython is installed by default on MacOS and most Linux distributions.\nHowever, it can be challenging to navigate between the versions and packages that your operating system uses and those needed for other projects.\nThus, there are a variety of version, package, and environment management tools:\n\n- **Version management**: Which version of Python are you using? Can you change versions to run a specific Python program if it requires?\n    - `pyenv`\n    - `conda`/`mamba`\n- **Package management**: How can you install the many amazing Python packages people have created?\n    - `pip`\n    - `conda`/`mamba`\n- **Environment management**: If you have two projects that require different packages (or different versions of the same package), can you switch which packages are available depending on which project you are working on?\n    - `venv`\n    - `virtualenv`\n    - `poetry`\n    - `conda`/`mamba`\n    - many more\n    \nThe `conda` package manager is the only one that fills all three roles.\nIt is formally a part of the Anaconda Python distribution which is a favorite in the fields of data science and machine learning.\n`mamba` is a newer and faster rewrite used in exactly the same way and which is highly recommended.\n\nThe best way to get started with `mamba` is to install `mambaforge`.\nYou can find installer downloads for Windows, MacOS, or Linux [here](https://github.com/conda-forge/miniforge#mambaforge).\n\nFor Windows, run the `.exe` file once it is downloaded.\n\nFor MacOS and Linux, open a terminal and navigate to the download location:\n```bash\ncd ~/Downloads\n```\nThen run the installer as follows:\n```bash\n./Mambaforge-Linux-x86_64.sh\n```\nThe installer will walk you through a few steps and end by asking if you'd like to \"initialize Mambaforge by running conda init?\"\nAnswer yes and restart your terminal.\nThis final command will have added `conda` and `mamba` to your system `$PATH` variable, which means it is available to your terminal.\nOnce restarted, run `mamba -V` to print the version and to verify that the installation worked.\n\n### Environments\nThe idea of a `conda`/`mamba` environment is that once an environment is created and activated, all new packages installed will be added to that environment and will be accessible to any Python program run while the environment is active.\nAs an example, let's create an environment called `workshop` with a specific version of Python installed.\nThe following will create the environment and install a specific version of `python`:\n```bash\nmamba create -n workshop python=3.9\n```\nOnce created, we can list our environments via the command\n```bash\nmamba env list\n```\n```\n# conda environments:\n#\nbase                     /home/user/mambaforge\nworkshop                 /home/user/mambaforge/envs/workshop\n```\nNote that there is a \"base\" environment which is where `conda` and `mamba` themselves are installed as well as their dependencies.\nThe best practice is to create an environment for each of your projects to minimize dependency issues (when packages require separate versions of the same package).\n\nTo activate our new environment:\n```bash\nmamba activate workshop\n```\nRunning `mamba env list` will now show our active environment via an asterisk:\n```\nbase                     /home/user/mambaforge\nworkshop              *  /home/user/mambaforge/envs/workshop\n```\n\n### Installing packages\nNow that we have activated the `workshop` `conda` environment, let's install some common machine learning packages in Python.\nIt is as easy as writing:\n```bash\nmamba install numpy matplotlib pandas jupyter scipy scikit-learn scikit-image\n```\nThis command will search the [conda-forge](https://conda-forge.org/#page-top) repository of packages and install the most up-to-date versions (the `forge` in `mambaforge`).\n\n:::{.callout-tip}\nEither `conda` or `mamba` could be used for all the commands discussed in this section.\nHowever, `mamba` is significantly faster when installing packages.\n:::\n\nNow that these packages have been installed, we can easily use them in an interactive `ipython` prompt (installed with the `jupyter` package):\n```bash\nipython\n```\n```default\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50)\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import numpy as np\n\nIn [2]: import matplotlib.pyplot as plt\n\nIn [3]: x = np.linspace(0,10,100)\n\nIn [4]: y = np.sin(x)\n\nIn [5]: plt.plot(x,y); plt.show()\n```\nThis should plot a simple $\\sin$ curve.\n\n### Cleaning up\nAfter we are done using the environment that has our desired version of Python and the needed packages, we can go back to our regular terminal by deactivating the environment:\n```bash\nmamba deactivate workshop\n```\nIf we have somehow broken our environment and need to remove it:\n```bash\nmamba env remove -n workshop\n```\nThere are many more commands and functionalities that `conda` and `mamba` provide that can be found in the [python resources and packages](#python-resources-and-packages) section of the Appendix.\n\n### Google Colab\nAs an alternative to the entire procedure above, you can use an online [Jupyter Notebook](https://jupyter.org) service hosted by Google called [Colab](https://colab.research.google.com).\nThis service will get you up and running immediately but cannot save your environment between notebooks and has limited functionality to run scripts or save data.\nThus, if your notebook requires a package that is not installed by default, you will need to add the installation command in one of the first notebook cells.\nFor example, to install the [reservoirpy]() package, we would write in a notebook cell:\n```default\n!pip install reservoirpy\n```\nIn a notebook, the `!` denotes a terminal command.\nThe package will now be ready for import and use within the current notebook session:\n```default\nfrom reservoirpy.nodes import Input,Reservoir,Ridge\n```\n\n## Basic concepts of machine learning \n\nMachine learning is, at its most basic, automated data analysis, usually with the goal of finding patterns or making predictions.\nThe \"machines\" in this analysis are equations or algorithms and the \"learning\" is usually some form of parameter selection and/or fitting.\nDue to the uncertain nature of most data, the majority of these models are probabilistic in nature.\nIn fact, it can be hard to distinguish the methodological lines between what is termed \"machine learning\" and the field of statistics.\nHowever, there are a few important distinctions between the tools, goals, and terminology of the two areas.\nToday, machine learning has emerged as a broad description of almost any data-driven computing which may or may not include classical descriptive and inferential statistics[@murphy2012machine].\n\nAt first glance, machine learning can be separated into three main classes: \n\n- **Supervised learning**: Given dependent and independent variable data, train a model which effectively maps the independent variable data to produce the dependent variable data.\n    - Generalized linear models (linear, logistic, etc. regression)\n    - Naive Bayes\n    - Neural networks (most)\n    - Support vector machine (SVM)\n    - Random forests\n    - etc.\n- **Unsupervised learning**: Given data, find patterns (no specified output, though there is still a measure of success)\n    - Clustering\n    - Mixture models\n    - Dimensionality reduction\n    - Association rules\n    - etc.\n- **Reinforcement learning**: Given input data and desired outcomes, simulate and use the results to update a model to improve the simulation's ability to achieve those outcomes\n    - Q-learning\n    - SARSA\n    - etc.\n\nThere is an enormous amount of current interest in machine learning methods and there is a corresponding amount of high-quality material discussing it.\nWe will end the introduction here and direct you to established textbooks[@james2013introduction;@hastie2009elements;@murphy2012machine], NJIT classes (Math 478, Math 678, Math 680, CS 675, CS 677), and online resources (too many to even start listing).\n\n### General procedure\nIn practice, machine learning algorithms often boil down to an optimization problem.\nTo characterize this in a few steps, consider a problem with data $x$:\n\n1. Select a model representation $f$ with parameters $p$ for the problem:\n$$\ny = f(x;p)\n$$\n2. Determine an appropriate objective function $\\mathcal{L}$:\n$$\n\\mathcal{L}(f(x;p),x)\n$$\n3. Use an optimization method $\\mathcal{O}$ with parameters $d$ to find parameters $p$ that minimize or maximize the objective for the model:\n$$\np^* = \\mathcal{O}(\\mathcal{L},f,x;d)\n$$\n\nIn some sense, this is the same procedure used for _inverse problems_ in traditional applied mathematics but with a broader set of models $f$ that may or may not be based on first-principles understanding of the problem.\n\n### Incorporating data\nDepending on the class of problem considered (supervised, unsupervised, or reinforcement), there are a variety of choices for models $f$ and objectives $\\mathcal{L}$.\nFor supervised learning (the most common), the objective is often to predict or generate the output or dependent variable data of some process.\nFor this, data can be separated into three sets:\n\n1. **Training data** ($x$): used to tune the parameters $p$\n2. **Validation data** ($x^v$): used to evaluate the generalization of the model $f$ to data not in the training set during training\n3. **Testing data** ($x^t$): used to benchmark the predictive or generative ability of the model after training is completed\n\n## A first machine learning problem\n\n:::{.callout-note}\nThe code for this problem will require the following packages: `numpy`, `matplotlib`, `autograd`\n:::\n\nThese workshops are about learning by doing, so let's build understanding by fitting a simple \"machine\" to some data as a supervised problem.\nConsider some data $(x,y)$:\n<!-- Consider some data $(x,y)$ generated with the function: -->\n<!-- $$ -->\n<!-- y = e^{-x}(\\sin(x^3) + \\sin(x^2) - x) + \\frac{1}{1 + e^{-(x-1)}} -->\n<!-- $$ -->\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Data generation\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef f_known(x):\n    part1 = np.exp(-x)*(np.sin((x)**3) + np.sin((x)**2) - x)\n    part2 = 1/(1 + np.exp(-1*(x-1)))\n    return part1 + part2\nxsamples = np.random.uniform(-1/2,5,100)\nysamples = f_known(xsamples)\nplt.scatter(xsamples,ysamples)\nplt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Data\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop1_intro_files/figure-html/cell-2-output-1.png){width=589 height=449}\n:::\n:::\n\n\nWe would like to fit a model of the following form to this data:\n$$\nf(x;p_0,p_1) = e^{-p_0x}(\\sin((p_0x)^3) + \\sin((p_0x)^2) - p_0x) + \\frac{1}{1 + e^{-p_1(x-1)}}\n$$\n<!-- which looks like the following for $p_0=1,p_1=1$: -->\n\nTo formulate this as a machine learning/optimization problem, we can consider an objective/loss to minimize the $L^2$ norm distance between the model output $f(x)$ and the true data $y$:\n$$\n\\mathcal{L(f(x;\\vec{p}),y)} = ||f(x) - y||_2^2\n$$\nThe problem can then be written as the unconstrained optimization problem:\n$$\np^* = \\underset{\\vec{p}}{\\text{minimize }} \\mathcal{L}(f(x;\\vec{p}),y)\n$$\nWe then expect our model $f(x;p^*)$ to represent a \"machine\" that has accurately \"learned\" the relationship between $x$ and $y$.\n\nThere are several ways to approach this problem, but a simple and popular approach for a continuous and unconstrained problem is to use an iterative gradient method.\n\n### Gradient descent\n_Gradient descent_ is a straightforward method taught early in an undergraduate numerical methods class.\nIts simplicity and relatively low computational cost has made it popular for machine learning methods (which can contain enough parameters that second-order methods, like Newton's method, are infeasibly expensive because of the Hessian computation).\nBeginning with an initial parameter guess $\\vec{p}_0$, its update procedure can be written as:\n$$\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i\nv^i &= -\\alpha \\nabla_p \\mathcal{L} \\\\\n\\end{align*}\n$$\nwhere $\\alpha$ controls the step size in the direction of the gradient (usually called a \"learning rate\" in machine learning).\nThis method will follow the gradient of the objective/loss $\\mathcal{L}$ until the objective is sufficiently small, or until it reaches a steady state.\n\nSimply implemented in Python, this method can be written as:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef gradient_descent(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    for s in range(steps):\n        v_i = -alpha*f_p(x)\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\nHowever, this method contains a troublesome parameter $\\alpha$ which, if chosen too large, could prevent convergence of the solution or, if chosen too small, could require an unreasonable number of steps to converge.\nThe method itself is also prone to terminate in local minima rather than in a global minimum unless the correct initial guess and learning rate are chosen.\nFor this reason, \"vanilla\" (or normal) gradient descent is almost always replaced with a modified method in learning problems[@sebastian_ruder_2020;@john_chen_2020].\n\nThe following demonstrates an animation of the above gradient descent method applied to our data with two different learning rates, one successful, one not.\nIt uses the following animation code and the `autograd` automatic differentiation library that will be further discussed later:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Animation code\"}\nfrom matplotlib import animation as anim\nfrom matplotlib import gridspec\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef animate_steps_2d(xs,loss,savefile=None,xmin=-.1,xmax=2.5,ymin=-1,ymax=3,interval=50):\n    fig = plt.figure(figsize=(10,6),constrained_layout=True)\n    gs = gridspec.GridSpec(ncols=6,nrows=2,figure=fig)\n    ax = fig.add_subplot(gs[:,0:4],projection=\"3d\",computed_zorder=False)\n    ax1 = fig.add_subplot(gs[0,4:])\n    ax2 = fig.add_subplot(gs[1,4:])\n    ax.view_init(47,47)\n    ax.set_xlabel(\"$p_0$\"); ax.set_ylabel(\"$p_2$\"); ax.set_zlabel(\"loss\",rotation=90)\n    ax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"loss\")\n    ax2.set_xlabel(\"$p_1$\"); ax2.set_ylabel(\"loss\")\n\n    xs_arr = np.array(xs)\n    fxs = np.linspace(xmin,xmax,100)\n    fys = np.linspace(ymin,ymax,100)\n    loss_fx = [loss([fxs[j],xs[0][1]]) for j in range(len(fxs))]\n    loss_fy = [loss([xs[0][0],fys[j]]) for j in range(len(fys))]\n    X,Y = np.meshgrid(fxs,fys)\n    Z = np.zeros_like(X)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Z[i,j] = loss([X[i,j],Y[i,j]])\n    # Add surface plot\n    surf = ax.plot_surface(X,Y,Z,cmap=\"gist_earth\")\n    ax1.set_xlim(np.min(X),np.max(X)); ax1.set_ylim(np.min(Z),np.max(Z))\n    ax2.set_xlim(np.min(Y),np.max(Y)); ax2.set_ylim(np.min(Z),np.max(Z))\n    plot1 = ax.plot(xs[0][0],xs[0][1],loss(xs[0]),zorder=100,color=\"red\",linestyle=\"\",marker=\"o\")[0]\n    plot2 = ax.plot([],[],[],color=\"orange\")[0]\n    # Add flat plots for perspective\n    plot3 = ax1.plot(fxs,loss_fx)[0]\n    plot4 = ax1.scatter(xs[0][0],loss(xs[0]),color=\"red\",s=100,zorder=100)\n    plot5 = ax2.plot(fys,loss_fy)[0]\n    plot6 = ax2.scatter(xs[0][1],loss(xs[0]),color=\"red\",s=100,zorder=100)\n    def anim_func(i):\n        x_loss = loss(xs[i])\n        plot1.set_data_3d(xs[i][0],xs[i][1],x_loss)\n        temp_x1 = [xs[j][0] for j in range(i)]\n        temp_x2 = [xs[j][1] for j in range(i)]\n        temp_losses = [loss(xs[j]) for j in range(i)]\n        plot2.set_data_3d(temp_x1,temp_x2,temp_losses)\n        loss_fx = [loss([fxs[j],xs[i][1]]) for j in range(len(fxs))]\n        loss_fy = [loss([xs[i][0],fys[j]]) for j in range(len(fys))]\n        plot3.set_data(fxs,loss_fx)\n        plot4.set_offsets([xs[i][0],x_loss])\n        plot5.set_data(fys,loss_fy)\n        plot6.set_offsets([xs[i][1],x_loss])\n        plots = [plot1,plot2,plot3,plot4,plot5,plot6]\n        return plots\n\n    tanim = anim.FuncAnimation(fig,anim_func,interval=50,frames=len(xs),blit=True)\n    if savefile == None:\n        plt.show()\n    else:\n        tanim.save(savefile)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom autograd import grad\nimport autograd.numpy as anp\ndef f_model(p):\n    part1 = anp.exp(-p[0]*xsamples)*(anp.sin((p[0]*xsamples)**3) + anp.sin((p[0]*xsamples)**2) - p[0]*xsamples) \n    part2 = 1/(1 + anp.exp(-p[1]*(xsamples-1)))\n    return part1 + part2\nloss = lambda p: anp.sum((f_model(p) - ysamples)**2)\ngrad_loss = grad(loss) # automatically differentiated\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = gradient_descent(grad_loss,p0,.005,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/gradient_descent1.gif)\n\nAlthough this behavior is somewhat typical of vanilla gradient descent, this model was pathologically chosen to be challenging.\nThe curvature of the loss function for each parameter at the correct parameter values are as follows:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting parameter loss curves\"}\nps = np.linspace(-.1,2.5,1000)\nfig,axs = plt.subplots(1,2,sharey=True,figsize=(9,4))\nax1,ax2 = axs\nax1.plot(ps,[loss(np.array([p_i,1])) for p_i in ps])\nax1.set_xlabel(\"$p_0$\"); ax1.set_ylabel(\"Loss\")\nax2.plot(ps,[loss(np.array([1,p_i])) for p_i in ps])\nax2.set_xlabel(\"$p_1$\")\n# plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\"); plt.title(\"Model\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop1_intro_files/figure-html/cell-7-output-1.png){width=733 height=356}\n:::\n:::\n\n\nIn this loss, $p_0$ demonstrates a plethora of local minima which could trap the descent algorithm while $p_1$ has a small gradient which will slow down the convergence.\nThe following methods are meant to simplify the choice of a learning rate while overcoming these specific convergence issues.\n\n### Adaptive steps\nDue to the challenges of determining a good learning rate (especially in models with many parameters and large variance in loss gradients), many methods have been developed to automatically adjust the $\\alpha$ parameter with each step and for each parameter.\nOne of the most common adaptive algorithms is called `adagrad` (for adaptive gradient. very creative).\nOriginally developed to provide parameter specific learning rates in sparse problems (in the case that some parameters are only occasionally important) it scales the learning rate by a squared sum of previous gradients:\n\n**adagrad:**\n$$\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\nv^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\nG^i &= \\sum_{j=0}^i (\\nabla_p \\mathcal{L}(x;\\vec{p}^j))^2\n\\end{align*}\n$$\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"adagrad code\"}\ndef adagrad(f_p,x0,alpha=.2,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_sq_grad = 0\n    for s in range(steps):\n        sum_sq_grad = f_p(x)**2 + sum_sq_grad\n        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = adagrad(grad_loss, p0,.2,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/adagrad.gif)\nHowever, depending on the problem, this scaled learning rate may slow down convergence considerably.\nAs an adjustment to remove this monotone decreasing learning rate, `RMSprop` attempts to balance the current gradient with a dampened version of the sum of squares of previous gradients from `adagrad`:\n\n**RMSprop:**\n$$\n\\begin{align*}\n\\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\nv^i &= -\\frac{\\alpha}{\\sqrt{G^i}} \\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\nG^i &= \\gamma G^{i-1} + (1-\\gamma)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n\\end{align*}\n$$\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"RMSprop code\"}\ndef rmsprop(f_p,x0,gamma=0.9,alpha=0.001,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_sq_grad = 0\n    for s in range(steps):\n        sum_sq_grad = (1-gamma)*(f_p(x)**2) + gamma*sum_sq_grad\n        v_i = -alpha*f_p(x)/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = rmsprop(grad_loss, p0,0.2,.05,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/rmsprop.gif)\nThis helps with the challenge of a monotone decreasing learning rate, but it introduces a dampening parameter $\\gamma$ that must be chosen (recommended values are $\\alpha = 0.001$ and $\\gamma = 0.9$).\n\n\n### With momentum\nThough the previous adaptive methods address the challenge of determining a learning rate, these methods are still likely to terminate in local minima.\nTo address this issue, there are several methods which utilize a concept of \"momentum\" to propel iterations out of local minima with the hope of landing in the global minimum.\nThis momentum is most simply added by incorporating previous gradients into the current update:\n\n**Gradient descent with momentum:**\n$$\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n    v^i &= -\\alpha G^i \\\\\n    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i) + \\gamma G^{i-1}\n\\end{align*}\n$$\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Gradient descent with momentum code\"}\ndef gradient_descent_momentum(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    for s in range(steps):\n        sum_grad = f_p(x) + gamma*sum_grad\n        v_i = -alpha*sum_grad\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = gradient_descent_momentum(grad_loss, p0,.9,.005,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/momentum.gif)\nwhere $\\gamma$ is a momentum parameter that determines how much of previous updates are kept for the current step ($0 <= \\gamma <=1$ where $\\gamma = 0$ includes no momentum).\n\nHowever, iterations that include this momentum may jump right out of global minima and/or delay convergence.\nTo incorporate a counterbalance to the momentum based on current success (to slow down in the right places), `Nesterov acceleration` adjusts the gradient according to an approximated step (to see how successful it may be in the future).\nBy so doing, it can effectively reduce or increase the momentum according to the next future iteration:\n\n**Nesterov accelerated gradient descent:**\n$$\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\\n    v^i &= -\\alpha G^i \\\\\n    G^i &= \\nabla_p \\mathcal{L}(x;\\vec{p}^i - \\gamma v^{i-1}) + \\gamma G^{i-1}\n\\end{align*}\n$$\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Gradient descent with Nesterov acceleration code\"}\ndef gradient_descent_nesterov(f_p,x0,gamma,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    for s in range(steps):\n        sum_grad = f_p(x-gamma*v_i) + gamma*sum_grad\n        v_i = -alpha*sum_grad\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = gradient_descent_nesterov(grad_loss, p0,.8,.002,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/nesterov.gif)\nwhere again $\\gamma$ is a momentum parameter.\nNotice that the only adjustment compared to vanilla gradient descent with momentum is calculating the gradient at an approximation of the next parameter values rather than at the current parameters.\n\n### Combining ideas\nOf course, the ideas of adaptive gradients and momentum both address important issues and are not mutually exclusive, so they can both be used simultaneously (with the downside of adding more parameters to tune).\nNote that the adaptive step size methods (`adagrad`,`RMSprop`) use a sum of squares of the previous gradient while the momentum methods (gradient descent with momentum or Nesterov acceleration) use a sum of the previous gradient.\nThese can be seen as the second moment and first moment of the previous gradients respectively.\nThe `Adam` (adaptive moment estimation) method uses both of these gradient moments to incorporate both momentum and adaptivity:\n\n**adam:**\n$$\n\\begin{align*}\n    \\vec{p}^{i+1} &= \\vec{p}^i + v^i \\\\ \n    v^i &= -\\frac{\\alpha}{\\sqrt{\\hat{v}^i}}\\hat{m}^i \\\\\n    m^i &= \\beta_1m^{i-1} + (1-\\beta_1)\\nabla_p \\mathcal{L}(x;\\vec{p}^i) \\\\\n    b^i &= \\beta_2v^{i-1} + (1-\\beta_2)(\\nabla_p \\mathcal{L}(x;\\vec{p}^i))^2\n\\end{align*}\n$$\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Adam code\"}\ndef adam(f_p,x0,beta1,beta2,alpha=0.01,tol=1e-12,steps=1000):\n    x = x0\n    xs = [x]\n    # --------- NEW -----------\n    sum_grad = 0\n    sum_sq_grad = 0\n    for s in range(1,steps):\n        sum_grad = beta1*sum_grad + (1-beta1)*f_p(x)\n        sum_sq_grad = beta2*sum_sq_grad + (1-beta2)*(f_p(x)**2)\n        v_i = -alpha*sum_grad/np.sqrt(sum_sq_grad)\n    # -------------------------\n        xnew = x + v_i\n        if np.linalg.norm(f_p(xnew)) < tol:\n            print(\"Converged to objective loss gradient below {} in {} steps.\".format(tol,s))\n            return x,xs\n        elif np.linalg.norm(xnew - x) < tol:\n            print(\"Converged to steady state of tolerance {} in {} steps.\".format(tol,s))\n            return x,xs\n        x = xnew\n        xs.append(x)\n    print(\"Did not converge after {} steps (tolerance {}).\".format(steps,tol))\n    return x,xs\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\np0 = np.array([2.1,.2])\nxs = adam(grad_loss, p0,0.9,0.99,.05,tol=1e-8,steps=100)[1]\nanimate_steps_2d(xs,loss)\n```\n:::\n\n\n![](videos/adam.gif)\nParameters $\\beta_1,\\beta_2$ represent decay rates in incorporating previous moments into the update step.\n\nAlthough this list is not comprehensive, it demonstrates the reasoning behind the common solutions for the challenges of using gradient descent in learning methods.\nFor a more comprehensive list of recently proposed methods, see[@john_chen_2020].\n\n### Automatic differentiation\nGradient descent methods rely on computing the gradient of the loss $\\nabla_p \\mathcal{L}$ for a given parameter set $\\vec{p}^i$.\nFor simple models, the gradients can be calculated by hand.\nHowever, for models with many nested functions and parameters (neural networks in particular) or methods whose form depends on hyperparameters, we will need an automated method.\nThe \"automatic differentiation\" method is the solution to these challenges.\nThough it was developed for neural networks and mainly used there, its breadth of applications is only just becoming apparent.\n(Consider the Julia language which has worked hard to make automatic differentiation possible everywhere).\n\nThere are both \"forward\" and \"backward\" automatic differentiation methods, but we will consider only the forward which best demonstrates the \"automatic\" moniker.\nTo do so, consider the first order expansion of two functions at a given point $a$:\n$$\n\\begin{align*}\nf(a + \\epsilon) &= f(a) + \\epsilon f'(a) \\\\\ng(a + \\epsilon) &= g(a) + \\epsilon g'(a)\n\\end{align*}\n$$\nwhere $\\epsilon$ is a small perturbation.\nBasic operations with these functions at this point can then be written as:\n$$\n\\begin{align*}\nf + g &= [f(a) + g(a)] + \\epsilon[f'(a) + g'(a)] \\\\\nf - g &= [f(a) - g(a)] + \\epsilon[f'(a) - g'(a)] \\\\\nf \\cdot g &= [f(a) \\cdot g(a)] + \\epsilon[f(a)\\cdot g'(a) + g(a)\\cdot f'(a)] \\\\\nf \\div g &= [f(a) \\div g(a)] + \\epsilon\\left[\\frac{f'(a)\\cdot g(a) + f(a)\\cdot g'(a)}{g(a)^2}\\right]\n\\end{align*}\n$$\nThe derivatives are then represented by the $\\epsilon$ terms in the above equalities.\n\nTo implement this on a computer, you can create a new \"Dual\" number that performs additions, subtractions, multiplications, and divisions with the above operating rules (and can be extended for other functions: $\\sin,\\cos,\\exp$,etc).\n\n:::{.callout-note}\nThis is cumbersome and slow to implement in Python which is why large and complicated libraries have been written in C (`tensorflow`,`pytorch`,`jax`, and others).\n\nIt can be cleanly and easily implemented in Julia.\nTo see a demonstration of this, see[@chris_rackauckas_2020].\n:::\n\nA quick demonstration of a simple implementation of this is as follows[@mostafa_samir_2020].\nDefine a dual number class:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nclass DualNumber:\n    def __init__(self, val, der):\n        self.val = val\n        self.der = der\n    def __add__(self, other):\n        if isinstance(other, DualNumber):\n            # DualNumber + DualNumber\n            return DualNumber(self.val + other.val, self.der + other.der)\n        else:\n            # DualNumber + a number\n            return DualNumber(self.val + other, self.der)\n    def __radd__(self, other):\n        # a number + DualNumber\n        return self.__add__(other)\n    def __mul__(self,other):\n        if isinstance(other, DualNumber):\n            # DualNumber * DualNumber\n            return DualNumber(self.val * other.val, self.val*other.der + self.der*other.val)\n        else:\n            # DualNumber * a number\n            return DualNumber(self.val * other, self.der * other)\n    def __rmul__(self,other):\n        # a number * DualNumber\n        return self.__mul__(other)\n    def __pow__(self,power):\n        # DualNumber ^ a number\n            return DualNumber(self.val ** power, self.der * power * (self.val ** (power - 1)))\n    def __repr__(self):\n        # Printing a DualNumber\n        return \"DualNumber({},{})\".format(self.val,self.der)\ndual1 = DualNumber(1,2); dual2 = DualNumber(3,4); other = 5\nprint(dual1,\"+\",dual2,\"=\",dual1+dual2)\nprint(dual1,\"+\",other,\"=\",dual1+other)\nprint(dual1,\"*\",dual2,\"=\",dual1*dual2)\nprint(dual1,\"*\",other,\"=\",dual1*other)\nprint(dual1,\"^\",2,\"=\",dual1**2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDualNumber(1,2) + DualNumber(3,4) = DualNumber(4,6)\nDualNumber(1,2) + 5 = DualNumber(6,2)\nDualNumber(1,2) * DualNumber(3,4) = DualNumber(3,10)\nDualNumber(1,2) * 5 = DualNumber(5,10)\nDualNumber(1,2) ^ 2 = DualNumber(1,4)\n```\n:::\n:::\n\n\nThis seems simple, but by using a few rules like this, we can compute simple polynomial derivatives at a point automatically.\nFor example, computing the derivative of $f(x) = 3x^3 + 2x + 4$ at $x=2$.\nWe first initialize the function and the dual number $(2,1)$ ($1$ because the derivative of $x$ is $1$).\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nf = lambda x: 3*x**3 + 2*x + 4\ndf = lambda x: 9*x**2 + 2\nx = 2; dualx = DualNumber(x,1)\n```\n:::\n\n\nWe then pass the dual number through the polynomial function, unpack the results, and compare the result with the true derivative $f'(2)$:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\npoly_dual = f(dualx)\ndual_val = poly_dual.val; dual_der = poly_dual.der\ntrue_der = df(x)\nprint(\"True derivative at x=2: \", true_der)\nprint(\"Dual number derivative: \", dual_der)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue derivative at x=2:  38\nDual number derivative:  38\n```\n:::\n:::\n\n\nUsing the rules for dual numbers we provided above, the derivative \"automatically\" popped out of the polynomial evaluation.\n\nIn contrast with the forward method, the backward method keeps a log of the operations performed and then uses defined chain rules (similar to how we defined rules for the forward pass) to backtrack from the final output back to the start. \nThis approach is more efficient when the dimension of the gradient is large and that of the output small.\nForward evaluation is more efficient when the dimension of the gradient is small and that of the output large.\n\nAll in all, automatic differentiation is programming with hard-coded differential rules for primitive operations.\nIt can also require keeping track of which operations happen and in what order.\nImplementing such a method relies heavily on computer science techniques since it boils down to parsing operation calls.\nThis represents one of the clearest differences in the approaches of machine learning to those of statistics or optimization due to its computer language-heavy principles.\nThrough this lens, machine learning could be viewed as \"inferential statistics using new tools from computer science for non-traditional problems.\"\n\n## Appendix\n\n### Python resources and packages\n**Python**\n\n- [Python documentation (especially sections 3-9)](https://docs.python.org/3.9/tutorial/introduction.html)\n- [Quick cheatsheet of general Python knowledge](https://www.pythoncheatsheet.org)\n- [Quicker introduction](https://learnxinyminutes.com/docs/python/)\n\n**Conda/mamba**\n\n- [Conda user guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html)\n- [Mamba website](https://mamba.readthedocs.io/en/latest/index.html)\n\n**Essential packages**\n\n- [`numpy`](https://numpy.org) - Creating, manipulating, and operating (linear algebra, fft, etc.) on multi-dimensional arrays. A list of packages built on `numpy` for a variety of domains can be found on the homepage under the _ECOSYSTEM_ heading.\n- [`scipy`](https://scipy.org) - Fundamentals in optimization, integration, interpolation, differential equations, statistics, signal processing, etc.\n- [`matplotlib`](https://matplotlib.org) - Static or interactive plots and animations\n- [`scikit-learn`](https://scikit-learn.org/stable/index.html) - Standard machine learning tools and algorithms built on `numpy`, `scipy`, and `matplotlib`\n- [`pandas`](https://pandas.pydata.org) - Easily represent, manipulate, and visualize structured datasets (matrices with names for columns and rows)\n- [`keras`](https://keras.io) - High level neural network framework built on `tensorflow`\n- [`tensorflow`](https://www.tensorflow.org) - In depth neural network framework focused on ease and production\n- [`pytorch`](https://pytorch.org) - In depth neural network framework focused on facilitating the path from research to production\n- [`scikit-image`](https://scikit-image.org) - Image processing algorithms and tools\n- [`jupyter`](https://jupyter.org) - Interactive \"notebook\" style programming\n\n### Julia as an alternative to Python\nJulia is a fairly new language that has been mainly proposed as an alternative to Python and Matlab, though it is general use.\nIts strength and its weakness is that it is \"just-in-time\" compiled (meaning your code is automatically analyzed and compiled just before it is run).\nA clever language design combined with just-in-time compilation makes Julia as clear to read and write as Python while being much faster.\nIt can even approach the speed of C when written carefully.\nHowever, the just-in-time compilation and type system remove a chunk of the interactive convenience of Python and its young age also means that it does not have the volume of packages that Python does.\n\nNonetheless, it is an elegant and high-performance language to use and has shown rapid growth recently.\nConcise, simple, and easy to read and contribute to packages have been quickly emerging and it already provides many useful tools.\nAs a result, it is worth describing it's installation process, environment management, and noteable packages.\n\n#### Installation\nThe officially supported method of installation for Julia is now using the `juliaup` version manager.\nThe [installer](https://github.com/JuliaLang/juliaup#windows) can be downloaded from the Windows store on Windows or run on MacOS or Linux with:\n\n```bash\ncurl -fsSL https://install.julialang.org | sh\n```\n#### Environments\nJulia comes with a standard environment and package manager named [`Pkg`](https://pkgdocs.julialang.org/v1/).\nInterestingly, the easiest way to use it is to run the Julia REPL (read-eval-print-loop), i.e. to run `julia` interactively.\nYou can do so by typing `julia` into the terminal.\nYou will then be presented with a terminal interface such as:\n```default\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.8.0 (2022-08-17)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia>\n```\nTyping `]` will put you into \"`Pkg` mode\":\n```default\n(@v1.8) pkg>\n```\nType `?` and hit enter to get options in this mode.\nWe can create and activate a new environment called workshop with the command:\n```default\n(@v1.8) pkg> activate --shared workshop\n```\nNote that the `--shared` flag will make a \"global\" environment that can be accessed from any directory.\nIf we were to leave out this flag, `Pkg` would put a `Project.toml` and `Manifest.toml` file in the current directory that contain the name of the environment, its installed packages, and their dependencies.\nThis can be useful to easily isolate and share environments.\nAfter running this command, our `Pkg` mode will have changed to represent the active environment:\n```default\n(@workshop) pkg>\n```\n\n#### Installing packages\nTo install some packages in the active environment, write:\n```default\n(@workshop) pkg> add Plots MLJ DataFrames Flux Pluto\n```\nThese packages will install and precompile.\nTo test one of them, press backspace to leave `Pkg` mode and input:\n```default\njulia> using Plots\n[ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\njulia> x = range(0,10,100);\njulia> y = sin.(x);\njulia> plot(x,y)\n```\nThis should show a plot of a simple $\\sin$ curve.\nNote that the precompilation of `Plots` took some time.\nHowever, this will not need to occur again until the package is updated.\nAlso note that the call to `plot(x,y)` took some time.\nThis is due to the just-in-time compilation.\nNow that the compilation has been done for inputs of the types of `x` and `y`, if you run `plot(x,y)` again, it should be almost instantaneous.\n\n#### Cleaning up\nTo deactivate the environment, enter the `Pkg` mode again by pressing `]` on an empty line, then enter:\n\n```default\n(@workshop) pkg> activate\n```\nTo delete the environment we created you can delete the environment folder at the listed location at creation.\nThis is usually `/home/user/.julia/environments/workshop` on MacOS or Linux.\n\n#### References\n\n**Julia**\n\n- [Julia documentation](https://docs.julialang.org/en/v1/)\n- [Quick cheatsheet of Julia](https://juliadocs.github.io/Julia-Cheat-Sheet/)\n- [Comparison of the syntax of Julia, Python, and Matlab](https://cheatsheets.quantecon.org)\n\n**Packages**\n\nAs compared to Python, Julia has many scientific computing tools built into its standard library.\nThus, a lot of the functionality found in `numpy` are loaded by default.\nOn the other hand, because of the interoperability of the language and the reduced need for a polyglot codebase (i.e. needing C and Fortran code for a Python package to be fast), packages are usually much smaller modules in Julia.\nFor example, the functionality of the `scipy` package in Python can be found spread across possibly a dozen different packages in Julia.\nThis is convenient to only load and use what you need, but inconvenient in that it may require more searching to find and the interfaces may not be standardized.\nThe following are some packages that roughly recreate the essential Python packages [here](#python-resources-and-packages). \n\n- `numpy` - [Standard library](https://docs.julialang.org/en/v1/manual/arrays/),[`FFTW.jl`](https://juliamath.github.io/FFTW.jl/latest/)\n- `scipy` - [`Statistics.jl`]()\n- `matplotlib` - [`Plots.jl`](https://docs.juliaplots.org/latest/)\n- `scikit-learn` - [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n- `pandas` - [`DataFrames.jl`](https://dataframes.juliadata.org/stable/)\n- `keras`,`tensorflow`,`pytorch` - [`Flux.jl`](https://fluxml.ai/Flux.jl/stable/)\n- `scikit-image` - [`Images.jl`](https://juliaimages.org/latest/install/)\n- `jupyter` - [`Pluto.jl`](https://github.com/fonsp/Pluto.jl) although you can use Julia with Jupyter via [`IJulia.jl`](https://julialang.github.io/IJulia.jl/stable/)\n\n",
    "supporting": [
      "workshop1_intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}