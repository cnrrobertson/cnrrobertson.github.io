{
  "hash": "f3c4c044e9efc71fe3b89c7e72ec6c9f",
  "result": {
    "markdown": "---\nexecute:\n  daemon: 500\nformat:\n  html: default\n  ipynb: default\ntitle: ‚ùÑÔ∏è Frozen Lake\n---\n\n\n\n\n\n **Frozen Lake** is a simple environment composed of tiles, where the AI has to **move from an initial tile to a goal**.\n\nTiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever. \n\nThe AI, or agent, has 4 possible actions: go **‚óÄÔ∏è LEFT**, **üîΩ DOWN**, **‚ñ∂Ô∏è RIGHT**, or **üîº UP**. \n\nThe agent must learn to avoid holes in order to reach the goal in a **minimal number of actions**.\n\n# Required Libraries \n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport gym\nimport random\nimport numpy as np\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n# Initialize the Environment\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nenv = gym.make(\"FrozenLake-v1\", is_slippery = False) #in non-slippery version actions cannot be ignored\nenv.reset()\nenv.render()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSFFF\nFHFH\nFFFH\nHFFG\n```\n:::\n:::\n\n\n* S: starting point, safe\n* F: frozen surface, safe\n* H: hole, stuck forever\n* G: goal, safe\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nImage(filename = \"FrozenLake.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nImage(filename = \"Final.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n# Reward\n\nReward schedule:\n\n* Reach goal(G): +1\n\n* Reach hole(H): 0\n\n* Reach frozen surface(F): 0\n\n# Size of Action and State Space\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(\"State space: \", env.observation_space.n)\nprint(\"Action space: \", env.action_space.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nState space:  16\nAction space:  4\n```\n:::\n:::\n\n\nIn Frozen Lake, there are **16 tiles**, which means our agent can be found in 16 different positions, called states. \n\nFor each state, there are **4 possible actions**: \n\n* ‚óÄÔ∏è LEFT: **0**\n* üîΩ DOWN: **1**\n* ‚ñ∂Ô∏è RIGHT: **2**\n* üîº UP: **3**\n\n# Initialize Q Table\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nImage(filename = \"QTable.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Our table has the following dimensions:\n# (rows x columns) = (states x actions) = (16 x 4)\n\nnb_states = env.observation_space.n  # = 16\nnb_actions = env.action_space.n      # = 4\nqtable = np.zeros((nb_states, nb_actions))\n\n# Let's see how it looks\nprint('Q-table =')\nprint(qtable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQ-table =\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n```\n:::\n:::\n\n\n# Update Formula\n\n### $ Q_{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha \\times (r_t + \\gamma \\times max_a Q(s_{t+1}, a) - Q(s_t, a_t)) $\n\n# Epsilon-Greedy Algorithm\n\nIn this method, we want to allow our agent to either:\n\n* Take the action with the highest value **(exploitation)**;\n* Choose a random action to try to find even better ones **(exploration)**.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nImage(filename = \"tradeoff.gif\", width=700)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n# Hyperparameters\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nepisodes = 1000        # Total number of episodes\nalpha = 0.5            # Learning rate\ngamma = 0.9            # Discount factor\nepsilon = 1.0          # Amount of randomness in the action selection\nepsilon_decay = 0.001  # Fixed amount to decrease\n```\n:::\n\n\n# Traning\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# List of outcomes to plot\noutcomes = []\n\nfor _ in range(episodes):\n    \n    state = env.reset()\n    done = False\n\n    # By default, we consider our outcome to be a failure\n    outcomes.append(\"Failure\")\n    \n    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n    while not done:\n        # Generate a random number between 0 and 1\n        rnd = np.random.random()\n\n        # If random number < epsilon, take a random action\n        if rnd < epsilon:\n            action = env.action_space.sample()\n        # Else, take the action with the highest value in the current state\n        else:\n            action = np.argmax(qtable[state])\n             \n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q(s,a)\n        qtable[state, action] = qtable[state, action] + \\\n                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n        \n        # Update our current state\n        state = new_state\n\n        # If we have a reward, it means that our outcome is a success\n        if reward:\n            outcomes[-1] = \"Success\"\n\n    # Update epsilon\n    epsilon = max(epsilon - epsilon_decay, 0)\n```\n:::\n\n\n# Updated Q Table\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprint('===========================================')\nprint('Q-table after training:')\nprint(qtable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n===========================================\nQ-table after training:\n[[0.531441   0.59049    0.59049    0.531441  ]\n [0.53144099 0.         0.6561     0.59048631]\n [0.59037976 0.729      0.59048952 0.65609994]\n [0.65609993 0.         0.50712647 0.57097084]\n [0.59049    0.6561     0.         0.531441  ]\n [0.         0.         0.         0.        ]\n [0.         0.81       0.         0.65609609]\n [0.         0.         0.         0.        ]\n [0.6561     0.         0.729      0.59049   ]\n [0.6561     0.81       0.81       0.        ]\n [0.729      0.9        0.         0.729     ]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.80990302 0.9        0.72145318]\n [0.81       0.9        1.         0.81      ]\n [0.         0.         0.         0.        ]]\n```\n:::\n:::\n\n\n# Plot Outcomes\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 5))\nplt.xlabel(\"Run number\")\nplt.ylabel(\"Outcome\")\nax = plt.gca()\nax.set_facecolor('gainsboro')\nplt.bar(range(len(outcomes)), outcomes, color=\"navy\", width=1.0)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop6_reinforcement_files/figure-html/cell-13-output-1.png){width=994 height=429}\n:::\n:::\n\n\n# Evaluation \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nepisodes = 1\nnb_success = 0\n\n\nstate = env.reset()\nenv.render()\ndone = False\n\n# Until the agent gets stuck or reaches the goal, keep training it\nwhile not done:\n    \n    # Choose the action with the highest value in the current state\n    action = np.argmax(qtable[state])\n\n    # Implement this action and move the agent in the desired direction\n    new_state, reward, done, info = env.step(action)\n\n    # Render the environment \n    print()\n    env.render()\n\n    # Update our current state\n    state = new_state\n\n    # When we get a reward, it means we solved the game\n    nb_success += reward\n\n# Let's check our success rate!\nprint()\nprint (f\"Success rate = {nb_success/episodes*100}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Down)\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Down)\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Right)\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Right)\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Down)\nSFFF\nFHFH\nFFFH\nHFFG\n\n  (Right)\nSFFF\nFHFH\nFFFH\nHFFG\n\nSuccess rate = 100.0%\n```\n:::\n:::\n\n\n",
    "supporting": [
      "workshop6_reinforcement_files"
    ],
    "filters": [],
    "includes": {}
  }
}