{
  "hash": "e412f0d96b38317714ea9e12490bcd52",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nexecute:\n  daemon: 500\nformat:\n  html: default\n  ipynb: default\ntitle: ‚ùÑÔ∏è Frozen Lake\n---\n\n\n\n\n\n **Frozen Lake** is a simple environment composed of tiles, where the AI has to **move from an initial tile to a goal**.\n\nTiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever.\n\nThe AI, or agent, has 4 possible actions: go **‚óÄÔ∏è LEFT**, **üîΩ DOWN**, **‚ñ∂Ô∏è RIGHT**, or **üîº UP**.\n\nThe agent must learn to avoid holes in order to reach the goal in a **minimal number of actions**.\n\n# Required Libraries\n\n\n::: {#9b8dceb9 .cell execution_count=1}\n``` {.python .cell-code}\nimport gymnasium as gym\nimport random\nimport numpy as np\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n# Initialize the Environment\n\n::: {#ad8f4932 .cell execution_count=2}\n``` {.python .cell-code}\nenv = gym.make(\"FrozenLake-v1\", is_slippery = False) #in non-slippery version actions cannot be ignored\nenv.reset()\nenv.render()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/cjrobe/miniforge3/envs/website/lib/python3.11/site-packages/gymnasium/envs/toy_text/frozen_lake.py:328: UserWarning: WARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n  gym.logger.warn(\n```\n:::\n:::\n\n\n* S: starting point, safe\n* F: frozen surface, safe\n* H: hole, stuck forever\n* G: goal, safe\n\n::: {#a34a3760 .cell execution_count=3}\n``` {.python .cell-code}\nImage(filename = \"FrozenLake.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n::: {#830606bd .cell execution_count=4}\n``` {.python .cell-code}\nImage(filename = \"Final.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n# Reward\n\nReward schedule:\n\n* Reach goal(G): +1\n\n* Reach hole(H): 0\n\n* Reach frozen surface(F): 0\n\n# Size of Action and State Space\n\n::: {#46f53586 .cell execution_count=5}\n``` {.python .cell-code}\nprint(\"State space: \", env.observation_space.n)\nprint(\"Action space: \", env.action_space.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nState space:  16\nAction space:  4\n```\n:::\n:::\n\n\nIn Frozen Lake, there are **16 tiles**, which means our agent can be found in 16 different positions, called states.\n\nFor each state, there are **4 possible actions**:\n\n* ‚óÄÔ∏è LEFT: **0**\n* üîΩ DOWN: **1**\n* ‚ñ∂Ô∏è RIGHT: **2**\n* üîº UP: **3**\n\n# Initialize Q Table\n\n::: {#3b73f093 .cell execution_count=6}\n``` {.python .cell-code}\nImage(filename = \"QTable.gif\", width=400)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n::: {#ad2e3043 .cell execution_count=7}\n``` {.python .cell-code}\n# Our table has the following dimensions:\n# (rows x columns) = (states x actions) = (16 x 4)\n\nnb_states = env.observation_space.n  # = 16\nnb_actions = env.action_space.n      # = 4\nqtable = np.zeros((nb_states, nb_actions))\n\n# Let's see how it looks\nprint('Q-table =')\nprint(qtable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQ-table =\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n```\n:::\n:::\n\n\n# Update Formula\n\n### $ Q_{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha \\times (r_t + \\gamma \\times max_a Q(s_{t+1}, a) - Q(s_t, a_t)) $\n\n# Epsilon-Greedy Algorithm\n\nIn this method, we want to allow our agent to either:\n\n* Take the action with the highest value **(exploitation)**;\n* Choose a random action to try to find even better ones **(exploration)**.\n\n::: {#530f638a .cell execution_count=8}\n``` {.python .cell-code}\nImage(filename = \"tradeoff.gif\", width=700)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<IPython.core.display.Image object>\n```\n:::\n:::\n\n\n# Hyperparameters\n\n::: {#6672b630 .cell execution_count=9}\n``` {.python .cell-code}\nepisodes = 1000        # Total number of episodes\nalpha = 0.5            # Learning rate\ngamma = 0.9            # Discount factor\nepsilon = 1.0          # Amount of randomness in the action selection\nepsilon_decay = 0.001  # Fixed amount to decrease\n```\n:::\n\n\n# Training\n\n::: {#f8b5c891 .cell execution_count=10}\n``` {.python .cell-code}\n# List of outcomes to plot\noutcomes = []\n\nfor _ in range(episodes):\n\n    state,info = env.reset()\n    done = False\n\n    # By default, we consider our outcome to be a failure\n    outcomes.append(\"Failure\")\n\n    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n    while not done:\n        # Generate a random number between 0 and 1\n        rnd = np.random.random()\n\n        # If random number < epsilon, take a random action\n        if rnd < epsilon:\n            action = env.action_space.sample()\n        # Else, take the action with the highest value in the current state\n        else:\n            action = np.argmax(qtable[state])\n\n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, _, info = env.step(action)\n\n        # Update Q(s,a)\n        qtable[state, action] = qtable[state, action] + \\\n                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n\n        # Update our current state\n        state = new_state\n\n        # If we have a reward, it means that our outcome is a success\n        if reward:\n            outcomes[-1] = \"Success\"\n\n    # Update epsilon\n    epsilon = max(epsilon - epsilon_decay, 0)\n```\n:::\n\n\n# Updated Q Table\n\n::: {#459636fa .cell execution_count=11}\n``` {.python .cell-code}\nprint('===========================================')\nprint('Q-table after training:')\nprint(qtable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n===========================================\nQ-table after training:\n[[0.531441   0.59049    0.59049    0.531441  ]\n [0.531441   0.         0.6561     0.59049   ]\n [0.59049    0.729      0.59049    0.6561    ]\n [0.6561     0.         0.59047945 0.59045162]\n [0.59049    0.6561     0.         0.531441  ]\n [0.         0.         0.         0.        ]\n [0.         0.81       0.         0.6561    ]\n [0.         0.         0.         0.        ]\n [0.6561     0.         0.729      0.59049   ]\n [0.6561     0.81       0.81       0.        ]\n [0.729      0.9        0.         0.729     ]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.80134387 0.9        0.72880834]\n [0.81       0.9        1.         0.81      ]\n [0.         0.         0.         0.        ]]\n```\n:::\n:::\n\n\n# Plot Outcomes\n\n::: {#77bb4639 .cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 5))\nplt.xlabel(\"Run number\")\nplt.ylabel(\"Outcome\")\nax = plt.gca()\nax.set_facecolor('gainsboro')\nplt.bar(range(len(outcomes)), outcomes, color=\"navy\", width=1.0)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop6_reinforcement_files/figure-ipynb/cell-13-output-1.png){}\n:::\n:::\n\n\n# Evaluation\n\n::: {#f130fab4 .cell execution_count=13}\n``` {.python .cell-code}\nepisodes = 1\nnb_success = 0\n\n\nstate,info = env.reset()\nenv.render()\ndone = False\n\n# Until the agent gets stuck or reaches the goal, keep training it\nwhile not done:\n\n    # Choose the action with the highest value in the current state\n    action = np.argmax(qtable[state])\n\n    # Implement this action and move the agent in the desired direction\n    new_state, reward, done, _, info = env.step(action)\n\n    # Render the environment\n    print()\n    env.render()\n\n    # Update our current state\n    state = new_state\n\n    # When we get a reward, it means we solved the game\n    nb_success += reward\n\n# Let's check our success rate!\nprint()\nprint (f\"Success rate = {nb_success/episodes*100}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n\n\n\n\n\nSuccess rate = 100.0%\n```\n:::\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.11.8\n---\n",
    "supporting": [
      "workshop6_reinforcement_files/figure-ipynb"
    ],
    "filters": []
  }
}