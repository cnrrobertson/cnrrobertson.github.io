{
  "hash": "78c63a8a367e53e4ae8f5a718d7ef976",
  "result": {
    "markdown": "---\ntitle: \"Workshop 3: Data-driven discovery via Sparse Identification of Nonlinear Dynamics (SINDy) with neural network approximation and differentiation\"\ncsl: ../../american-physics-society.csl\nbibliography: ../../workshops.bib\nauthor: Connor Robertson\nexecute:\n    daemon: 500\n#     keep-ipynb: true\nformat:\n    html: default\n    ipynb: default\n---\n\n## Overview\n\nOne common hallmark of popular machine learning methods is their \"black-box\" nature.\nSince many of these methods are meant solely for prediction, this has not been too much of an issue.\nAfter all, a black box method can be as complex as needed since it does not need to be analyzed after the fact.\nThis mentality has given birth to increasingly complex but effective models (just take a look at [the model](https://nikcheerla.github.io/deeplearningschool//media/alphago_arch.png) that defeated the worlds best Go player).\n\nHowever, there has been some recent interest in models that can be understood and analyzed.\nThis is particularly true in the scientific realm, where practicioners looking to use machine learning would like to get an idea of the mechanisms underlying their system of study.\nIn order to do so, new tools have been created and old, interpretable tools, such as linear regression, have been adapted to meet this challenge.\n\nMany of these new, interpretable, models have been named \"data-driven model discovery.\"\nTheir goals is to model collected data from a system with machine learning tools to determine a human-readable model.\n\n## Sparse Identification of Nonlinear Dynamics\nOne method for model discovery as described above is called Sparse Identification of Nonlinear Dynamics (SINDy)[@brunton2016discovering].\nThe goal of this method is to extract the most probable differential equation directly from data of the important state variables of a continuum system.\n\n### Setting up linear problem\nAs its name suggests, this method works discover models for linear or nonlinear systems.\nIt is based on a simple idea that nonlinear differential equations can be expressed as a linear combination of nonlinear terms[@williams2015data].\nAssuming we are looking at the nonlinear time evolution of some quantity, this could then be written as the sum of $K$ nonlinear terms:\n$$\nu_t(x,t) = \\xi_1\\mathcal{N}_1(u,x,t) + \\ldots + \\xi_K\\mathcal{N}_K(u,x,t)\n$$\nIf we can then determine what nonlinear terms are possible $\\mathcal{N}_i(u,x,t)$, we can sift through these terms to determine which best contribute to the time evolution of the system.\n\nUltimately, this boils down to a regression problem.\nGiven some space and time samples of our state variable: $u(x_i,t_j)$ for $i \\leq N$ and $j \\leq M$, we can consider the linear system:\n$$\nu_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)\n$$\nExpanded for all the data samples (flattened across space and time), this can be written as the system:\n$$\n\\begin{bmatrix}\nu_t(x_1, t_1) \\\\\n\\vdots \\\\\nu_t(x_N, t_1) \\\\\n\\vdots \\\\\nu_t(x_N, t_M) \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathcal{N}_1(x_1, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n\\vdots &  & \\vdots \\\\\n\\mathcal{N}_1(x_N, t_1) & \\ldots & \\mathcal{N}_K(x_1, t_1) \\\\\n\\vdots &  & \\vdots \\\\\n\\mathcal{N}_1(x_N, t_M) & \\ldots & \\mathcal{N}_K(x_1, t_1)\n\\end{bmatrix}\n\\vec{\\xi}\n$$ {#eq-linear-system}\n\nSolving this system is then a straightforward linear regression.\n\n### Determining nonlinear \"library\" of terms\nDetermining what $\\mathcal{N}_i(u,x,t)$ are reasonable for the system is somewhat of a traditional modeling problem.\nAre there any symmetries in the system that need to be satisfied?\nIs there periodic behavior that might warrant inclusion of trignometric terms?\nWhat order of polynomial interactions are possible for the system?\n\nThe most common library of terms for a 1D function is to put together polynomial interactions with spatial derivatives.\nSuch a library up to 3rd order polynomials and derivatives could be written:\n$$\n\\begin{align*}\n\\mathcal{N}_1(u,x,t) &= u\\\\\n\\mathcal{N}_2(u,x,t) &= u^2\\\\\n&\\vdots \\\\\n\\mathcal{N}_i(u,x,t) &= u_x\\\\\n\\mathcal{N}_{i+1}(u,x,t) &= u_x^2\\\\\n&\\vdots \\\\\n\\mathcal{N}_K(u,x,t) &= u^3u_{xxx}\\\\\n\\end{align*}\n$$\n\n### Numerical differentiation of the terms\nIn order to actually compute the values in the linear system written in @eq-linear-system, we must compute numerical derivatives in both $t$ and in $x$.\nThis isn't an issue if we have smooth, reliable data and can be quickly computed with finite differences.\n\nHowever, the intent of this method is to use data samples $u(x_i,t_j)$ that are collected from the real world, implying that they will each be polluted with some level of noise.\nThere have been several classical methods presented for dealing with numerical differentiation of noisy data that could be used, but generally the methods revolve around an approximate fitting of a differentiable function basis to the data.\nNotable among these are:\n\n- Local polynomial regression (LOESS[@cleveland1988locally], Savitsky-Golay filter[@press1990savitzky], etc.)\n- Radial basis functions (Gaussian kernel) \n- Smoothing splines\n- Least squares spectral analysis (LSSA)\n\nThese can be written along the lines of:\n$$\n\\underset{\\vec{c}}{\\text{argmin}} \\; \\sum_{i,j}^{N,M}\\|u(x_i,t_j) - F(x_i,t_j,\\vec{c})\\|_2\n$$\nwhere\n$$\nF(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\phi_l(x_i,t_j)\n$$\nand $\\phi$ represents our chosen basis function.\nOnce computed, we can easily approximate derivatives of $u$ via:\n$$\nu_x(x_i,t_j) \\approx F_x(x_i,t_j,\\vec{c}) = \\sum_l^L c_l \\frac{d}{dx}\\phi_l(x_i,t_j)\n$$\n\n\n\nEach of these has the goal of smoothing the given data while simultaneously providing an exact derivative of the approximation.\nThis is a similar idea as we have discussed with automatic differentiation of neural networks.\nIn fact, you could consider fitting a neural network to be the same as fitting a randomly initialized nested basis of nonlinear functions (since they are dense according to the universal approximation theorem).\nWe will explore this idea in the example problem in @sec-simulated.\n\n### Sparse regression\nOnce the matrix in @eq-linear-system has been created using numerical differentiation, it remains to sift through the nonlinear terms to determine which, if any, contribute to the time evolution of our state variable of interest.\nIt is usually reasonable to consider that not all the nonlinear terms should be included in the equation, so we would like to determine the most parsimonious (smallest) combination of them that will capture our desired qualitative and quantitative behavior in the system.\n\nThere are two main families of sparse regression methods:\n\n**Greedy methods**: Iterative add/remove terms that best match the time derivative in some metric ($R^2$ coefficient of determination, Akaike Information Criteria (AIC), etc.).\n\n- Forward selection: Start with no terms, add one by one according to which maximizes $R^2$ or AIC at each step\n- Backward selection: Start with all terms, remove one by one according to which least reduces $R^2$ or AIC\n- (Orthogonal) Matching pursuit: Start with no terms, add one by one according to which maximizes correlation (orthogonalizing after each step)\n\n**Regularization methods**: Add a penalty to the regression for having too many terms or large coefficients $\\xi_i$.\nThese can be written roughly as:\n$$\n\\underset{\\vec{\\xi}}{\\text{argmin}}\\; \\|u_t(x_i,t_j) - \\mathbf{\\mathcal{N}}(u_{ij},x_i,t_j) \\cdot \\vec{\\xi}\\|_2^2 + \\lambda \\|\\xi\\|_C\n$$\n\n- Ridge regression: Let $C=2$ forcing coefficients $\\vec{\\xi}$ to be smaller. We hope that important coefficients will remain larger while unimportant ones shrink.\n- Lasso regression: Let $C=1$ forcing coefficients $\\vec{\\xi}$ to be smaller and various to be set to 0 (due to the geometry of the 1-norm).\n- 0-norm regression: Let $C=0$ which is a measure that counts the number of nonzero coefficients in $\\vec{\\xi}$. Computing this usually requires a combination of regularization and relaxation best captured by the SR3 method[@zheng2018unified].\n\nCombinations of these two methods which iterative perform regularization methods removing terms with small coefficients according to a given threshold have also been proposed (Sequential Threshold Ridge Regression[@rudy2017data] or the original SINDy algorithm[@brunton2016discovering]).\n\n### Summary of the method\nIn summary, the procedure to use SINDy is as follows:\n\n1. Collect sample points of a continuum state variable of interest $u(x_i,t_j)$\n2. Form a \"library\" of possible terms for the differential model of the system $\\mathcal{N}_k(u,x,t)$\n3. Compute the libary at sample points using noise robust numerical differentiation to compute both $u_t(x_i,t_j)$ and $\\mathcal{N}_k(u_{ij},x_i,t_j)$\n4. Use sparse regression to determine a sparse vector $\\vec{\\xi}$ which closely approximates $u_t(x_i,t_j) = \\xi_1\\mathcal{N}_1(u_{ij},x_i,t_j) + \\ldots + \\xi_K\\mathcal{N}_K(u_{ij},x_i,t_j)$\n\nTo really explore this method, we will walk through this process using simulated traveling wave data in @sec-simulated and using real extracted data in @sec-extracted.\n\n## Application to simulated wave data {#sec-simulated}\n\n:::{.callout-note}\nFor this workshop you will need to install the following packages:\n\n```bash\nmamba install numpy matplotlib py-pde sympy jax optax flax scikit-learn scikit-image av\n```\n:::\n\nGiven some data generated via finite differences of the simple advection equation:\n$$\nh_t(x,t) = h_x(x,t)\n$$\nwith periodic boundaries and a Gaussian initial condition, we have the following measurement of state variable $h$ (height of the wave):\n\n::: {#4708beea .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Generate simple wave data\"}\nimport numpy as np\nimport pde\nimport matplotlib.pyplot as plt\n\n# Domain\nxmax = 1.0\nnx = 100\ndt = 1e-6\ntmax = 1.0-2*dt\nsave_dt = 0.01\ninit_cond = \".1*exp(-(1/.01)*(x-0.3)**2)\"\n\ngrid = pde.CartesianGrid([(0.0,xmax)],nx,periodic=True)\nh = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\neq = pde.PDE({\"h\": \"-d_dx(h)\"})\nstorage = pde.MemoryStorage()\n\nresult = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt),ret_info=False)\n\n# pde.plot_kymograph(storage)\nmovie = pde.visualization.movie(storage,\"simple_wave.gif\")\n\nh=np.array(storage.data)\nx=storage.grid.coordinate_arrays[0]\nt=np.array(storage.times)\nnp.savez(\"simple_wave.npz\",h=h,x=x,t=t)\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n```{.json}\n{\"model_id\":\"6daac99cce254f48b987c57b5bd424ca\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n```\n:::\n:::\n\n\n![](simple_wave.gif)\n\n### Generating nonlinear library\nGenerating a library can be most easily accomplished using the `sympy` symbolic math Python library.\nTo be overly thorough, we will generate up to 4th order polynomial combinations of up to 4th order spatial derivatives.\n\nWe can first initialize our spatial and state variables:\n\n::: {#49f6b104 .cell execution_count=2}\n``` {.python .cell-code}\nimport sympy as sp\n\nx_sym,t_sym = sp.symbols(\"x t\")\nh_sym = sp.Function(\"h\")\n```\n:::\n\n\nGiven a specified order, we can now create symbolic derivative terms (constructed to be most legible):\n\n::: {#4c472948 .cell execution_count=3}\n``` {.python .cell-code}\n# Library parameters\nmax_poly_order = 4\nmax_diff_order = 4\n\ndiff_terms = [h_sym(x_sym,t_sym)]\ndiff_terms += [sp.Function(str(h_sym)+\"_\"+(i*str(x_sym)))(x_sym,t_sym) for i in range(1,max_diff_order+1)]\nprint(diff_terms)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t)]\n```\n:::\n:::\n\n\nNow, combining these into polynomials up to 4th order (again, this is overkill, but for a system you don't fully understand, you may want to have a very complete library):\n\n::: {#5dec9d5a .cell execution_count=4}\n``` {.python .cell-code}\nfrom itertools import combinations_with_replacement\n\nterms = []\nfor po in range(max_poly_order+1):\n    if po == 0:\n        term = sp.core.numbers.One()\n    else:\n        combos = combinations_with_replacement(diff_terms,po)\n        for combo in combos:\n            term = 1\n            for combo_term in combo:\n                term *= combo_term\n            terms.append(term)\nprint(terms)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[h(x, t), h_x(x, t), h_xx(x, t), h_xxx(x, t), h_xxxx(x, t), h(x, t)**2, h(x, t)*h_x(x, t), h(x, t)*h_xx(x, t), h(x, t)*h_xxx(x, t), h(x, t)*h_xxxx(x, t), h_x(x, t)**2, h_x(x, t)*h_xx(x, t), h_x(x, t)*h_xxx(x, t), h_x(x, t)*h_xxxx(x, t), h_xx(x, t)**2, h_xx(x, t)*h_xxx(x, t), h_xx(x, t)*h_xxxx(x, t), h_xxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t), h_xxxx(x, t)**2, h(x, t)**3, h(x, t)**2*h_x(x, t), h(x, t)**2*h_xx(x, t), h(x, t)**2*h_xxx(x, t), h(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_xxx(x, t)**2, h(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xxxx(x, t)**2, h_x(x, t)**3, h_x(x, t)**2*h_xx(x, t), h_x(x, t)**2*h_xxx(x, t), h_x(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xxxx(x, t)**2, h_xx(x, t)**3, h_xx(x, t)**2*h_xxx(x, t), h_xx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)*h_xxxx(x, t)**2, h_xxx(x, t)**3, h_xxx(x, t)**2*h_xxxx(x, t), h_xxx(x, t)*h_xxxx(x, t)**2, h_xxxx(x, t)**3, h(x, t)**4, h(x, t)**3*h_x(x, t), h(x, t)**3*h_xx(x, t), h(x, t)**3*h_xxx(x, t), h(x, t)**3*h_xxxx(x, t), h(x, t)**2*h_x(x, t)**2, h(x, t)**2*h_x(x, t)*h_xx(x, t), h(x, t)**2*h_x(x, t)*h_xxx(x, t), h(x, t)**2*h_x(x, t)*h_xxxx(x, t), h(x, t)**2*h_xx(x, t)**2, h(x, t)**2*h_xx(x, t)*h_xxx(x, t), h(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxx(x, t)**2, h(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h(x, t)**2*h_xxxx(x, t)**2, h(x, t)*h_x(x, t)**3, h(x, t)*h_x(x, t)**2*h_xx(x, t), h(x, t)*h_x(x, t)**2*h_xxx(x, t), h(x, t)*h_x(x, t)**2*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)**2, h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxx(x, t), h(x, t)*h_x(x, t)*h_xx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxx(x, t)**2, h(x, t)*h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_x(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xx(x, t)**3, h(x, t)*h_xx(x, t)**2*h_xxx(x, t), h(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxx(x, t)**3, h(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h(x, t)*h_xxxx(x, t)**3, h_x(x, t)**4, h_x(x, t)**3*h_xx(x, t), h_x(x, t)**3*h_xxx(x, t), h_x(x, t)**3*h_xxxx(x, t), h_x(x, t)**2*h_xx(x, t)**2, h_x(x, t)**2*h_xx(x, t)*h_xxx(x, t), h_x(x, t)**2*h_xx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxx(x, t)**2, h_x(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)**2*h_xxxx(x, t)**2, h_x(x, t)*h_xx(x, t)**3, h_x(x, t)*h_xx(x, t)**2*h_xxx(x, t), h_x(x, t)*h_xx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxx(x, t)**2, h_x(x, t)*h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t), h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxx(x, t)**3, h_x(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_x(x, t)*h_xxxx(x, t)**3, h_xx(x, t)**4, h_xx(x, t)**3*h_xxx(x, t), h_xx(x, t)**3*h_xxxx(x, t), h_xx(x, t)**2*h_xxx(x, t)**2, h_xx(x, t)**2*h_xxx(x, t)*h_xxxx(x, t), h_xx(x, t)**2*h_xxxx(x, t)**2, h_xx(x, t)*h_xxx(x, t)**3, h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t), h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2, h_xx(x, t)*h_xxxx(x, t)**3, h_xxx(x, t)**4, h_xxx(x, t)**3*h_xxxx(x, t), h_xxx(x, t)**2*h_xxxx(x, t)**2, h_xxx(x, t)*h_xxxx(x, t)**3, h_xxxx(x, t)**4]\n```\n:::\n:::\n\n\n### Approximating data\nIn order to provide numerical derivatives of our data, we will use a neural network approximation.\n\n:::{.callout-note}\nThis is far beyond what is necessary for this particular setting, but is a method that can generalize to data not on a uniform grid and in high dimension, which can be useful.\nThe lack of requirement for a grid can also help with robustly fitting to noisy data by using a train-test methodology in @sec-noisy which classical basis functions do not handle well.\nUsing neural networks in this way as a combination with SINDy is explored more in[@xu2019dl].\n:::\n\nTo begin, we will be using the Google developed [`flax`](https://flax.readthedocs.io/en/latest/) neural network framework which is built on their [`jax`](https://jax.readthedocs.io/en/latest/index.html) automatic differentiation library and the [`optax`](https://optax.readthedocs.io/en/latest/optax-101.html) optimization library.\nThe reason for this will become clearer when we consider taking a fourth order derivative in $x$ of the network, a task which many other popular frameworks (`pytorch`, `keras`, `tensorflow`, etc.) cannot do (at least not nearly as concisely).\nHowever, the `jax` library is state-of-the-art for automatic differentiation and is used heavily for differentiable programming and neural network research today (see Appendix for more information). \n\n#### Creating the neural network model\nFirst, we will create a simple dense neural network model using the $\\tanh$ activation (to ensure a smooth approximation):\n\n::: {#12536cff .cell execution_count=5}\n``` {.python .cell-code}\nimport flax.linen as nn\n\nclass MyNet(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(60)(x)\n        x = nn.tanh(x)\n        x = nn.Dense(12)(x)\n        x = nn.tanh(x)\n        x = nn.Dense(1)(x)\n        return x\n```\n:::\n\n\nThis model will take an input of $(x_i,t_j)$ (a dimension 2 array), linearly map it to a dimension 60 space, apply a tanh activation, linearly map to a dimension 12 space, apply a tanh activation, then linearly map to a dimension 1 output (this particular width and depth was chosen arbitrarily).\n\nWe next initialize the parameters of the network (each of the linear transformation matrices) and print out the dimensions of the corresponding arrays:\n\n::: {#02978c61 .cell execution_count=6}\n``` {.python .cell-code}\nimport jax\njax.config.update(\"jax_platform_name\", \"cpu\")\n\n# Random generator seed\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel1 = MyNet()\nparams1 = model1.init(rng2,random_data)\nprint(jax.tree_util.tree_map(lambda x: x.shape, params1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFrozenDict({\n    params: {\n        Dense_0: {\n            bias: (60,),\n            kernel: (2, 60),\n        },\n        Dense_1: {\n            bias: (12,),\n            kernel: (60, 12),\n        },\n        Dense_2: {\n            bias: (1,),\n            kernel: (12, 1),\n        },\n    },\n})\n```\n:::\n:::\n\n\n:::{.callout-note}\nThe confusing `tree_util.tree_map` command is a convenience function for mapping a function (in this case `lambda x: x.shape`) across a set of different objects.\nThis is useful because these objects can be arrays, dictionaries, lists, classes (i.e. other neural networks), etc.\n:::\n\n#### Loading and processing data\nIn order to fit this model to the data, we must load the data into batches of $(x_i,t_j,u(x_i,t_j))$ points.\nSince our data is known to be quite smooth and we want to maximize the fit, we will use batches of size 10000:\n\n::: {#964d3c99 .cell execution_count=7}\n``` {.python .cell-code}\nimport jax.numpy as jnp\n\ndef load_data(data_path,noise_scale=0,norm=True):\n    raw_data = np.load(data_path)\n    h = raw_data[\"h\"].astype(jnp.float32)\n    x = raw_data[\"x\"].astype(jnp.float32)\n    t = raw_data[\"t\"].astype(jnp.float32)\n\n    # Add noise if needed\n    h += noise_scale*jnp.std(h)*np.random.normal(size=h.shape)\n\n    # Mean center, std center data\n    if norm:\n        h = (h - jnp.mean(h)) / jnp.std(h)\n        x = (x - jnp.mean(x)) / jnp.std(x)\n        t = (t - jnp.mean(t)) / jnp.std(t)\n    return x,t,h\n\ndef batch_data(x,t,h,batch_size):\n    # Split data into batches\n    data = []\n    for i in range(0,len(x),batch_size):\n        temp_xt = jnp.vstack((x[i:i+batch_size], t[i:i+batch_size])).T\n        temp_h = h[i:i+batch_size].reshape((-1,1))\n        data.append((temp_xt,temp_h))\n    return data\n\nx,t,h = load_data(\"simple_wave.npz\")\nX,T = jnp.meshgrid(x,t)\ndata = batch_data(X.flatten(),T.flatten(),h.flatten(),10000)\n```\n:::\n\n\nNote that the data needed to be centered and scaled to have a mean of $\\bar{h}=0$ and standard deviation of $\\overline{(h - \\bar{h})}=1$ in order to best use the $\\tanh$ activation (which extends from -1 to 1). \n\n#### Training the model\nWe will use the mean squared error fit of the data to our neural network output (just in time compiled with `@jax.jit` for maximum speed):\n\n::: {#b08fd94e .cell execution_count=8}\n``` {.python .cell-code}\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model1.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n```\n:::\n\n\nWith this loss defined, we initialize an ADAM optimizer and optimizer state and wrap the loss function to return both the output and gradient:\n\n::: {#af929c09 .cell execution_count=9}\n``` {.python .cell-code}\nimport optax\n\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params1)\n```\n:::\n\n\nWe can now train the model to take in $(x_i,t_j)$ and output $u(x_i,t_j)$.\nPerforming 1000 iterations over the data, we will print the mean squared error on the data as we proceed with the training:\n\n::: {#d88af751 .cell execution_count=10}\n``` {.python .cell-code}\nepochs = 1000\nall_xt = jnp.array([data[i][0] for i in range(len(data))])\nall_h = jnp.array([data[i][1] for i in range(len(data))])\nfor i in range(epochs):\n    xt_batch = data[i%len(data)][0]\n    h_batch = data[i%len(data)][1]\n    loss_val, grads = loss_grad_fn(params1, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params1 = optax.apply_updates(params1, updates)\n    if i % 100 == 0:\n        train_loss = mse(params1,all_xt,all_h)\n        print(\"Training loss step {}: {}\".format(i,train_loss))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining loss step 0: 1.089915156364441\nTraining loss step 100: 0.16426406800746918\nTraining loss step 200: 0.09340785443782806\nTraining loss step 300: 0.01907801441848278\nTraining loss step 400: 0.0014767495449632406\nTraining loss step 500: 0.000554197293240577\nTraining loss step 600: 0.00032399679184891284\nTraining loss step 700: 0.00021209089027252048\nTraining loss step 800: 0.0001550953311379999\nTraining loss step 900: 0.0001249150518560782\n```\n:::\n:::\n\n\nAs you can tell, this procedure is somewhat more manual than other libraries such as `keras` but keep you closer to the details, allowing for more flexibility in implementation.\n\n#### Validating fit\nThe fit to the model can be visualized as follows:\n\n::: {#2dce835d .cell execution_count=11}\n``` {.python .cell-code}\nimport matplotlib.animation as anim\n\nX,T = jnp.meshgrid(x,t)\nxt_points = jnp.vstack([X.flatten(),T.flatten()]).T\nhhat1 = model1.apply(params1,xt_points).reshape(X.shape)\ndiff = np.sqrt((h - hhat1)**2)\n\ndef animate_data(x,t,data_list,labels):\n    fig = plt.figure()\n    plt.xlabel(\"$x$\")\n    plots = []\n\n    for i in range(len(data_list)):\n        plot = plt.plot(x,data_list[i][0,:],label=labels[i])[0]\n        plots.append(plot)\n\n    def anim_func(j):\n        for i in range(len(plots)):\n            plots[i].set_ydata(data_list[i][j,:])\n        return plots\n\n    plt.legend()\n    approx_anim = anim.FuncAnimation(fig, anim_func, range(len(t)))\n    return approx_anim\n\nanimation1 = animate_data(x,t,[h,hhat1,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation1.save(\"clean_h_compare.gif\")\nplt.close()\n```\n:::\n\n\n![](clean_h_compare.gif)\n\n### Numerically differentiating the neural network model\nThe original reason to fit this model to the data was to be able to construct each of the terms in our nonlinear libary for the system.\nIn order to differentiate the model, we must wrap it in a function that takes our inputs and returns the output. \n\n::: {#2e52957e .cell execution_count=12}\n``` {.python .cell-code}\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model1.apply(params1, new_x)[0]\n\n# Take a derivative with respect to the first input (x) at point (x_i,t_j)\nx_i = 0.3; t_j = 0.3\njax.grad(model_for_diff,0)(x_i,t_j)\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\nArray(0.02786821, dtype=float32, weak_type=True)\n```\n:::\n:::\n\n\n:::{.callout-note}\nIf we were to differentiate the model directly, we would compute derivatives for all the parameters!\nThis is the main challenge with using other neural network frameworks for this kind of function approximation.\n:::\nApplying this iteratively, we can construct derivatives $h_x(x,t), \\ldots, h_{xxxx}(x,t)$ as is required by our library:\n\n::: {#178a05d5 .cell execution_count=13}\n``` {.python .cell-code}\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))\n```\n:::\n\n\nWe can then reconstruct our terms attaching them to their corresponding values on our $(x,t)$ grid:\n\n::: {#45e6f0fc .cell execution_count=14}\n``` {.python .cell-code}\ndef construct_terms(diff_term_values):\n    term_values = {}\n    term_shape = np.shape(diff_term_values[list(diff_term_values.keys())[0]])\n    for order in range(max_poly_order+1):\n        if order == 0:\n            term = sp.core.numbers.One()\n            term_values[term] = np.ones(term_shape)\n        else:\n            combos = combinations_with_replacement(diff_terms,order)\n            for combo in combos:\n                term = 1\n                temp_term_value = 1\n                for combo_term in combo:\n                    term *= combo_term\n                    temp_term_value *= diff_term_values[combo_term]\n                term_values[term] = temp_term_value\n    return term_values\nterm_values = construct_terms(diff_term_values)\n```\n:::\n\n\nFinally, we compute the derivative of the network with respect to time:\n\n::: {#1bb00616 .cell execution_count=15}\n``` {.python .cell-code}\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))\n```\n:::\n\n\n### Solving the sparse regression problem\nIn order to cleanly work with our term library, we will use a very popular Python data science package called `pandas`.\nSimply put, this library allows you to easily load, manipulate, and save tabular data.\nHere is our library as a `pandas` `DataFrame`:\n\n::: {#41f4396e .cell execution_count=16}\n``` {.python .cell-code}\nimport pandas as pd\n\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\nterm_matrix\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>1</th>\n      <th>h(x, t)</th>\n      <th>h_x(x, t)</th>\n      <th>h_xx(x, t)</th>\n      <th>h_xxx(x, t)</th>\n      <th>h_xxxx(x, t)</th>\n      <th>h(x, t)**2</th>\n      <th>h(x, t)*h_x(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n    <tr>\n      <th>x</th>\n      <th>t</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1.71</th>\n      <th>-1.71</th>\n      <td>1.0</td>\n      <td>-0.586198</td>\n      <td>0.034345</td>\n      <td>0.328429</td>\n      <td>3.294152</td>\n      <td>31.305639</td>\n      <td>0.343628</td>\n      <td>-0.020133</td>\n      <td>-0.192524</td>\n      <td>-1.931026</td>\n      <td>...</td>\n      <td>105.712776</td>\n      <td>11.740110</td>\n      <td>111.570938</td>\n      <td>1060.302979</td>\n      <td>1.007648e+04</td>\n      <td>117.753731</td>\n      <td>1119.060547</td>\n      <td>1.063488e+04</td>\n      <td>1.010675e+05</td>\n      <td>9.604844e+05</td>\n    </tr>\n    <tr>\n      <th>-1.68</th>\n      <th>-1.71</th>\n      <td>1.0</td>\n      <td>-0.584787</td>\n      <td>0.047935</td>\n      <td>0.463436</td>\n      <td>4.565125</td>\n      <td>42.639248</td>\n      <td>0.341976</td>\n      <td>-0.028032</td>\n      <td>-0.271011</td>\n      <td>-2.669625</td>\n      <td>...</td>\n      <td>390.480194</td>\n      <td>44.090813</td>\n      <td>411.817627</td>\n      <td>3846.464600</td>\n      <td>3.592680e+04</td>\n      <td>434.321045</td>\n      <td>4056.651611</td>\n      <td>3.788999e+04</td>\n      <td>3.539006e+05</td>\n      <td>3.305508e+06</td>\n    </tr>\n    <tr>\n      <th>-1.65</th>\n      <th>-1.71</th>\n      <td>1.0</td>\n      <td>-0.582814</td>\n      <td>0.067048</td>\n      <td>0.650005</td>\n      <td>6.293957</td>\n      <td>57.914425</td>\n      <td>0.339672</td>\n      <td>-0.039077</td>\n      <td>-0.378832</td>\n      <td>-3.668204</td>\n      <td>...</td>\n      <td>1417.120361</td>\n      <td>162.064529</td>\n      <td>1491.251587</td>\n      <td>13721.887695</td>\n      <td>1.262632e+05</td>\n      <td>1569.260986</td>\n      <td>14439.698242</td>\n      <td>1.328682e+05</td>\n      <td>1.222599e+06</td>\n      <td>1.124986e+07</td>\n    </tr>\n    <tr>\n      <th>-1.61</th>\n      <th>-1.71</th>\n      <td>1.0</td>\n      <td>-0.580054</td>\n      <td>0.093777</td>\n      <td>0.906569</td>\n      <td>8.635473</td>\n      <td>78.191200</td>\n      <td>0.336462</td>\n      <td>-0.054395</td>\n      <td>-0.525858</td>\n      <td>-5.009038</td>\n      <td>...</td>\n      <td>5024.779785</td>\n      <td>583.793213</td>\n      <td>5286.043945</td>\n      <td>47863.289062</td>\n      <td>4.333854e+05</td>\n      <td>5560.893066</td>\n      <td>50351.949219</td>\n      <td>4.559194e+05</td>\n      <td>4.128191e+06</td>\n      <td>3.737933e+07</td>\n    </tr>\n    <tr>\n      <th>-1.58</th>\n      <th>-1.71</th>\n      <td>1.0</td>\n      <td>-0.576197</td>\n      <td>0.130949</td>\n      <td>1.257589</td>\n      <td>11.781665</td>\n      <td>104.532784</td>\n      <td>0.332003</td>\n      <td>-0.075453</td>\n      <td>-0.724619</td>\n      <td>-6.788559</td>\n      <td>...</td>\n      <td>17281.535156</td>\n      <td>2056.641846</td>\n      <td>18247.546875</td>\n      <td>161901.296875</td>\n      <td>1.436469e+06</td>\n      <td>19267.558594</td>\n      <td>170951.343750</td>\n      <td>1.516765e+06</td>\n      <td>1.345749e+07</td>\n      <td>1.194016e+08</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1.58</th>\n      <th>1.71</th>\n      <td>1.0</td>\n      <td>-0.595861</td>\n      <td>-0.032445</td>\n      <td>0.094316</td>\n      <td>0.828272</td>\n      <td>6.051974</td>\n      <td>0.355050</td>\n      <td>0.019333</td>\n      <td>-0.056199</td>\n      <td>-0.493535</td>\n      <td>...</td>\n      <td>0.325811</td>\n      <td>0.053593</td>\n      <td>0.391587</td>\n      <td>2.861229</td>\n      <td>2.090628e+01</td>\n      <td>0.470643</td>\n      <td>3.438868</td>\n      <td>2.512695e+01</td>\n      <td>1.835964e+02</td>\n      <td>1.341493e+03</td>\n    </tr>\n    <tr>\n      <th>1.61</th>\n      <th>1.71</th>\n      <td>1.0</td>\n      <td>-0.596922</td>\n      <td>-0.028636</td>\n      <td>0.127099</td>\n      <td>1.080399</td>\n      <td>8.665757</td>\n      <td>0.356316</td>\n      <td>0.017094</td>\n      <td>-0.075868</td>\n      <td>-0.644914</td>\n      <td>...</td>\n      <td>1.213107</td>\n      <td>0.160286</td>\n      <td>1.285635</td>\n      <td>10.311934</td>\n      <td>8.271088e+01</td>\n      <td>1.362498</td>\n      <td>10.928448</td>\n      <td>8.765587e+01</td>\n      <td>7.030781e+02</td>\n      <td>5.639311e+03</td>\n    </tr>\n    <tr>\n      <th>1.65</th>\n      <th>1.71</th>\n      <td>1.0</td>\n      <td>-0.597831</td>\n      <td>-0.023519</td>\n      <td>0.170407</td>\n      <td>1.441151</td>\n      <td>12.374259</td>\n      <td>0.357401</td>\n      <td>0.014061</td>\n      <td>-0.101875</td>\n      <td>-0.861564</td>\n      <td>...</td>\n      <td>4.446455</td>\n      <td>0.510054</td>\n      <td>4.379515</td>\n      <td>37.604141</td>\n      <td>3.228831e+02</td>\n      <td>4.313581</td>\n      <td>37.038013</td>\n      <td>3.180222e+02</td>\n      <td>2.730656e+03</td>\n      <td>2.344643e+04</td>\n    </tr>\n    <tr>\n      <th>1.68</th>\n      <th>1.71</th>\n      <td>1.0</td>\n      <td>-0.598532</td>\n      <td>-0.016657</td>\n      <td>0.228717</td>\n      <td>1.955418</td>\n      <td>17.616632</td>\n      <td>0.358240</td>\n      <td>0.009970</td>\n      <td>-0.136894</td>\n      <td>-1.170380</td>\n      <td>...</td>\n      <td>16.234627</td>\n      <td>1.710083</td>\n      <td>15.406376</td>\n      <td>138.798172</td>\n      <td>1.250452e+03</td>\n      <td>14.620379</td>\n      <td>131.717010</td>\n      <td>1.186657e+03</td>\n      <td>1.069075e+04</td>\n      <td>9.631448e+04</td>\n    </tr>\n    <tr>\n      <th>1.71</th>\n      <th>1.71</th>\n      <td>1.0</td>\n      <td>-0.598957</td>\n      <td>-0.007424</td>\n      <td>0.308405</td>\n      <td>2.686601</td>\n      <td>25.003866</td>\n      <td>0.358750</td>\n      <td>0.004447</td>\n      <td>-0.184721</td>\n      <td>-1.609159</td>\n      <td>...</td>\n      <td>59.464287</td>\n      <td>5.980401</td>\n      <td>55.658863</td>\n      <td>518.010254</td>\n      <td>4.821059e+03</td>\n      <td>52.096973</td>\n      <td>484.860199</td>\n      <td>4.512535e+03</td>\n      <td>4.199762e+04</td>\n      <td>3.908667e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 126 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe then use another extremely popular machine learning Python package called `scikit-learn` to easily work with our regression models.\n\n#### Ordinary least squares\nFirst, let's apply ordinary least squares to see if the solution is clear:\n\n::: {#d4901cda .cell execution_count=17}\n``` {.python .cell-code}\nimport sklearn.linear_model as lm\nimport sklearn.metrics as met\n\ndef compute_ols_results(A,b):\n    ols = lm.LinearRegression()\n    ols.fit(A, b)\n    Rsquare = met.r2_score(ols.predict(A), b)\n    print(\"R^2: {}\".format(Rsquare))\n    ols_results = pd.DataFrame(\n        data=[ols.coef_],\n        columns=term_matrix.columns,\n        index=[\"Coefficients\"]\n    )\n    return ols_results\ncompute_ols_results(term_matrix, h_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR^2: 0.9996492734907563\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=51}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>h(x, t)</th>\n      <th>h_x(x, t)</th>\n      <th>h_xx(x, t)</th>\n      <th>h_xxx(x, t)</th>\n      <th>h_xxxx(x, t)</th>\n      <th>h(x, t)**2</th>\n      <th>h(x, t)*h_x(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.006074</td>\n      <td>-0.000767</td>\n      <td>0.012011</td>\n      <td>-0.019441</td>\n      <td>0.020865</td>\n      <td>-0.001556</td>\n      <td>0.001374</td>\n      <td>-0.001566</td>\n      <td>-0.003335</td>\n      <td>-0.005921</td>\n      <td>...</td>\n      <td>-6.012885e-10</td>\n      <td>1.292627e-09</td>\n      <td>-3.423755e-11</td>\n      <td>1.410440e-11</td>\n      <td>-2.050787e-12</td>\n      <td>1.208457e-10</td>\n      <td>-2.007819e-12</td>\n      <td>8.242712e-14</td>\n      <td>1.095998e-14</td>\n      <td>-1.987343e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 126 columns</p>\n</div>\n```\n:::\n:::\n\n\nAlthough the $R^2$ value implies that we have successful explained the variance in $h_t$ by linearly combining our term library, it is unclear which of all the terms most contributes to the time evolution from their coefficients.\n\n#### Lasso\nNow, let's add some regularization to try to remove some terms with the Lasso regression:\n\n::: {#2c9d7cd3 .cell execution_count=18}\n``` {.python .cell-code}\ndef compute_lasso_results(A,b,lamb):\n    lasso = lm.Lasso(lamb)\n    lasso.fit(A,b)\n    lasso_results = pd.DataFrame(\n        data=[lasso.coef_[lasso.coef_ != 0]],\n        columns=term_matrix.columns[lasso.coef_ != 0],\n        index=[\"Coefficients\"]\n    )\n    return lasso_results\ncompute_lasso_results(term_matrix,h_t,30)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+03, tolerance: 1.101e+01\n  model = cd_fast.enet_coordinate_descent(\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=52}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)*h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.000042</td>\n      <td>0.000011</td>\n      <td>0.000006</td>\n      <td>-0.000013</td>\n      <td>0.000006</td>\n      <td>-9.324149e-08</td>\n      <td>0.000018</td>\n      <td>-0.00001</td>\n      <td>-0.000006</td>\n      <td>-0.000003</td>\n      <td>...</td>\n      <td>3.263501e-12</td>\n      <td>4.454580e-10</td>\n      <td>-3.401866e-10</td>\n      <td>7.938618e-12</td>\n      <td>-2.294608e-13</td>\n      <td>-1.175425e-11</td>\n      <td>1.186419e-11</td>\n      <td>-5.447040e-13</td>\n      <td>3.270449e-14</td>\n      <td>-2.627133e-16</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 85 columns</p>\n</div>\n```\n:::\n:::\n\n\nNow this at least removed some of the terms, but it also removed the term we know is correct!\nIt's somewhat hard to interpret exactly what this means.\nA convenient analysis using the Lasso method is to perform a \"lasso path\" in which we steadily decrease the regularization $\\lambda$ to add more and more terms and pay attention to the order with which they are added:\n\n::: {#87faceb8 .cell execution_count=19}\n``` {.python .cell-code}\ndef compute_lasso_path_results(A,b):\n    lambs, coef_path, _ = lm.lasso_path(A, b, alphas=[1000,200,100,10,2])\n    for i in range(coef_path.shape[1]):\n        print(\"lambda = {}\".format(lambs[i]))\n        temp_results = pd.DataFrame(\n            data=[coef_path[:,i][coef_path[:,i] != 0]],\n            columns=term_matrix.columns[coef_path[:,i] != 0],\n            index=[\"Coefficients\"]\n        )\n        display(temp_results)\ncompute_lasso_path_results(term_matrix,h_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlambda = 1000\nlambda = 200\nlambda = 100\nlambda = 10\nlambda = 2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6980.483780579942, tolerance: 11.006989386931043\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4524.135133175237, tolerance: 11.006989386931043\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3107.4025479351976, tolerance: 11.006989386931043\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1188.0183171689562, tolerance: 11.006989386931043\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/connor/mambaforge/envs/website/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 697.3581390562916, tolerance: 11.006989386931043\n  model = cd_fast.enet_coordinate_descent_gram(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_x(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xxx(x, t)**2</th>\n      <th>h_x(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xxxx(x, t)**2</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.000009</td>\n      <td>-1.087579e-07</td>\n      <td>-0.000001</td>\n      <td>-0.000005</td>\n      <td>2.171248e-07</td>\n      <td>-0.000024</td>\n      <td>-0.000019</td>\n      <td>0.000018</td>\n      <td>-2.755649e-07</td>\n      <td>3.451722e-08</td>\n      <td>...</td>\n      <td>-8.895495e-11</td>\n      <td>6.462359e-09</td>\n      <td>-6.202690e-10</td>\n      <td>-1.216947e-11</td>\n      <td>-6.853476e-13</td>\n      <td>-1.306299e-10</td>\n      <td>3.176435e-11</td>\n      <td>-1.961896e-12</td>\n      <td>3.128785e-14</td>\n      <td>-1.361317e-16</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 65 columns</p>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_x(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xx(x, t)**2</th>\n      <th>h_x(x, t)*h_xx(x, t)*h_xxx(x, t)</th>\n      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xxx(x, t)**2</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.000011</td>\n      <td>-1.387047e-07</td>\n      <td>-0.000025</td>\n      <td>-0.000006</td>\n      <td>2.198013e-07</td>\n      <td>-0.000021</td>\n      <td>0.000665</td>\n      <td>0.000017</td>\n      <td>-0.000011</td>\n      <td>0.000016</td>\n      <td>...</td>\n      <td>-1.070690e-10</td>\n      <td>7.987405e-09</td>\n      <td>-6.214230e-10</td>\n      <td>-1.383315e-11</td>\n      <td>-9.970465e-13</td>\n      <td>-7.850788e-11</td>\n      <td>4.038726e-11</td>\n      <td>-1.214295e-12</td>\n      <td>2.695820e-14</td>\n      <td>-1.176027e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 73 columns</p>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_x(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xx(x, t)**2</th>\n      <th>h_x(x, t)*h_xx(x, t)*h_xxx(x, t)</th>\n      <th>h_x(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_x(x, t)*h_xxx(x, t)**2</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.000013</td>\n      <td>-2.292844e-07</td>\n      <td>-0.000043</td>\n      <td>-0.000005</td>\n      <td>2.366792e-07</td>\n      <td>-0.000014</td>\n      <td>0.001129</td>\n      <td>0.000014</td>\n      <td>-0.000006</td>\n      <td>0.000011</td>\n      <td>...</td>\n      <td>-1.032142e-10</td>\n      <td>8.410341e-09</td>\n      <td>-6.058633e-10</td>\n      <td>-1.360524e-11</td>\n      <td>-1.086436e-12</td>\n      <td>-3.424014e-11</td>\n      <td>4.570394e-11</td>\n      <td>-6.688498e-13</td>\n      <td>2.538924e-14</td>\n      <td>-1.630228e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 73 columns</p>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)*h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)*h_xxx(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)**2</th>\n      <th>h(x, t)*h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.000067</td>\n      <td>0.000025</td>\n      <td>-0.000013</td>\n      <td>0.000012</td>\n      <td>-1.936250e-07</td>\n      <td>-0.000017</td>\n      <td>-0.000174</td>\n      <td>0.000006</td>\n      <td>-0.000052</td>\n      <td>-0.000005</td>\n      <td>...</td>\n      <td>-1.046131e-10</td>\n      <td>7.854440e-09</td>\n      <td>-5.748668e-10</td>\n      <td>-1.008073e-11</td>\n      <td>-1.033930e-12</td>\n      <td>-1.095605e-11</td>\n      <td>4.469762e-11</td>\n      <td>-1.600585e-13</td>\n      <td>2.423573e-14</td>\n      <td>-1.629860e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 88 columns</p>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)</th>\n      <th>h_xxxx(x, t)**2</th>\n      <th>h(x, t)**2*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_x(x, t)*h_xxxx(x, t)</th>\n      <th>h(x, t)*h_xx(x, t)*h_xxx(x, t)</th>\n      <th>...</th>\n      <th>h_xx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**3</th>\n      <th>h_xx(x, t)*h_xxx(x, t)**2*h_xxxx(x, t)</th>\n      <th>h_xx(x, t)*h_xxx(x, t)*h_xxxx(x, t)**2</th>\n      <th>h_xx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxx(x, t)**4</th>\n      <th>h_xxx(x, t)**3*h_xxxx(x, t)</th>\n      <th>h_xxx(x, t)**2*h_xxxx(x, t)**2</th>\n      <th>h_xxx(x, t)*h_xxxx(x, t)**3</th>\n      <th>h_xxxx(x, t)**4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.00013</td>\n      <td>-0.000047</td>\n      <td>0.000031</td>\n      <td>0.000036</td>\n      <td>-0.00001</td>\n      <td>0.00001</td>\n      <td>-1.567570e-07</td>\n      <td>-0.000049</td>\n      <td>-0.000029</td>\n      <td>-0.00042</td>\n      <td>...</td>\n      <td>-1.108456e-10</td>\n      <td>6.983191e-09</td>\n      <td>-4.148959e-10</td>\n      <td>-1.275555e-11</td>\n      <td>-8.681877e-13</td>\n      <td>-4.061798e-12</td>\n      <td>3.953281e-11</td>\n      <td>3.713025e-13</td>\n      <td>1.342052e-14</td>\n      <td>-1.264346e-15</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 95 columns</p>\n</div>\n```\n:::\n:::\n\n\nAgain, although this gives us a sense of sparsity, it also doesn't seem to capture the solution well.\n\n#### Greedy forward selection\nLet's instead try a greedy method for our system that will inform which terms should be included.\nTo do so, we will use a generic `scikit-learn` interface called `SequentialFeatureSelector` as well as the $R^2$ coefficient of determination `r2_score` to select terms one by one that best \"explain the variance\" in the time evolution $h_t(x,t)$.\nAs the terms are selected, we will compute the coefficients of the small libraries via ordinary least squares:\n\n::: {#e4231f2e .cell execution_count=20}\n``` {.python .cell-code}\nimport sklearn.feature_selection as fs\n\ndef forward_r2_select(A,b,num_terms=4):\n    for i in range(1,num_terms+1):\n        sfs = fs.SequentialFeatureSelector(\n            lm.LinearRegression(),\n            n_features_to_select=i,\n            scoring=met.make_scorer(met.r2_score)\n        )\n        new_A = sfs.fit_transform(A,b)\n        new_ols = sfs.estimator\n        new_ols.fit(new_A,b)\n        Rsquare = met.r2_score(new_ols.predict(new_A),b)\n        feat_names = sfs.get_feature_names_out(A.columns)\n        print(\"R^2: {}\".format(Rsquare))\n        temp_results = pd.DataFrame(\n            data=[new_ols.coef_],\n            columns=feat_names,\n            index=[\"Coefficients\"]\n        )\n        display(temp_results)\n\nforward_r2_select(term_matrix, h_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR^2: 0.9998860862791236\nR^2: 0.999894405563566\nR^2: 0.9998959178372478\nR^2: 0.999896315291591\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.994806</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h(x, t)*h_x(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.999196</td>\n      <td>-0.003702</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h(x, t)*h_x(x, t)</th>\n      <th>h_x(x, t)**2*h_xxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.999783</td>\n      <td>-0.002925</td>\n      <td>9.549915e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h(x, t)*h_x(x, t)</th>\n      <th>h_x(x, t)**2*h_xxx(x, t)</th>\n      <th>h_xxxx(x, t)**3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.999799</td>\n      <td>-0.002916</td>\n      <td>9.808213e-07</td>\n      <td>2.122813e-14</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis seems to easily pick up that the only term needed to completely resolve the time evolution is $h_x(x,t)$!\n\n## Application to noisy simulated wave data {#sec-noisy}\nIn a real system, we could not expect to immediately have data as smooth as that we used in @sec-simulated.\nHowever, the procedure is unchanged.\nThe only challenge will be fitting the neural network to our data.\nLet's add some noise to the data:\n\n::: {#a9dbde69 .cell execution_count=21}\n``` {.python .cell-code}\nx,t,noisy_h = load_data(\"simple_wave.npz\",.2)\nanimation2 = animate_data(x,t,[noisy_h], [\"h noisy\"])\nanimation2.save(\"noisy_h.gif\")\nplt.close()\n```\n:::\n\n\n![](noisy_h.gif)\n\nGiven our data is now noisy, we may want to implement a train-validation-test method for fitting.\nSimply put, this means that we will hold out a portion of our data from the training procedure.\nPart of this held-back data (validation set) will be used to validate that our model can generalize to other points during training.\nThe other part of the held-back data (test set) will be used as a final check on how well the model extrapolates out of the training data.\n\n::: {#a6a8fed1 .cell execution_count=22}\n``` {.python .cell-code}\nimport sklearn.model_selection as ms\n\nX,T = jnp.meshgrid(x,t)\nxt_noisy = np.vstack((X.flatten(),T.flatten())).T\nh_noisy = noisy_h.flatten()\nxt_train, xt_test, h_train, h_test = ms.train_test_split(xt_noisy,h_noisy,test_size=.1,train_size=.9)\nxt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n\ntrain_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\nvalid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\ntest_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)\n```\n:::\n\n\nNow, we apply our previous model construction and training:\n\n::: {#2e449a1e .cell execution_count=23}\n``` {.python .cell-code}\n# Initialize model\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel2 = MyNet()\nparams2 = model2.init(rng2,random_data)\n\n# Loss function\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model2.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\n# Optimizer\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params2)\n\n# Training (adjusted to use our validation data\nepochs = 1200\nfor i in range(epochs):\n    xt_batch = train_data[i%len(train_data)][0]\n    h_batch = train_data[i%len(train_data)][1]\n    loss_val, grads = loss_grad_fn(params2, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params2 = optax.apply_updates(params2, updates)\n    if i % 100 == 0:\n        train_loss = mse(params2,xt_train,h_train)\n        valid_loss = mse(params2,xt_valid,h_valid)\n        print(\"Step {}\".format(i))\n        print(\"Training loss: {}\".format(train_loss))\n        print(\"Validation loss: {}\".format(valid_loss))\n        print()\ntest_loss = mse(params2,xt_test,h_test)\nprint(\"Test loss after training: {}\".format(test_loss))\n\nhhat2 = model2.apply(params2,xt_points).reshape(X.shape)\ndiff = np.sqrt((noisy_h - hhat2)**2)\ndiff2 = np.sqrt((hhat1 - hhat2)**2)\nanimation3 = animate_data(x,t,[noisy_h,hhat2,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation3.save(\"noisy_h_compare.gif\")\nplt.close()\nanimation3 = animate_data(x,t,[hhat1,hhat2,diff2],[\"$\\hat{h}$ clean\",\"$\\hat{h}$ noisy\",\"$L^2$ error\"])\nanimation3.save(\"noisy_hhat_compare.gif\")\nplt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStep 0\nTraining loss: 1.1158357858657837\nValidation loss: 1.1611391305923462\n\nStep 100\nTraining loss: 0.2566785216331482\nValidation loss: 0.27670320868492126\n\nStep 200\nTraining loss: 0.16225649416446686\nValidation loss: 0.16113916039466858\n\nStep 300\nTraining loss: 0.13091057538986206\nValidation loss: 0.12360212206840515\n\nStep 400\nTraining loss: 0.08167731761932373\nValidation loss: 0.07373432070016861\n\nStep 500\nTraining loss: 0.04770171642303467\nValidation loss: 0.04413319751620293\n\nStep 600\nTraining loss: 0.04175411909818649\nValidation loss: 0.03958415612578392\n\nStep 700\nTraining loss: 0.040661100298166275\nValidation loss: 0.038582943379879\n\nStep 800\nTraining loss: 0.041066572070121765\nValidation loss: 0.039159368723630905\n\nStep 900\nTraining loss: 0.04048626124858856\nValidation loss: 0.03811538964509964\n\nStep 1000\nTraining loss: 0.039836008101701736\nValidation loss: 0.03688812628388405\n\nStep 1100\nTraining loss: 0.04021302983164787\nValidation loss: 0.03748621046543121\n\nTest loss after training: 0.0397583544254303\n```\n:::\n:::\n\n\nThe resulting fit can be seen in the following video:\n\n![](noisy_h_compare.gif)\n\nLooks pretty good all things considered!\nWe can also compare this with the fit on clean data to see how impressive the robustness to noise was:\n\n![](noisy_hhat_compare.gif)\n\nFinally, we construct the terms and check the results after forward selection:\n\n::: {#f5177dee .cell execution_count=24}\n``` {.python .cell-code}\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model2.apply(params2, new_x)[0]\n\n# Construct terms numerically\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_points))\nterm_values = construct_terms(diff_term_values)\n\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_points))\n\n# Forward selection\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_points),2).T, names=(\"x\",\"t\")))\nforward_r2_select(term_matrix, h_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR^2: 0.9990509168414861\nR^2: 0.9990732768205717\nR^2: 0.9990812471604171\nR^2: 0.9990885015533407\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.986246</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_x(x, t)**2*h_xxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.991677</td>\n      <td>0.000004</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_x(x, t)**2*h_xxx(x, t)</th>\n      <th>h_x(x, t)**2*h_xx(x, t)*h_xxxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.992142</td>\n      <td>0.000004</td>\n      <td>4.674522e-09</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_x(x, t)**3</th>\n      <th>h_x(x, t)**2*h_xxx(x, t)</th>\n      <th>h_x(x, t)**2*h_xx(x, t)*h_xxxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>0.99853</td>\n      <td>-0.000202</td>\n      <td>0.000002</td>\n      <td>4.747428e-09</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBoom!\nLanded right on the money.\nThis is a simple example with a straightforward answer, but example holds to show the overall procedure for handling data with additive noise (multiplicative noise, which is more structural, would be an altogether different challenge).\n\n## Application to extracted wave data {#sec-extracted}\nNow, applying this procedure to real data is as simple as replacing our original dataset with an experimental dataset.\nHowever, the extraction process has a strong influence on the quality of the data that we will be using, so it deserves to be treated with some detail.\n\n### Image data extraction\nThe original video we will be using can be found on YouTube [here](https://www.youtube.com/watch?v=wEbYELtGZwI).\n\n![](youtube_video.mp4)\n\nWe can load this video into individual image frames via:\n\n::: {#913e99fd .cell execution_count=25}\n``` {.python .cell-code}\nimport skimage as img\nimport imageio.v3 as iio\n\nraw_frames = []\ncut = (160,200)\nfor i in range(200,232):\n    frame = iio.imread(\"youtube_video.mp4\",plugin=\"pyav\",index=i)\n\n    # Cut the image to focus only on the wave portion\n    raw_frame = frame[cut[0]:cut[1],:,:]\n    raw_frames.append(raw_frame)\nraw_frames = np.array(raw_frames)\nplt.figure(figsize=(8,1))\nplt.imshow(raw_frames[16])\nplt.axis(False); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop3_discovery_files/figure-ipynb/cell-26-output-1.png){}\n:::\n:::\n\n\nWe then need to remove the background and isolate the wave portion of the image, which is facilitated by the green color of the water in this video:\n\n::: {#1a695377 .cell execution_count=26}\n``` {.python .cell-code}\nframes = []\nfor i in range(len(raw_frames)):\n    frame = raw_frames[i]\n\n    # Find where the image is more green than red or blue and very bright green\n    mean_green = np.mean(frame[:,:,1])\n    std_green = np.std(frame[:,:,1])\n    frame = (frame[:,:,1] > frame[:,:,0]) & (frame[:,:,1] > frame[:,:,2]) & (frame[:,:,1] > mean_green+std_green)\n    frames.append(frame)\nframes = np.array(frames)\nplt.figure(figsize=(8,1))\nplt.imshow(frames[16],cmap=\"gray\")\nplt.axis(False); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop3_discovery_files/figure-ipynb/cell-27-output-1.png){}\n:::\n:::\n\n\nBy averaging these pixels across all vertical pixels in the image, we can get a rough wave outline:\n\n::: {#f7b80f3b .cell execution_count=27}\n``` {.python .cell-code}\nheights = []\nfor i in range(len(frames)):\n    frame = frames[i]\n    \n    # Approximate wave height by averaging y-locations of bright green areas\n    height = np.zeros(frame.shape[1])\n    for j in range(frame.shape[1]):\n        height[j] = np.mean(np.where(frame[:,j] == 1)[0])\n    heights.append(height)\nheights = np.array(heights)\nbase = heights[16, 0]\n\nplt.figure(figsize=(8,1))\nplt.imshow(frames[16],cmap=\"gray\")\nline = plt.plot(heights[16], color=\"red\",lw=3)[0]\nline2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\nplt.axis(False); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop3_discovery_files/figure-ipynb/cell-28-output-1.png){}\n:::\n:::\n\n\nFinally, we can note that the video is not quite level to the wave surface, so we can use a linear adjustment to align the water boundary heights at the middle of the video:\n\n::: {#aa5d2ef5 .cell execution_count=28}\n``` {.python .cell-code}\n# Adjust images and heights for an un-leveled camera\nim_width = len(heights[16])\nslope = (heights[16][-1] - heights[16][0]) / im_width\nfor i in range(len(heights)):\n    frame = frames[i]\n    height = heights[i]\n\n    # Adjust\n    for j in range(len(height)):\n        shift = int(slope*(im_width-j))\n        # Move frame pixels per column\n        frame[:,j] = np.roll(frame[:,j], shift)\n        # Move height of wave\n        height[j] += shift\n    frames[i] = frame\n    heights[i] = height\n\nframes = np.array(frames)\nraw_frames = np.array(raw_frames)\nheights = np.array(heights)\n\nfig = plt.figure(figsize=(8,1))\nim = plt.imshow(frames[0],cmap=\"gray\")\nline = plt.plot(heights[0], color=\"red\",lw=3)[0]\nline2 = plt.plot([0,heights.shape[1]], [31,31], color=\"orange\", ls=\"--\")[0]\nplt.axis(False);\n\ndef animation_function(i):\n    im.set_array(frames[i])\n    line.set_ydata(heights[i])\n    return [im,line,line2]\n\nwave_animation = anim.FuncAnimation(fig, animation_function, frames=range(len(frames)), blit=True)\nwave_animation.save(\"extracted_wave.gif\")\nplt.close()\n```\n:::\n\n\n![](extracted_wave.gif)\n\nWe can now save this data to be used with our previous procedure:\n\n::: {#975a0cc2 .cell execution_count=29}\n``` {.python .cell-code}\n# Video portion is about 2 seconds long\ntimes = np.linspace(0,2,len(heights))\n# No given space scale\nx_domain = np.arange(len(heights[0]))\nnp.save(\"video_wave_images.npy\",raw_frames)\nnp.savez(\"video_wave_heights.npz\",h=heights,x=x_domain,t=times)\n```\n:::\n\n\n### Using our experimental dataset\nUsing the same methods as listed in @sec-noisy, we can discover an equation for this particular dataset:\n\n::: {#f9f7839e .cell execution_count=30}\n``` {.python .cell-code}\nx,t,ext_h = load_data(\"video_wave_heights.npz\")\n# Flip image wave to be more familiar\next_h = -ext_h\nanimation2 = animate_data(x,t,[ext_h], [\"extracted h\"])\nanimation2.save(\"extracted_h.gif\")\nplt.close()\n```\n:::\n\n\n![](extracted_h.gif)\n\n::: {#8c8b9ac0 .cell execution_count=31}\n``` {.python .cell-code}\n# Splitting data\nX,T = jnp.meshgrid(x,t)\nxt_ext = np.vstack((X.flatten(),T.flatten())).T\nh_ext = ext_h.flatten()\nxt_train, xt_test, h_train, h_test = ms.train_test_split(xt_ext,h_ext,test_size=.1,train_size=.9)\nxt_train, xt_valid, h_train, h_valid = ms.train_test_split(xt_train,h_train,test_size=.1,train_size=.9)\n\ntrain_data = batch_data(xt_train[:,0], xt_train[:,1], h_train, 1000)\nvalid_data = batch_data(xt_valid[:,0], xt_valid[:,1], h_valid, 1000)\ntest_data = batch_data(xt_test[:,0], xt_test[:,1], h_test, 1000)\n\n# Initialize model\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel3 = MyNet()\nparams3 = model3.init(rng2,random_data)\n\n# Loss function\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model3.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\n# Optimizer\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params3)\n\n# Training (adjusted to use our validation data\nepochs = 1200\nfor i in range(epochs):\n    xt_batch = train_data[i%len(train_data)][0]\n    h_batch = train_data[i%len(train_data)][1]\n    loss_val, grads = loss_grad_fn(params3, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params3 = optax.apply_updates(params3, updates)\n    if i % 100 == 0:\n        train_loss = mse(params3,xt_train,h_train)\n        valid_loss = mse(params3,xt_valid,h_valid)\n        print(\"Step {}\".format(i))\n        print(\"Training loss: {}\".format(train_loss))\n        print(\"Validation loss: {}\".format(valid_loss))\n        print()\ntest_loss = mse(params3,xt_test,h_test)\nprint(\"Test loss after training: {}\".format(test_loss))\n\nhhat = model3.apply(params3,xt_ext).reshape(X.shape)\ndiff = np.sqrt((ext_h - hhat)**2)\nanimation3 = animate_data(x,t,[ext_h,hhat,diff],[\"$extracted h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation3.save(\"ext_h_compare.gif\")\nplt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStep 0\nTraining loss: 1.0704325437545776\nValidation loss: 1.0239237546920776\n\nStep 100\nTraining loss: 0.018038345500826836\nValidation loss: 0.018494710326194763\n\nStep 200\nTraining loss: 0.015583153814077377\nValidation loss: 0.015977952629327774\n\nStep 300\nTraining loss: 0.015503301285207272\nValidation loss: 0.015727590769529343\n\nStep 400\nTraining loss: 0.014337100088596344\nValidation loss: 0.014470705762505531\n\nStep 500\nTraining loss: 0.013790973462164402\nValidation loss: 0.014019783586263657\n\nStep 600\nTraining loss: 0.013632175512611866\nValidation loss: 0.013687998987734318\n\nStep 700\nTraining loss: 0.013248699717223644\nValidation loss: 0.013375375419855118\n\nStep 800\nTraining loss: 0.012480981647968292\nValidation loss: 0.012584874406456947\n\nStep 900\nTraining loss: 0.012430530972778797\nValidation loss: 0.012353399768471718\n\nStep 1000\nTraining loss: 0.011869644746184349\nValidation loss: 0.011919286102056503\n\nStep 1100\nTraining loss: 0.011265994049608707\nValidation loss: 0.011274118907749653\n\nTest loss after training: 0.010519650764763355\n```\n:::\n:::\n\n\n![](ext_h_compare.gif)\n\n::: {#d93919bb .cell execution_count=32}\n``` {.python .cell-code}\ndef model_for_diff(x,t):\n    new_x = jnp.array([x,t])\n    return model3.apply(params3, new_x)[0]\n\n# Construct terms numerically\ndiff_term_values = {}\nfor i in range(max_diff_order+1):\n    diff_func = model_for_diff\n    # Iteratively apply derivatives\n    for _ in range(i):\n        diff_func = jax.grad(diff_func, 0)\n    def unpack_diff_func(x):\n        new_x,new_t = x\n        return diff_func(new_x,new_t)\n    diff_term_values[diff_terms[i]] = np.array(jax.lax.map(unpack_diff_func, xt_ext))\nterm_values = construct_terms(diff_term_values)\n\ndef unpack_diff_func(x):\n    new_x,new_t = x\n    return jax.grad(model_for_diff,1)(new_x,new_t)\n\nh_t_term = sp.Function(\"h_t\")(x_sym,t_sym)\nh_t = -np.array(jax.lax.map(unpack_diff_func, xt_ext))\n\n# Forward selection\nterm_matrix = pd.DataFrame(term_values,index=pd.MultiIndex.from_arrays(np.round(np.array(xt_ext),2).T, names=(\"x\",\"t\")))\nforward_r2_select(term_matrix, h_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR^2: 0.9841035484441384\nR^2: 0.9842864609584957\nR^2: 0.9866974048132867\nR^2: 0.9901451531874754\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.959266</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_xxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.969612</td>\n      <td>-0.000351</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_xxx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.962383</td>\n      <td>-0.003749</td>\n      <td>0.001811</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>h_x(x, t)</th>\n      <th>h_xxx(x, t)</th>\n      <th>h(x, t)*h_xxx(x, t)</th>\n      <th>h(x, t)**2*h_xxx(x, t)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficients</th>\n      <td>-0.936698</td>\n      <td>-0.006393</td>\n      <td>0.007898</td>\n      <td>-0.002174</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFeel free to play with the parameters of each step to try to change/improve the results we have seen here.\n\n## Appendix {.appendix}\n\n### The benefits of JAX {.appendix}\n[`jax`](https://jax.readthedocs.io/en/latest/index.html) is an automatic differentiation based on the [XLA](https://www.tensorflow.org/xla) compiler for Tensorflow.\nThe largest difference between this library and the alternative libraries (like those included in `tensorflow` main, `pytorch`, `keras`, etc.) is that it compiles Python code down to a computational graph structure.\nAlthough the majority of excitement around this compiler has surrounded the optimizations that can take place one the graph structure has been identified, it also facilitates taking derivatives of arbitrarry objects.\nThis is because rather than compute gradients along the path (forward mode automatic differentiation) or keeping track of operations as it goes (backward mode automatic differentiation), it has a graph structure to analyze exactly what happens to each value and parameter.\nAt the end of the day, this means it is much easier to compute gradients of exactly what you want.\n\nIf `jax` and `flax` have appeared too hands-on and complicated after this workshop, consider trying [`treex`](https://cgarciae.github.io/treex/) which aims to make using `jax` for neural networks simple and only need a few lines of code.\n\n### Training without normalizing the data {.appendix}\n\nIn the @sec-simulated section, the `load_data` function performs a normalization of the simulated data from a range of $h(x,t) \\in [0,.1]$ where $x \\in [0,1]$ and $t \\in [0,1]$ to a range of $h(x,t) \\in [-1,1]$ with mean $\\bar{h}=0$ and standard deviation 1 with $x \\in [-1.7,1.7]$ and $t \\in [-1.7,1.7]$.\nNormalizing data like this is common in machine learning, but it is not always apparent why.\nOur case can give a strong demonstration as to the benefits of normalizing in this way.\n\nConsider that our neural network uses only the $\\tanh$ activation function.\nFor those unfamiliar, this function has the form:\n\n::: {#beae6876 .cell execution_count=33}\n``` {.python .cell-code}\ntanh_dom = np.linspace(-5,5,100)\ntanh_range = np.tanh(tanh_dom)\nplt.plot(tanh_dom, tanh_range, label=\"$\\\\tanh(x)$\")\nplt.xlabel(\"$x$\"); plt.legend(); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop3_discovery_files/figure-ipynb/cell-34-output-1.png){}\n:::\n:::\n\n\nEach layer of our neural network is connected via linear transformations of the form $\\vec{x}^T\\mathbf{W} + \\vec{b}$, thus we should be able to shift the data into the appropriate domain and range for the $\\tanh$ function.\nHowever, in practice, optimizing our parameters to attain this is hard to find.\nTo demonstrate, consider the training of the neural network on clean simulation data without normalization:\n\n::: {#9b818272 .cell execution_count=34}\n``` {.python .cell-code}\nx,t,h = load_data(\"simple_wave.npz\",norm=False)\nX,T = jnp.meshgrid(x,t)\ndata = batch_data(X.flatten(),T.flatten(),h.flatten(),10000)\n# Random generator seed\nrng1,rng2 = jax.random.split(jax.random.PRNGKey(42))\nrandom_data = jax.random.normal(rng1,(2,))\nmodel4 = MyNet()\nparams4 = model1.init(rng2,random_data)\n\n@jax.jit\ndef mse(params,input,targets):\n    def squared_error(x,y):\n        pred = model4.apply(params,x)\n        return jnp.mean((y - pred)**2)\n    return jnp.mean(jax.vmap(squared_error)(input,targets),axis=0)\nloss_grad_fn = jax.value_and_grad(mse)\n\nlearning_rate = 1e-2\ntx = optax.adam(learning_rate)\nopt_state = tx.init(params4)\n\nepochs = 1000\nall_xt = jnp.array([data[i][0] for i in range(len(data))])\nall_h = jnp.array([data[i][1] for i in range(len(data))])\nfor i in range(epochs):\n    xt_batch = data[i%len(data)][0]\n    h_batch = data[i%len(data)][1]\n    loss_val, grads = loss_grad_fn(params4, xt_batch, h_batch)\n    updates, opt_state = tx.update(grads, opt_state)\n    params4 = optax.apply_updates(params4, updates)\n    if i % 100 == 0:\n        train_loss = mse(params4,all_xt,all_h)\n        print(\"Training loss step {}: {}\".format(i,train_loss))\n\nxt_points = jnp.vstack([X.flatten(),T.flatten()]).T\nhhat4 = model1.apply(params4,xt_points).reshape(X.shape)\ndiff = np.sqrt((h - hhat4)**2)\nanimation4 = animate_data(x,t,[h,hhat4,diff],[\"$h$\",\"$\\hat{h}$\",\"$L^2$ error\"])\nanimation4.save(\"nonorm_h_compare.gif\")\nplt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining loss step 0: 0.3563382923603058\nTraining loss step 100: 0.0009187606628984213\nTraining loss step 200: 0.0008465295541100204\nTraining loss step 300: 0.0007735079852864146\nTraining loss step 400: 0.0006919147563166916\nTraining loss step 500: 0.0006207430851645768\nTraining loss step 600: 0.0005757115432061255\nTraining loss step 700: 0.0005529409390874207\nTraining loss step 800: 0.0005375399487093091\nTraining loss step 900: 0.0005216019926592708\n```\n:::\n:::\n\n\n![](nonorm_h_compare.gif)\n\nThe fit is terrible!\nWe have apparently fallen into a local minimum far from the global minimum we would like to find.\nThis demonstrates two important ideas relating to neural networks (validated by experience):\n\n1. They are fickle and in some cases small changes to data, architecture, and training, can dramatically change results\n2. Any help that can be given to the neural network via knowledge of the system or data can help. In this case, adjusting for the gap between the range of our data and that of the activation function was sufficient.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  widgets: {"application/vnd.jupyter.widget-state+json":{"state":{"078fea23f666446f85d98c4c53c11e47":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09451c4b02804c498e655c87cfffa0fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0adbdee5882f42dc85e89dab03930057":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c9f444012b44428ab59d515d5ab2b62":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f157f5928a447bca20ca21310222c50":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f598cdf50a54a6b8cf3083092c57115":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122bdf724c2c44ed8ca879b9cec1ff33":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0c9f444012b44428ab59d515d5ab2b62","placeholder":"​","style":"IPY_MODEL_9414800c12b144288f52897080e9150a","tabbable":null,"tooltip":null,"value":"100%"}},"2dd9113033de41459648e8369ede3f82":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_122bdf724c2c44ed8ca879b9cec1ff33","IPY_MODEL_b835baa8826b4ac7862d7e561994e86c","IPY_MODEL_96f0c64520bc46abb42584458729115e"],"layout":"IPY_MODEL_7efdd7d624a3499484bc6b9d66b6028f","tabbable":null,"tooltip":null}},"6daac99cce254f48b987c57b5bd424ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddc61e02f03c42a68831747b70870a40","IPY_MODEL_cc8632c6184742daaa5947a3c693e920","IPY_MODEL_9388a3469f68411798cfe2725f3ee150"],"layout":"IPY_MODEL_078fea23f666446f85d98c4c53c11e47","tabbable":null,"tooltip":null}},"7a7fb53beb094fea9de8f001b9fed6db":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7efdd7d624a3499484bc6b9d66b6028f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9388a3469f68411798cfe2725f3ee150":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0adbdee5882f42dc85e89dab03930057","placeholder":"​","style":"IPY_MODEL_f9085d7ce339432a93a6e8a02628a2af","tabbable":null,"tooltip":null,"value":" 100/100 [00:01&lt;00:00, 101.76it/s]"}},"9414800c12b144288f52897080e9150a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"96f0c64520bc46abb42584458729115e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f1e5ada77e744470aa1f6796ce95d6bd","placeholder":"​","style":"IPY_MODEL_7a7fb53beb094fea9de8f001b9fed6db","tabbable":null,"tooltip":null,"value":" 100/100 [00:01&lt;00:00, 104.68it/s]"}},"982250f3005c41f6a5f4efbcd8bb516a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b835baa8826b4ac7862d7e561994e86c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c99ddf05cb3644dba127498d48d2dd77","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f157f5928a447bca20ca21310222c50","tabbable":null,"tooltip":null,"value":100}},"c16334fb85794dd28098e98018f4355a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c99ddf05cb3644dba127498d48d2dd77":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc8632c6184742daaa5947a3c693e920":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0f598cdf50a54a6b8cf3083092c57115","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09451c4b02804c498e655c87cfffa0fc","tabbable":null,"tooltip":null,"value":100}},"ddc61e02f03c42a68831747b70870a40":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_982250f3005c41f6a5f4efbcd8bb516a","placeholder":"​","style":"IPY_MODEL_c16334fb85794dd28098e98018f4355a","tabbable":null,"tooltip":null,"value":"100%"}},"f1e5ada77e744470aa1f6796ce95d6bd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9085d7ce339432a93a6e8a02628a2af":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}\n---\n",
    "supporting": [
      "workshop3_discovery_files/figure-ipynb"
    ],
    "filters": []
  }
}