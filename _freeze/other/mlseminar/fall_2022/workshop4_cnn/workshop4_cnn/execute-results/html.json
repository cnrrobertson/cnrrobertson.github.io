{
  "hash": "416c586f3976a3d168c422808ef6e8e5",
  "result": {
    "markdown": "---\nexecute:\n  daemon: 500\n  keep-ipynb: true\n---\n\n# Machine Learning Workshop -- Convolutional Neural Network (CNN)\n\n## Installing and loading some needed packages\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Needed package for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# Needed package for getting the current working directory\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n# Needed package for builing the convolutional neural network\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nfrom tensorflow.keras.utils import plot_model\n\n# Needed package for computer vision problems\nimport cv2\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-06 11:21:34.952123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n## Loading the MNIST data from keras library and split that into train and test dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nimage_index = 0\nprint('The number for index = ' + str(image_index) + ' is ' + str(y_train[image_index])) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe number for index = 0 is 5\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.imshow(x_train[image_index], cmap='Greys')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<matplotlib.image.AxesImage at 0x7f5f219e3130>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-4-output-2.png){width=415 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(60000, 28, 28)\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_train[image_index].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(28, 28)\n```\n:::\n:::\n\n\n## Normalize data and put them into the correct format\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)\n# Making sure that the values are float so that we can get decimal points after division\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint('Number of images in x_train', x_train.shape[0])\nprint('Number of images in x_test', x_test.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_train shape: (60000, 28, 28, 1)\nNumber of images in x_train 60000\nNumber of images in x_test 10000\n```\n:::\n:::\n\n\n## Convolutional Neural Network (CNN) structure\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten()) \nmodel.add(Dense(256, activation=tf.nn.relu))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation=tf.nn.softmax))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-06 11:21:36.765905: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n )                                                               \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n conv2d_1 (Conv2D)           (None, 11, 11, 32)        9248      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n 2D)                                                             \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n flatten (Flatten)           (None, 800)               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense (Dense)               (None, 256)               205056    \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout (Dropout)           (None, 256)               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_1 (Dense)             (None, 10)                2570      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 217,194\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 217,194\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\n## Training the CNN\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\nmodel.fit(x=x_train,y=y_train, epochs=5)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmodel.evaluate(x_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  1/313 [..............................] - ETA: 28s - loss: 0.0717 - accuracy: 0.9688\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 31/313 [=>............................] - ETA: 0s - loss: 0.0495 - accuracy: 0.9859 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 66/313 [=====>........................] - ETA: 0s - loss: 0.0592 - accuracy: 0.9830\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r101/313 [========>.....................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9827\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r139/313 [============>.................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9831\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r178/313 [================>.............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9847\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r217/313 [===================>..........] - ETA: 0s - loss: 0.0534 - accuracy: 0.9856\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r257/313 [=======================>......] - ETA: 0s - loss: 0.0462 - accuracy: 0.9874\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r298/313 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9886\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r313/313 [==============================] - 1s 1ms/step - loss: 0.0406 - accuracy: 0.9889\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n[0.04061735048890114, 0.9889000058174133]\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimage_index = 59\nplt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\npred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\nprint(' For image_index = ' + str(image_index) + ' CNN predicted number = ' + str(pred.argmax()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/1 [==============================] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 48ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n For image_index = 59 CNN predicted number = 5\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-12-output-4.png){width=415 height=411}\n:::\n:::\n\n\n# In this part we are going to take a picture of your own handwriting and pass it into the CNN model, for doing that we need to do the following steps:\n\n## 1. Write a number in 6 different ways on a paper and transfer the picture into the same directory that your Jupyter Notebook is\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ncwd = os.getcwd()\nprint('current working directory is = ' + cwd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncurrent working directory is = /home/connor/GDrive/Software/cnrrobertson.github.io/other/mlseminar/fall_2022/workshop4_cnn\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfile = r'test_images/original_image.jpeg'\ntest_image_original = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.imshow(test_image_original, cmap = 'gray')\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<matplotlib.image.AxesImage at 0x7f5f08167910>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-15-output-2.png){width=580 height=392}\n:::\n:::\n\n\n## 2. Take a picture of each of them individually -- you can have only one picture and then crop it into 6 different pieces\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfig, axs = plt.subplots(2, 3)\n\n\ncounter = 1\nfor row_number in range(0, 2):\n    for col_number in range(0,3):\n        \n        file = r'test_images/copy_' + str(counter) +'.jpeg'\n        copy_image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n        axs[row_number, col_number].imshow(copy_image, cmap = 'gray')\n        counter = counter + 1\n\n```\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-16-output-1.png){width=569 height=416}\n:::\n:::\n\n\n## 4. Change the format of the picture into a readable form for your CNN model\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfile = r'test_images/copy_5.jpeg'\ncopy_1 = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\nplt.imshow(copy_1, cmap = 'gray')\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<matplotlib.image.AxesImage at 0x7f5f001c2100>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-17-output-2.png){width=281 height=415}\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# copy_1_resized = cv2.resize(copy_1, (28, 28), interpolation = cv2.INTER_LINEAR)\ncopy_1_resized = cv2.resize(copy_1, (28, 28))\ncopy_1_resized = cv2.bitwise_not(copy_1_resized)\n\nplt.imshow(copy_1_resized, cmap = 'Greys')\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n<matplotlib.image.AxesImage at 0x7f5f00198b50>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop4_cnn_files/figure-html/cell-18-output-2.png){width=415 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\npred = model.predict(copy_1_resized.reshape(1, 28, 28, 1))\nprint('CNN predicted number = ' + str(pred.argmax()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/1 [==============================] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 33ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCNN predicted number = 2\n```\n:::\n:::\n\n\n",
    "supporting": [
      "workshop4_cnn_files"
    ],
    "filters": [],
    "includes": {}
  }
}