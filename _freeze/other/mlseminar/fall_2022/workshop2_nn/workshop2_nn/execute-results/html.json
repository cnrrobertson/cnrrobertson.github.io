{
  "hash": "c1b85d2807c8fccc9f77505e49dbdff3",
  "result": {
    "markdown": "---\ntitle: 'Workshop 2: Neural networks - structure, building, and training'\nauthor: Jake Brusca\nexecute:\n  keep-ipynb: true\n---\n\n# Setup \nInformation on setting up a Python environment and package management can be found in the [first workshop.](https://cnrrobertson.github.io/other/mlseminar/fall_2022/workshop1_intro/workshop1_intro.html)\n\n\nIn Terminal:\n```\nmamba activate workshop \nmamba install numpy py-pde tensorflow matplotlib\n```\n\nYou can use [Google colab](https://colab.research.google.com) if unable to run local Jupyter Notebooks.\n\nIn cell:\n```\n!pip install numpy py-pde tensorflow matplotlib\n```\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#Import packages\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-09 11:52:49.406478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-09 11:52:49.443538: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n```\n:::\n:::\n\n\n\n# Learning a simple model\nAs a quick example, we'll try to fit a wide, shallow neural network to $sin(x)$ on $[0, 4\\pi]$. We pick a network with a single layer, using ReLU as the activation function. We create a set of sample data $\\overline{X},\\overline{Y}$, then try to fit the network $N(x_i,\\beta) \\approx y_i$\n$$\n\\beta^* = \\text{argmin} \\sum (N(x_i;\\beta)-y_i)^2\n$$\nFrom the universal approximation theorem, we know that some network exists which can approximate this curve to any precision, however it's unclear that the network we discover from our optimization problem will be that network.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Generate sin data\nxmin_s = 0\nxmax_s= 4*np.pi\nnx_s = 100\nx_s= np.linspace(xmin_s,xmax_s,nx_s)\ny_s= np.sin(x_s)\n\n# Build a model\nmodel_s = Sequential()\nmodel_s.add(Dense(700, input_shape=(1,), activation='relu'))\nmodel_s.add(Dense(1))\nmodel_s.compile(loss = 'mae',optimizer = 'adam')\nmodel_s.fit(x_s,y_s,epochs=10000,batch_size = 25, verbose = 0)\n\n# Make Prediction\ny_pred_s = model_s.predict(x_s) # Prediction with training data\nx_test_s = np.linspace(xmin_s,2*xmax_s,4*nx_s)\ny_test_s = model_s.predict(x_test_s)\n\n# Plot Results\nplt.figure(figsize = (12,4))\nplt.plot(x_test_s,y_test_s)\nplt.subplot(1,3,1)\nplt.plot(x_s,y_s)\nplt.subplot(1,3,2)\nplt.plot(x_s,y_pred_s)\nplt.subplot(1,3,3)\nplt.plot(x_test_s,y_test_s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/4 [======>.......................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 0s 939us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/13 [=>............................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/13 [==============================] - 0s 458us/step\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop2_nn_files/figure-html/cell-3-output-5.png){width=962 height=337}\n:::\n:::\n\n\n\n# Learning a PDE\nWe consider a simple 1D transport equation with periodic boundary conditions\n\\begin{cases}\nu_t = u_x, \\quad (x,t) \\in [0,L]\\times(0,\\infty)\n\\\\\nu(x,0) = u_0(x)\n\\\\\nu(x,t) = u(x+L,t)\n\\end{cases}\nThe true solution to this PDE is given by $u(x,t) = u_0(\\text{mod}(x-t,1))$, which can be found using the method of characteristics on the free domain and then truncating to a periodic domain. Though we have access to the true solution, we will generate our training data using a prepackaged numerical PDE solver `py-pde`. We will use a Gaussian as our initial data $u_0(x) = e^{-100(x-.3)^2}$ and solve on the domain $[0,1]$.\n\nNote that this isn't an attempt to solve the PDE using NNs, rather we are just using the PDE data as a specific set of training data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pde\n\n# Domain\nxmin = 0.0\nxmax = 1.0\nnx = 101\ntmin = 0.0\ntmax = 1.0\ndt = 1e-6\nsave_dt = 0.01\ninit_cond = \"1*exp(-(1/.01)*(x-0.3)**2)\"\n\n# Initialize objects\ngrid = pde.CartesianGrid([(xmin,xmax)],nx,periodic=True)\nh = pde.ScalarField.from_expression(grid,init_cond,label=\"h(x,t)\")\neq = pde.PDE({\"h\": \"-d_dx(h)\"})\nstorage = pde.MemoryStorage()\n\n# Run\nresult = eq.solve(h,t_range=tmax,dt=dt,tracker=storage.tracker(save_dt))\n\n# Save data\ndata = np.array(storage.data)\nnp.save(\"simple_wave.npy\", data)\n```\n:::\n\n\n# Data Processing\nArguably the most important part of training a machine learning algorithm is the data. Most prepackaged algorithms expect data to be formatted in a specific way, usually as an array where each column represents different features and each row represents the samples. As it stands, we have our target $h(x,t)$ data represented as a matrix, and have our $x,t$ each represented as single arrays. We need create an array of each pair of $x,t$ data points, and map the $h(x,t)$ to array of the corresponding values.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Create training data\nx = np.linspace(xmin,xmax,nx)\nnt = int((tmax-tmin)/save_dt+1)\nt = np.linspace(tmin,tmax,nt)\nT = np.repeat(t,nx)\nX = np.tile(x,int(nt))\nXT = np.transpose(np.array([X,T]))\ny = np.transpose(data.reshape(nt*nx))\n```\n:::\n\n\n# Building a Network\nWe'll use the Keras package to build our Neural Networks. Keras is an API for building Neural Networks built on `tensorflow`. We can initialize the network using the Sequential() class, then add layers .add() method for our model. We will use Dense layers, which means that each node takes inputs from all of the other nodes in the previous layer. We can specify the initial input shape in the first layer, and each size and activation function for each layer. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Build model\nmodel = Sequential()\nmodel.add(Dense(12, input_shape=(2,), activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n\n# Train model\nmodel.fit(XT,y,epochs=1000, batch_size=200, verbose = 0)\nyPred = np.array(model.predict(XT)).reshape(nt,nx)\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\np = plt.imshow(data,aspect = 'auto')\nplt.colorbar()\n\nplt.subplot(1,2,2)\np = plt.imshow(yPred,aspect = 'auto')\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  1/319 [..............................] - ETA: 9s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/319 [==========>...................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r260/319 [=======================>......] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r319/319 [==============================] - 0s 386us/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<matplotlib.colorbar.Colorbar at 0x7f6647798df0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop2_nn_files/figure-html/cell-6-output-6.png){width=931 height=341}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Create subset of training data\nt_stop= .5/tmax\nt_split = nx*int(t_stop*nt)\nXT_Train = XT[0:t_split,:]\ny_Train = y[0:t_split]\n\n# Fit new model\nmodel.fit(XT_Train,y_Train,epochs=1000, batch_size=200, verbose = 0)\n\nplt.figure(figsize = (12,4))\n\nplt.subplot(1,2,1)\np = plt.imshow(y_Train.reshape(int(t_stop*nt),nx))\nplt.colorbar()\n\nplt.subplot(1,2,2)\nyPred = np.array(model.predict(XT)).reshape(nx,nt)\np = plt.imshow(yPred)\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  1/319 [..............................] - ETA: 3s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r134/319 [===========>..................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r267/319 [========================>.....] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r319/319 [==============================] - 0s 377us/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.colorbar.Colorbar at 0x7f664756ad30>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop2_nn_files/figure-html/cell-7-output-6.png){width=931 height=341}\n:::\n:::\n\n\n# Hyper Parameters \nOne of the many challenges you face when working with Neural Networks is the wide range of hyper-parameters you need to choose in order to build the network. Some of the more obvious ones are the number of layers, the depth of each layer, and the activation function you use. It's clear that these can have dramatic effects on the resulting neural network, but even smaller changes can too. In this example, we double the number of epochs that the network is trained on. This tends to result in a lower quality prediction, likely from overfitting to specific training data. There are ways to better chose hyper-parameters and mitigate things like over fitting, but that is beyond the scope of this workshop. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmodel.fit(XT_Train,y_Train,epochs=2000, batch_size=200, verbose = 0)\nyPred = np.array(model.predict(XT)).reshape(nx,nt)\np = plt.imshow(yPred)\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  1/319 [..............................] - ETA: 3s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r129/319 [===========>..................] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r257/319 [=======================>......] - ETA: 0s\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r319/319 [==============================] - 0s 390us/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<matplotlib.colorbar.Colorbar at 0x7f6647474bb0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](workshop2_nn_files/figure-html/cell-8-output-6.png){width=507 height=414}\n:::\n:::\n\n\n",
    "supporting": [
      "workshop2_nn_files"
    ],
    "filters": [],
    "includes": {}
  }
}